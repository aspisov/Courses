{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsZpFIaRfROD",
        "outputId": "e5e6a59d-d91b-41f7-a230-fa4e9bc3e449"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/_meta_registrations.py:25\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;129m@register_meta\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroi_align\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_roi_align\u001b[39m(\u001b[38;5;28minput\u001b[39m, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):\n\u001b[1;32m     27\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(rois\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrois must have shape as Tensor[K, 5]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m rois\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         ),\n\u001b[1;32m     34\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torchvision\u001b[38;5;241m.\u001b[39mextension\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[1;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9HlqnlYoUJM"
      },
      "source": [
        "### Get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "AYTvLpTFfR9L",
        "outputId": "3baedbdb-2b28-4ed1-d627-647633ef1d94"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=10f1H2T-5W-BiqabHHtlZ4ASs19TZmg8R\n",
            "From (redirected): https://drive.google.com/uc?id=10f1H2T-5W-BiqabHHtlZ4ASs19TZmg8R&confirm=t&uuid=8c91a23e-b723-404e-864e-01c84f6f72f9\n",
            "To: /content/data.zip\n",
            "100%|██████████| 979M/979M [00:19<00:00, 50.4MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'data.zip'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=10f1H2T-5W-BiqabHHtlZ4ASs19TZmg8R'\n",
        "output = 'data.zip'\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLSvVki2fzUf"
      },
      "outputs": [],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1g03B9mtZeb"
      },
      "source": [
        "### Utilities (0.5 point)\n",
        "\n",
        "Complete dataset to load prepared images and masks. Don't forget to use augmentations.\n",
        "\n",
        "Some of the images are 1 channels, so use `gray2rgb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YT2QUTqFooxJ"
      },
      "outputs": [],
      "source": [
        "def gray2rgb(img):\n",
        "    if len(img.shape) != 3:\n",
        "        img = np.dstack([img, img, img])\n",
        "    return img\n",
        "\n",
        "def get_iou(gt, pred):\n",
        "    pred = pred > 0.5\n",
        "    return (gt & pred).sum() / (gt | pred).sum()\n",
        "\n",
        "class BirdsDataset(Dataset):\n",
        "    def __init__(self, folder, ...) -> None:\n",
        "        images_folder = os.path.join(folder, 'images')\n",
        "        gt_folder = os.path.join(folder, 'gt')\n",
        "\n",
        "        for class_name in os.listdir(images_folder):\n",
        "            for fname in os.listdir(os.path.join(images_folder, class_name)):\n",
        "                # YOUR CODE HERE\n",
        "\n",
        "        self.transform = A.Compose([\n",
        "            # YOUR CODE HERE\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # YOUR CODE HERE\n",
        "        img = ...\n",
        "        mask = ...\n",
        "        img = gray2rgb(img)\n",
        "        # YOUR CODE HERE\n",
        "        return transformed_img, transformed_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        # YOUR CODE HERE\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dss-ZnpTuI1V"
      },
      "source": [
        "### Architecture (1 point)\n",
        "Your task for today is to build your own Unet to solve the segmentation problem.\n",
        "\n",
        "As an encoder, you can use pre-trained on IMAGENET models(or parts) from torchvision. The decoder must be trained from scratch.\n",
        "It is forbidden to use data not from the `data` folder.\n",
        "\n",
        "I advise you to experiment with the number of blocks so as not to overfit on the training sample and get good quality on validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Elr1Uw3uITD"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels, out_channels):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def forward(self,x):\n",
        "        # YOUR CODE HERE\n",
        "        return\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE\n",
        "        # encoder blocks\n",
        "        self.encoder1=\n",
        "        self.encoder2=\n",
        "        self.encoder3=\n",
        "        # decoder blocks\n",
        "        self.decoder1=\n",
        "        self.decoder2=\n",
        "        self.decoder3=\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # YOUR CODE HERE\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sq4WwZsuMeD"
      },
      "source": [
        "### Train script (0.5 point)\n",
        "\n",
        "Complete the train and predict scripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_ha44iifROE"
      },
      "outputs": [],
      "source": [
        "def train_segmentation_model(data_path):\n",
        "    BATCH_SIZE = 8\n",
        "    N_EPOCH = 15\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    train_dataset = BirdsDataset(data_path + 'train')\n",
        "    val_dataset = BirdsDataset(data_path + 'val')\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = Unet().to(DEVICE)\n",
        "    optimizer = # YOUR CODE HERE\n",
        "    criterion = # YOUR CODE HERE\n",
        "    losses_train, losses_val, ious_train, ious_val = [], [], [], []\n",
        "\n",
        "    for epoch in range(N_EPOCH):\n",
        "        model.train()\n",
        "\n",
        "        for tqdm(inputs, masks) in train_dataloader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            masks = masks.to(DEVICE)\n",
        "            # YOUR CODE HERE\n",
        "        losses_train.append(...)\n",
        "        ious_train.append(...)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, masks in tqdm(val_dataloader):\n",
        "                inputs = inputs.to(DEVICE)\n",
        "                masks = masks.to(DEVICE)\n",
        "                # YOUR CODE HERE\n",
        "        losses_val.append(...)\n",
        "        ious_val.append(...)\n",
        "\n",
        "        torch.save(model.state_dict(), f'model_{epoch}.pth')\n",
        "\n",
        "        print(f\"Epoch: {epoch}, train loss: {losses_train[-1]}, val loss: {losses_val[-1]}, train iou: {ious_train[-1]}, val iou: {ious_val[-1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96EkIQmutpdS"
      },
      "outputs": [],
      "source": [
        "def predict(model, img_path):\n",
        "    with torch.no_grad():\n",
        "        # YOUR CODE HERE TO PREPARE IMAGE\n",
        "        # GET PREDICTIONS\n",
        "        # POST PROCESS\n",
        "        return segm\n",
        "\n",
        "def get_model(path):\n",
        "    model = Unet()\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzZS9Z2jfROF"
      },
      "outputs": [],
      "source": [
        "train_segmentation_model('data/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWKD09whySKA"
      },
      "source": [
        "You can also experiment with models and write a small report about results. If the report will be meaningful, you will receive an extra point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCHacSHutHo4"
      },
      "source": [
        "### Testing (8 points)\n",
        "Your model will be tested on the new data, similar to validation, so use techniques to prevent overfitting the model.\n",
        "\n",
        "* IoU > 0.85 — 8 points\n",
        "* IoU > 0.80 — 7 points\n",
        "* IoU > 0.75 — 6 points\n",
        "* IoU > 0.70 — 5 points\n",
        "* IoU > 0.60 — 4 points\n",
        "* IoU > 0.50 — 3 points\n",
        "* IoU > 0.40 — 2 points\n",
        "* IoU > 0.30 — 1 points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ6h11Q0tUHN"
      },
      "outputs": [],
      "source": [
        "model = get_model('model_14.pth').to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV9zadusfROF"
      },
      "outputs": [],
      "source": [
        "ious, times = [], []\n",
        "test_dir = 'data/val/'\n",
        "\n",
        "for class_name in tqdm(sorted(os.listdir(os.path.join(test_dir, 'images')))):\n",
        "    for img_name in sorted(os.listdir(os.path.join(test_dir, 'images', class_name))):\n",
        "\n",
        "        t_start = time()\n",
        "        pred = predict(model, os.path.join(test_dir, 'images', class_name, img_name))\n",
        "        times.append(time() - t_start)\n",
        "\n",
        "        gt_name = img_name.replace('jpg', 'png')\n",
        "        gt = np.asarray(Image.open(os.path.join(test_dir, 'gt', class_name, gt_name)), dtype = np.uint8)\n",
        "        if len(gt.shape) > 2:\n",
        "            gt = gt[:, :, 0]\n",
        "\n",
        "        iou = get_iou(gt==255, pred>0.5)\n",
        "        ious.append(iou)\n",
        "\n",
        "np.mean(ious), np.mean(times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47KgrqdpvKWS"
      },
      "source": [
        "### Compression (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kJiLB__vTC3"
      },
      "source": [
        "Try to speed up the model in any way without losing more than 1% in iou score.\n",
        "For example [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQyNHbt0vtMu"
      },
      "outputs": [],
      "source": [
        "def get_fast_model():\n",
        "    # YOUR CODE HERE\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2DedST0v6aF"
      },
      "outputs": [],
      "source": [
        "fast_model = get_fast_model().to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryWUekS2vlv8"
      },
      "outputs": [],
      "source": [
        "ious, times = [], []\n",
        "test_dir = 'data/val/'\n",
        "\n",
        "for class_name in tqdm(sorted(os.listdir(os.path.join(test_dir, 'images')))):\n",
        "    for img_name in sorted(os.listdir(os.path.join(test_dir, 'images', class_name))):\n",
        "\n",
        "        t_start = time()\n",
        "        pred = predict(fast_model, os.path.join(test_dir, 'images', class_name, img_name))\n",
        "        times.append(time() - t_start)\n",
        "\n",
        "        gt_name = img_name.replace('jpg', 'png')\n",
        "        gt = np.asarray(Image.open(os.path.join(test_dir, 'gt', class_name, gt_name)), dtype = np.uint8)\n",
        "        if len(gt.shape) > 2:\n",
        "            gt = gt[:, :, 0]\n",
        "\n",
        "        iou = get_iou(gt==255, pred>0.5)\n",
        "        ious.append(iou)\n",
        "\n",
        "np.mean(ious), np.mean(times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCdMgBoOwXAb"
      },
      "source": [
        "**Bonus:** For the best iou score on test(without compression) in group you will get 1.5, 1, 0.5 extra points(for 1st, 2nd, 3rd places)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daanikNkwo5t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
