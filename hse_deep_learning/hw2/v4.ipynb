{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tsZpFIaRfROD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import wandb\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9HlqnlYoUJM"
      },
      "source": [
        "### Get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYTvLpTFfR9L",
        "outputId": "70e3ec38-c1b5-4b95-de75-d181d8142839",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=10f1H2T-5W-BiqabHHtlZ4ASs19TZmg8R\n",
            "From (redirected): https://drive.google.com/uc?id=10f1H2T-5W-BiqabHHtlZ4ASs19TZmg8R&confirm=t&uuid=f3e62ebf-e2c7-426e-a89b-6ff761a73994\n",
            "To: /content/data.zip\n",
            "100%|██████████| 979M/979M [00:10<00:00, 89.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "replace data/val/gt/193.Bewick_Wren/Bewick_Wren_0124_184771.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=10f1H2T-5W-BiqabHHtlZ4ASs19TZmg8R'\n",
        "output = 'data.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1g03B9mtZeb"
      },
      "source": [
        "### Utilities (0.5 point)\n",
        "\n",
        "Complete dataset to load prepared images and masks. Don't forget to use augmentations.\n",
        "\n",
        "Some of the images are 1 channels, so use `gray2rgb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2RjELs3Aalwo"
      },
      "outputs": [],
      "source": [
        "# config.py\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    # Training parameters\n",
        "    epochs: int = 15\n",
        "    batch_size: int = 16\n",
        "    learning_rate: float = 1e-4\n",
        "    dropout_rate: float = 0.1\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Dataset parameters\n",
        "    input_size: Tuple[int, int] = (256, 256)\n",
        "    num_classes: int = 1\n",
        "\n",
        "    # Optimizer parameters\n",
        "    weight_decay: float = 0.01\n",
        "    scheduler_patience: int = 2\n",
        "    scheduler_factor: float = 0.1\n",
        "\n",
        "    # WandB parameters\n",
        "    project_name: str = \"bird-segmentation\"\n",
        "    run_name: Optional[str] = None\n",
        "\n",
        "    # Data augmentation parameters\n",
        "    normalize_mean: Tuple[float, float, float] = (0.485, 0.456, 0.406)\n",
        "    normalize_std: Tuple[float, float, float] = (0.229, 0.224, 0.225)\n",
        "\n",
        "    pred_threshold: float = 0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tzIzK0qmaJf"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YT2QUTqFooxJ"
      },
      "outputs": [],
      "source": [
        "# dataset.py\n",
        "import cv2\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from typing import Tuple, Optional\n",
        "import torch\n",
        "\n",
        "\n",
        "class BirdsDataset(Dataset):\n",
        "    \"\"\"Dataset class for bird segmentation\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        folder: str,\n",
        "        config: TrainingConfig,\n",
        "        transform: Optional[A.Compose] = None,\n",
        "        is_training: bool = True\n",
        "    ) -> None:\n",
        "        self.image_paths, self.mask_paths = self._get_paths(folder)\n",
        "        self.transform = transform or (\n",
        "            self._get_train_transforms(config) if is_training\n",
        "            else self._get_default_transforms(config)\n",
        "        )\n",
        "\n",
        "    def _get_paths(self, folder: str) -> Tuple[list, list]:\n",
        "        \"\"\"Get paths for images and masks\"\"\"\n",
        "        images_folder = os.path.join(folder, \"images\")\n",
        "        gt_folder = os.path.join(folder, \"gt\")\n",
        "\n",
        "        image_paths = []\n",
        "        mask_paths = []\n",
        "\n",
        "        for class_name in os.listdir(images_folder):\n",
        "            class_folder = os.path.join(images_folder, class_name)\n",
        "            if os.path.isdir(class_folder):\n",
        "                for fname in os.listdir(class_folder):\n",
        "                    image_paths.append(os.path.join(class_folder, fname))\n",
        "                    mask_paths.append(\n",
        "                        os.path.join(gt_folder, class_name, fname[:-3] + \"png\")\n",
        "                    )\n",
        "\n",
        "        return image_paths, mask_paths\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_train_transforms(config: TrainingConfig) -> A.Compose:\n",
        "        \"\"\"Get enhanced training augmentation pipeline\"\"\"\n",
        "        return A.Compose([\n",
        "            # Spatial augmentations\n",
        "            A.RandomResizedCrop(\n",
        "                *config.input_size,\n",
        "                scale=(0.8, 1.0),\n",
        "                ratio=(0.9, 1.1),\n",
        "                p=1.0\n",
        "            ),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(\n",
        "                shift_limit=0.2,\n",
        "                scale_limit=0.2,\n",
        "                rotate_limit=30,\n",
        "                border_mode=cv2.BORDER_CONSTANT,\n",
        "                value=0,\n",
        "                p=0.5\n",
        "            ),\n",
        "\n",
        "            # Color augmentations\n",
        "            A.OneOf([\n",
        "                A.RandomBrightnessContrast(\n",
        "                    brightness_limit=0.2,\n",
        "                    contrast_limit=0.2,\n",
        "                    p=1\n",
        "                ),\n",
        "                A.RandomGamma(gamma_limit=(80, 120), p=1),\n",
        "                A.HueSaturationValue(\n",
        "                    hue_shift_limit=20,\n",
        "                    sat_shift_limit=30,\n",
        "                    val_shift_limit=20,\n",
        "                    p=1\n",
        "                )\n",
        "            ], p=0.3),\n",
        "\n",
        "            # Noise augmentations\n",
        "            A.OneOf([\n",
        "                A.GaussNoise(var_limit=(10.0, 50.0), p=1),\n",
        "                A.GaussianBlur(blur_limit=(3, 7), p=1),\n",
        "                A.MotionBlur(blur_limit=7, p=1)\n",
        "            ], p=0.2),\n",
        "\n",
        "            # Dropout for regularization\n",
        "            A.CoarseDropout(\n",
        "                max_holes=8,\n",
        "                max_height=32,\n",
        "                max_width=32,\n",
        "                min_holes=5,\n",
        "                min_height=8,\n",
        "                min_width=8,\n",
        "                fill_value=0,\n",
        "                p=0.2\n",
        "            ),\n",
        "\n",
        "            # Normalization and conversion to tensor\n",
        "            A.Normalize(\n",
        "                mean=config.normalize_mean,\n",
        "                std=config.normalize_std\n",
        "            ),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_default_transforms(config: TrainingConfig) -> A.Compose:\n",
        "        \"\"\"Get default transforms for validation\"\"\"\n",
        "        return A.Compose([\n",
        "            A.Resize(*config.input_size),\n",
        "            A.Normalize(\n",
        "                mean=config.normalize_mean,\n",
        "                std=config.normalize_std\n",
        "            ),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Get a sample from the dataset\"\"\"\n",
        "        # Load images\n",
        "        img = cv2.imread(self.image_paths[index])\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(self.mask_paths[index], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Apply transforms\n",
        "        transformed = self.transform(image=img, mask=mask)\n",
        "\n",
        "        return (\n",
        "            transformed[\"image\"],\n",
        "            transformed[\"mask\"].float().unsqueeze(0) / 255.0,\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dss-ZnpTuI1V"
      },
      "source": [
        "### Architecture (1 point)\n",
        "Your task for today is to build your own Unet to solve the segmentation problem.\n",
        "\n",
        "As an encoder, you can use pre-trained on IMAGENET models(or parts) from torchvision. The decoder must be trained from scratch.\n",
        "It is forbidden to use data not from the `data` folder.\n",
        "\n",
        "I advise you to experiment with the number of blocks so as not to overfit on the training sample and get good quality on validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_Elr1Uw3uITD"
      },
      "outputs": [],
      "source": [
        "# model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "class AttentionGate(nn.Module):\n",
        "    \"\"\"Attention Gate for focusing on relevant features\"\"\"\n",
        "\n",
        "    def __init__(self, F_g: int, F_l: int, F_int: int):\n",
        "        super().__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(F_g, F_int, kernel_size=1),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(F_l, F_int, kernel_size=1),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(F_int, 1, kernel_size=1),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "        return x * psi\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block for enhanced feature learning\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        residual = self.shortcut(x)\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.bn2(self.conv2(x))\n",
        "        x += residual\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"Enhanced decoder block with attention and residual connections\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, skip_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.attention = AttentionGate(out_channels, skip_channels, out_channels)\n",
        "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "        self.residual1 = ResidualBlock(out_channels + skip_channels, out_channels)\n",
        "        self.residual2 = ResidualBlock(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, skip: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.upconv(x)\n",
        "        x = self._handle_size_mismatch(x, skip)\n",
        "        skip = self.attention(x, skip)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        x = self.residual1(x)\n",
        "        x = self.residual2(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _handle_size_mismatch(x: torch.Tensor, skip: torch.Tensor) -> torch.Tensor:\n",
        "        if x.size() != skip.size():\n",
        "            diff_h = skip.size()[2] - x.size()[2]\n",
        "            diff_w = skip.size()[3] - x.size()[3]\n",
        "            x = F.pad(\n",
        "                x,\n",
        "                [diff_w // 2, diff_w - diff_w // 2,\n",
        "                 diff_h // 2, diff_h - diff_h // 2],\n",
        "            )\n",
        "        return x\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"U-Net architecture with ResNet50 encoder\"\"\"\n",
        "\n",
        "    def __init__(self, config: TrainingConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Initialize ResNet encoder\n",
        "        resnet = torchvision.models.resnet50(\n",
        "            weights=torchvision.models.ResNet50_Weights.DEFAULT\n",
        "        )\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            resnet.conv1,\n",
        "            resnet.bn1,\n",
        "            resnet.relu,\n",
        "        )  # 64 channels\n",
        "        self.pool = resnet.maxpool\n",
        "        self.encoder2 = resnet.layer1  # 256 channels\n",
        "        self.encoder3 = resnet.layer2  # 512 channels\n",
        "        self.encoder4 = resnet.layer3  # 1024 channels\n",
        "        self.encoder5 = resnet.layer4  # 2048 channels\n",
        "\n",
        "        # Decoder with adjusted channel sizes for ResNet50\n",
        "        self.decoder4 = DecoderBlock(2048, 1024, 512)\n",
        "        self.decoder3 = DecoderBlock(512, 512, 256)\n",
        "        self.decoder2 = DecoderBlock(256, 256, 128)\n",
        "        self.decoder1 = DecoderBlock(128, 64, 64)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(64, config.num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Encoder\n",
        "        enc1 = self.encoder1(x)           # 64 channels\n",
        "        enc2 = self.encoder2(self.pool(enc1))  # 256 channels\n",
        "        enc3 = self.encoder3(enc2)        # 512 channels\n",
        "        enc4 = self.encoder4(enc3)        # 1024 channels\n",
        "        enc5 = self.encoder5(enc4)        # 2048 channels\n",
        "\n",
        "        # Decoder\n",
        "        dec4 = self.decoder4(enc5, enc4)\n",
        "        dec3 = self.decoder3(dec4, enc3)\n",
        "        dec2 = self.decoder2(dec3, enc2)\n",
        "        dec1 = self.decoder1(dec2, enc1)\n",
        "\n",
        "        # Final output\n",
        "        out = self.final_conv(dec1)\n",
        "        return F.interpolate(\n",
        "            out,\n",
        "            size=x.shape[2:],\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sq4WwZsuMeD"
      },
      "source": [
        "### Train script (0.5 point)\n",
        "\n",
        "Complete the train and predict scripts."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou(outputs: torch.Tensor, masks: torch.Tensor, threshold: float) -> float:\n",
        "    \"\"\"Calculate IoU score\"\"\"\n",
        "    pred = (torch.sigmoid(outputs) > threshold)\n",
        "    intersection = (pred & masks.bool()).float().sum((1, 2, 3))\n",
        "    union = (pred | masks.bool()).float().sum((1, 2, 3))\n",
        "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
        "    return iou.mean().item()"
      ],
      "metadata": {
        "id": "DTs4-0j7m4e-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "d_ha44iifROE"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Dict\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
        "        return 1 - dice\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"Training class for U-Net model\"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, config: TrainingConfig):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.device = config.device\n",
        "\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
        "        self.dice_loss = DiceLoss()\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config.learning_rate,\n",
        "            weight_decay=config.weight_decay,\n",
        "        )\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode=\"min\",\n",
        "            patience=config.scheduler_patience,\n",
        "            factor=config.scheduler_factor,\n",
        "        )\n",
        "\n",
        "        self._setup_wandb()\n",
        "\n",
        "    def _setup_wandb(self):\n",
        "        \"\"\"Initialize WandB run\"\"\"\n",
        "        self.run = wandb.init(\n",
        "            project=self.config.project_name,\n",
        "            name=self.config.run_name,\n",
        "            config=self.config,\n",
        "        )\n",
        "        wandb.watch(self.model, self.bce_loss, log=\"all\", log_freq=10)\n",
        "\n",
        "    def _init_metrics(self) -> Dict[str, float]:\n",
        "        \"\"\"Initialize metrics dictionary\"\"\"\n",
        "        return {\n",
        "            \"loss\": 0.0,\n",
        "            \"iou\": 0.0,\n",
        "            \"count\": 0,\n",
        "        }\n",
        "\n",
        "    def _update_metrics(\n",
        "        self, epoch_metrics: Dict[str, float], batch_metrics: Dict[str, float]\n",
        "    ):\n",
        "        \"\"\"Update epoch metrics with batch metrics\"\"\"\n",
        "        epoch_metrics[\"loss\"] += batch_metrics[\"loss\"]\n",
        "        epoch_metrics[\"iou\"] += batch_metrics[\"iou\"]\n",
        "        epoch_metrics[\"count\"] += 1\n",
        "\n",
        "    def _finalize_metrics(\n",
        "        self, epoch_metrics: Dict[str, float], num_batches: int\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"Calculate final metrics for the epoch\"\"\"\n",
        "        return {\n",
        "            \"loss\": epoch_metrics[\"loss\"] / num_batches,\n",
        "            \"iou\": epoch_metrics[\"iou\"] / num_batches,\n",
        "        }\n",
        "\n",
        "    def _validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:\n",
        "        \"\"\"Validation loop for one epoch\"\"\"\n",
        "        self.model.eval()\n",
        "        val_metrics = self._init_metrics()\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            for inputs, masks in tqdm(val_loader):\n",
        "                inputs = inputs.to(self.device)\n",
        "                masks = masks.to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.bce_loss(outputs, masks) + self.dice_loss(outputs, masks)\n",
        "                iou = calculate_iou(outputs, masks, self.config.pred_threshold)\n",
        "\n",
        "                self._update_metrics(\n",
        "                    val_metrics, {\"loss\": loss.item(), \"iou\": iou}\n",
        "                )\n",
        "\n",
        "        return {\n",
        "            \"val_loss\": val_metrics[\"loss\"] / len(val_loader),\n",
        "            \"val_iou\": val_metrics[\"iou\"] / len(val_loader),\n",
        "        }\n",
        "\n",
        "    def _log_metrics(\n",
        "        self,\n",
        "        epoch: int,\n",
        "        train_metrics: Dict[str, float],\n",
        "        val_metrics: Dict[str, float],\n",
        "    ):\n",
        "        \"\"\"Log metrics to WandB\"\"\"\n",
        "        metrics = {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_metrics[\"loss\"],\n",
        "            \"train_iou\": train_metrics[\"iou\"],\n",
        "            \"val_loss\": val_metrics[\"val_loss\"],\n",
        "            \"val_iou\": val_metrics[\"val_iou\"],\n",
        "            \"learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n",
        "        }\n",
        "        wandb.log(metrics)\n",
        "        print(f\"Epoch {epoch}:\", metrics)\n",
        "\n",
        "    def train(\n",
        "        self, train_loader: DataLoader, val_loader: DataLoader\n",
        "    ) -> nn.Module:\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        best_val_iou = 0\n",
        "\n",
        "        for epoch in range(self.config.epochs):\n",
        "            # Training phase\n",
        "            train_metrics = self._train_epoch(train_loader)\n",
        "\n",
        "            # Validation phase\n",
        "            val_metrics = self._validate_epoch(val_loader)\n",
        "\n",
        "            # Update scheduler\n",
        "            self.scheduler.step(val_metrics[\"val_loss\"])\n",
        "\n",
        "            # Log metrics\n",
        "            self._log_metrics(epoch, train_metrics, val_metrics)\n",
        "\n",
        "            # Save best model\n",
        "            if val_metrics[\"val_iou\"] > best_val_iou:\n",
        "                best_val_iou = val_metrics[\"val_iou\"]\n",
        "                self._save_model(\"best_model.pth\")\n",
        "\n",
        "        self.run.finish()\n",
        "        return self.model\n",
        "\n",
        "    def _train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:\n",
        "        \"\"\"Training loop for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        epoch_metrics = self._init_metrics()\n",
        "\n",
        "        for step, (inputs, masks) in enumerate(tqdm(train_loader)):\n",
        "            batch_metrics = self._train_step(inputs, masks)\n",
        "            self._update_metrics(epoch_metrics, batch_metrics)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                wandb.log(\n",
        "                    {\n",
        "                        \"train_batch_loss\": batch_metrics[\"loss\"],\n",
        "                        \"train_batch_iou\": batch_metrics[\"iou\"],\n",
        "                        \"learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        return self._finalize_metrics(epoch_metrics, len(train_loader))\n",
        "\n",
        "    def _train_step(\n",
        "        self, inputs: torch.Tensor, masks: torch.Tensor\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"Single training step\"\"\"\n",
        "        inputs = inputs.to(self.device)\n",
        "        masks = masks.to(self.device)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        loss = self.bce_loss(outputs, masks) + self.dice_loss(outputs, masks)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            iou = calculate_iou(outputs, masks, self.config.pred_threshold)\n",
        "\n",
        "        return {\"loss\": loss.item(), \"iou\": iou}\n",
        "\n",
        "    @staticmethod\n",
        "\n",
        "\n",
        "    def _save_model(self, filename: str):\n",
        "        \"\"\"Save model to WandB\"\"\"\n",
        "        path = os.path.join(wandb.run.dir, filename)\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "        artifact = wandb.Artifact('best_model', type='model')\n",
        "        artifact.add_file(path)\n",
        "        self.run.log_artifact(artifact)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzZS9Z2jfROF"
      },
      "outputs": [],
      "source": [
        "# main.py\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Initialize configuration\n",
        "    config = TrainingConfig()\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = BirdsDataset(\"data/train\", config, is_training=True)\n",
        "    val_dataset = BirdsDataset(\"data/val\", config, is_training=False)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=config.batch_size, shuffle=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=config.batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model = UNet(config).to(config.device)\n",
        "\n",
        "    # Initialize trainer and train\n",
        "    trainer = Trainer(model, config)\n",
        "    trained_model = trainer.train(train_loader, val_loader)\n",
        "\n",
        "    return trained_model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWKD09whySKA"
      },
      "source": [
        "You can also experiment with models and write a small report about results. If the report will be meaningful, you will receive an extra point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCHacSHutHo4"
      },
      "source": [
        "### Testing (8 points)\n",
        "Your model will be tested on the new data, similar to validation, so use techniques to prevent overfitting the model.\n",
        "\n",
        "* IoU > 0.85 — 8 points\n",
        "* IoU > 0.80 — 7 points\n",
        "* IoU > 0.75 — 6 points\n",
        "* IoU > 0.70 — 5 points\n",
        "* IoU > 0.60 — 4 points\n",
        "* IoU > 0.50 — 3 points\n",
        "* IoU > 0.40 — 2 points\n",
        "* IoU > 0.30 — 1 points"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(config):\n",
        "    wandb.init(project=\"bird-segmentation\", entity=\"team-aspisov\")\n",
        "    artifact = wandb.use_artifact('model:v1')\n",
        "    artifact_dir = artifact.download()\n",
        "    model_path = f\"{artifact_dir}/best_model.pth\"\n",
        "    model = UNet(config)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    return model"
      ],
      "metadata": {
        "id": "VyiCBO4Mf8Rm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DZ6h11Q0tUHN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "collapsed": true,
        "outputId": "80dab952-d5f1-4e46-fe5d-021c5c4137de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241027_201846-rpsgbkla</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/team-aspisov/bird-segmentation/runs/rpsgbkla' target=\"_blank\">earnest-frost-33</a></strong> to <a href='https://wandb.ai/team-aspisov/bird-segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/team-aspisov/bird-segmentation' target=\"_blank\">https://wandb.ai/team-aspisov/bird-segmentation</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/team-aspisov/bird-segmentation/runs/rpsgbkla' target=\"_blank\">https://wandb.ai/team-aspisov/bird-segmentation/runs/rpsgbkla</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model:v1, 188.33MB. 1 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "Done. 0:0:7.7\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 174MB/s]\n",
            "<ipython-input-26-414d8d178fd9>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        }
      ],
      "source": [
        "config = TrainingConfig()\n",
        "\n",
        "model = get_model(config).to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, img_path):\n",
        "    transform = A.Compose([\n",
        "        A.Resize(256, 256),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    transformed = transform(image=img)\n",
        "    input_tensor = transformed['image'].unsqueeze(0)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        return model(input_tensor.to(config.device))"
      ],
      "metadata": {
        "id": "OUTATt0yon5X"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "yV9zadusfROF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "382d1986-5eea-45eb-f98e-5845ece78852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:30<00:00,  6.46it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 0.01816144233747024)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "ious, times = [], []\n",
        "test_dir = 'data/val/'\n",
        "\n",
        "for class_name in tqdm(sorted(os.listdir(os.path.join(test_dir, 'images')))):\n",
        "    for img_name in sorted(os.listdir(os.path.join(test_dir, 'images', class_name))):\n",
        "\n",
        "        t_start = time()\n",
        "        pred = predict(model, os.path.join(test_dir, 'images', class_name, img_name))\n",
        "        times.append(time() - t_start)\n",
        "\n",
        "        gt_name = img_name.replace('jpg', 'png')\n",
        "        gt = np.asarray(Image.open(os.path.join(test_dir, 'gt', class_name, gt_name)), dtype = np.uint8)\n",
        "        if len(gt.shape) > 2:\n",
        "            gt = gt[:, :, 0]\n",
        "        transform = A.Compose([\n",
        "            A.Resize(256, 256),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "        gt = transform(image=gt)['image'].unsqueeze(0).to(config.device)\n",
        "\n",
        "        iou = calculate_iou(gt==255, pred, config.pred_threshold)\n",
        "        ious.append(iou)\n",
        "\n",
        "np.mean(ious), np.mean(times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47KgrqdpvKWS"
      },
      "source": [
        "### Compression (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kJiLB__vTC3"
      },
      "source": [
        "Try to speed up the model in any way without losing more than 1% in iou score.\n",
        "For example [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQyNHbt0vtMu"
      },
      "outputs": [],
      "source": [
        "def get_fast_model():\n",
        "    # YOUR CODE HERE\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2DedST0v6aF"
      },
      "outputs": [],
      "source": [
        "fast_model = get_fast_model().to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryWUekS2vlv8"
      },
      "outputs": [],
      "source": [
        "ious, times = [], []\n",
        "test_dir = 'data/val/'\n",
        "\n",
        "for class_name in tqdm(sorted(os.listdir(os.path.join(test_dir, 'images')))):\n",
        "    for img_name in sorted(os.listdir(os.path.join(test_dir, 'images', class_name))):\n",
        "\n",
        "        t_start = time()\n",
        "        pred = predict(fast_model, os.path.join(test_dir, 'images', class_name, img_name))\n",
        "        times.append(time() - t_start)\n",
        "\n",
        "        gt_name = img_name.replace('jpg', 'png')\n",
        "        gt = np.asarray(Image.open(os.path.join(test_dir, 'gt', class_name, gt_name)), dtype = np.uint8)\n",
        "        if len(gt.shape) > 2:\n",
        "            gt = gt[:, :, 0]\n",
        "\n",
        "        iou = get_iou(gt==255, pred>0.5)\n",
        "        ious.append(iou)\n",
        "\n",
        "np.mean(ious), np.mean(times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCdMgBoOwXAb"
      },
      "source": [
        "**Bonus:** For the best iou score on test(without compression) in group you will get 1.5, 1, 0.5 extra points(for 1st, 2nd, 3rd places)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daanikNkwo5t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}