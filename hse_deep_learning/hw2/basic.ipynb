{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsZpFIaRfROD",
        "outputId": "70e4f8df-3414-48f4-fea6-76ae6d7cb13a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.20 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9HlqnlYoUJM"
      },
      "source": [
        "### Get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "AYTvLpTFfR9L",
        "outputId": "393866cb-2548-432e-fa6c-7e290faff015"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=10f1H2T-5W-BiqabHHtlZ4ASs19TZmg8R\n",
            "From (redirected): https://drive.google.com/uc?id=10f1H2T-5W-BiqabHHtlZ4ASs19TZmg8R&confirm=t&uuid=11782639-53e8-4b19-a639-1b9d366a401d\n",
            "To: /content/data.zip\n",
            "100%|██████████| 979M/979M [00:19<00:00, 49.5MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'data.zip'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=10f1H2T-5W-BiqabHHtlZ4ASs19TZmg8R'\n",
        "output = 'data.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "!unzip data.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1g03B9mtZeb"
      },
      "source": [
        "### Utilities (0.5 point)\n",
        "\n",
        "Complete dataset to load prepared images and masks. Don't forget to use augmentations.\n",
        "\n",
        "Some of the images are 1 channels, so use `gray2rgb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2RjELs3Aalwo"
      },
      "outputs": [],
      "source": [
        "# config.py\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Optional\n",
        "import torch\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    # Training parameters\n",
        "    epochs: int = 15\n",
        "    batch_size: int = 8\n",
        "    learning_rate: float = 1e-4\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Dataset parameters\n",
        "    input_size: Tuple[int, int] = (256, 256)\n",
        "    num_classes: int = 1\n",
        "\n",
        "    # Optimizer parameters\n",
        "    weight_decay: float = 0.01\n",
        "    scheduler_patience: int = 2\n",
        "    scheduler_factor: float = 0.1\n",
        "\n",
        "    # WandB parameters\n",
        "    project_name: str = \"bird-segmentation\"\n",
        "    run_name: Optional[str] = None\n",
        "\n",
        "    # Data augmentation parameters\n",
        "    normalize_mean: Tuple[float, float, float] = (0.485, 0.456, 0.406)\n",
        "    normalize_std: Tuple[float, float, float] = (0.229, 0.224, 0.225)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "YT2QUTqFooxJ"
      },
      "outputs": [],
      "source": [
        "# dataset.py\n",
        "import cv2\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from typing import Tuple, Optional\n",
        "import torch\n",
        "\n",
        "\n",
        "class BirdsDataset(Dataset):\n",
        "    \"\"\"Dataset class for bird segmentation\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        folder: str,\n",
        "        config: TrainingConfig,\n",
        "        transform: Optional[A.Compose] = None,\n",
        "    ) -> None:\n",
        "        self.image_paths, self.mask_paths = self._get_paths(folder)\n",
        "        self.transform = transform or self._get_default_transforms(config)\n",
        "\n",
        "    def _get_paths(self, folder: str) -> Tuple[list, list]:\n",
        "        \"\"\"Get paths for images and masks\"\"\"\n",
        "        images_folder = os.path.join(folder, \"images\")\n",
        "        gt_folder = os.path.join(folder, \"gt\")\n",
        "\n",
        "        image_paths = []\n",
        "        mask_paths = []\n",
        "\n",
        "        for class_name in os.listdir(images_folder):\n",
        "            class_folder = os.path.join(images_folder, class_name)\n",
        "            if os.path.isdir(class_folder):\n",
        "                for fname in os.listdir(class_folder):\n",
        "                    image_paths.append(os.path.join(class_folder, fname))\n",
        "                    mask_paths.append(\n",
        "                        os.path.join(gt_folder, class_name, fname[:-3] + \"png\")\n",
        "                    )\n",
        "\n",
        "        return image_paths, mask_paths\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_default_transforms(config: TrainingConfig) -> A.Compose:\n",
        "        \"\"\"Get default augmentation pipeline\"\"\"\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(*config.input_size),\n",
        "                A.Normalize(\n",
        "                    mean=config.normalize_mean, std=config.normalize_std\n",
        "                ),\n",
        "                ToTensorV2(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Get a sample from the dataset\"\"\"\n",
        "        # Load images\n",
        "        img = cv2.imread(self.image_paths[index])\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(self.mask_paths[index], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Apply transforms\n",
        "        transformed = self.transform(image=img, mask=mask)\n",
        "\n",
        "        return (\n",
        "            transformed[\"image\"],\n",
        "            transformed[\"mask\"].float().unsqueeze(0) / 255.0,\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_paths)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dss-ZnpTuI1V"
      },
      "source": [
        "### Architecture (1 point)\n",
        "Your task for today is to build your own Unet to solve the segmentation problem.\n",
        "\n",
        "As an encoder, you can use pre-trained on IMAGENET models(or parts) from torchvision. The decoder must be trained from scratch.\n",
        "It is forbidden to use data not from the `data` folder.\n",
        "\n",
        "I advise you to experiment with the number of blocks so as not to overfit on the training sample and get good quality on validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "_Elr1Uw3uITD"
      },
      "outputs": [],
      "source": [
        "# model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"Decoder block for U-Net architecture\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, skip_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.upconv = nn.ConvTranspose2d(\n",
        "            in_channels, out_channels, kernel_size=2, stride=2\n",
        "        )\n",
        "\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(out_channels + skip_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, skip: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.upconv(x)\n",
        "        x = self._handle_size_mismatch(x, skip)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        return self.conv_block(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def _handle_size_mismatch(\n",
        "        x: torch.Tensor, skip: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Handle size mismatch between decoder and skip connection features\"\"\"\n",
        "        if x.size() != skip.size():\n",
        "            diff_h = skip.size()[2] - x.size()[2]\n",
        "            diff_w = skip.size()[3] - x.size()[3]\n",
        "            x = F.pad(\n",
        "                x,\n",
        "                [\n",
        "                    diff_w // 2,\n",
        "                    diff_w - diff_w // 2,\n",
        "                    diff_h // 2,\n",
        "                    diff_h - diff_h // 2,\n",
        "                ],\n",
        "            )\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"U-Net architecture with ResNet34 encoder\"\"\"\n",
        "\n",
        "    def __init__(self, config: TrainingConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Initialize ResNet encoder\n",
        "        resnet = torchvision.models.resnet34(\n",
        "            weights=torchvision.models.ResNet34_Weights.DEFAULT\n",
        "        )\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.encoder2 = resnet.layer1  # 64 channels\n",
        "        self.encoder3 = resnet.layer2  # 128 channels\n",
        "        self.encoder4 = resnet.layer3  # 256 channels\n",
        "        self.encoder5 = resnet.layer4  # 512 channels\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder4 = DecoderBlock(512, 256, 256)\n",
        "        self.decoder3 = DecoderBlock(256, 128, 128)\n",
        "        self.decoder2 = DecoderBlock(128, 64, 64)\n",
        "        self.decoder1 = DecoderBlock(64, 64, 32)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(32, config.num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Encoder\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool(enc1))\n",
        "        enc3 = self.encoder3(enc2)\n",
        "        enc4 = self.encoder4(enc3)\n",
        "        enc5 = self.encoder5(enc4)\n",
        "\n",
        "        # Decoder\n",
        "        dec4 = self.decoder4(enc5, enc4)\n",
        "        dec3 = self.decoder3(dec4, enc3)\n",
        "        dec2 = self.decoder2(dec3, enc2)\n",
        "        dec1 = self.decoder1(dec2, enc1)\n",
        "\n",
        "        # Final output\n",
        "        out = self.final_conv(dec1)\n",
        "        return F.interpolate(\n",
        "            out, size=x.shape[2:], mode=\"bilinear\", align_corners=False\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sq4WwZsuMeD"
      },
      "source": [
        "### Train script (0.5 point)\n",
        "\n",
        "Complete the train and predict scripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "d_ha44iifROE"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Dict\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"Training class for U-Net model\"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, config: TrainingConfig):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.device = config.device\n",
        "\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config.learning_rate,\n",
        "            weight_decay=config.weight_decay,\n",
        "        )\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode=\"min\",\n",
        "            patience=config.scheduler_patience,\n",
        "            factor=config.scheduler_factor,\n",
        "        )\n",
        "\n",
        "        self._setup_wandb()\n",
        "\n",
        "    def _setup_wandb(self):\n",
        "        \"\"\"Initialize WandB run\"\"\"\n",
        "        self.run = wandb.init(\n",
        "            project=self.config.project_name,\n",
        "            name=self.config.run_name,\n",
        "            config=self.config,\n",
        "        )\n",
        "        wandb.watch(self.model, self.criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    def _init_metrics(self) -> Dict[str, float]:\n",
        "        \"\"\"Initialize metrics dictionary\"\"\"\n",
        "        return {\n",
        "            \"loss\": 0.0,\n",
        "            \"iou\": 0.0,\n",
        "            \"count\": 0,\n",
        "        }\n",
        "\n",
        "    def _update_metrics(\n",
        "        self, epoch_metrics: Dict[str, float], batch_metrics: Dict[str, float]\n",
        "    ):\n",
        "        \"\"\"Update epoch metrics with batch metrics\"\"\"\n",
        "        epoch_metrics[\"loss\"] += batch_metrics[\"loss\"]\n",
        "        epoch_metrics[\"iou\"] += batch_metrics[\"iou\"]\n",
        "        epoch_metrics[\"count\"] += 1\n",
        "\n",
        "    def _finalize_metrics(\n",
        "        self, epoch_metrics: Dict[str, float], num_batches: int\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"Calculate final metrics for the epoch\"\"\"\n",
        "        return {\n",
        "            \"loss\": epoch_metrics[\"loss\"] / num_batches,\n",
        "            \"iou\": epoch_metrics[\"iou\"] / num_batches,\n",
        "        }\n",
        "\n",
        "    def _validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:\n",
        "        \"\"\"Validation loop for one epoch\"\"\"\n",
        "        self.model.eval()\n",
        "        val_metrics = self._init_metrics()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, masks in tqdm(val_loader):\n",
        "                inputs = inputs.to(self.device)\n",
        "                masks = masks.to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, masks)\n",
        "                iou = self._calculate_iou(outputs, masks)\n",
        "\n",
        "                self._update_metrics(\n",
        "                    val_metrics, {\"loss\": loss.item(), \"iou\": iou}\n",
        "                )\n",
        "\n",
        "        return {\n",
        "            \"val_loss\": val_metrics[\"loss\"] / len(val_loader),\n",
        "            \"val_iou\": val_metrics[\"iou\"] / len(val_loader),\n",
        "        }\n",
        "\n",
        "    def _log_metrics(\n",
        "        self,\n",
        "        epoch: int,\n",
        "        train_metrics: Dict[str, float],\n",
        "        val_metrics: Dict[str, float],\n",
        "    ):\n",
        "        \"\"\"Log metrics to WandB\"\"\"\n",
        "        metrics = {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_metrics[\"loss\"],\n",
        "            \"train_iou\": train_metrics[\"iou\"],\n",
        "            \"val_loss\": val_metrics[\"val_loss\"],\n",
        "            \"val_iou\": val_metrics[\"val_iou\"],\n",
        "            \"learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n",
        "        }\n",
        "        wandb.log(metrics)\n",
        "        print(f\"Epoch {epoch}:\", metrics)\n",
        "\n",
        "    def train(\n",
        "        self, train_loader: DataLoader, val_loader: DataLoader\n",
        "    ) -> nn.Module:\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        best_val_iou = 0\n",
        "\n",
        "        for epoch in range(self.config.epochs):\n",
        "            # Training phase\n",
        "            train_metrics = self._train_epoch(train_loader)\n",
        "\n",
        "            # Validation phase\n",
        "            val_metrics = self._validate_epoch(val_loader)\n",
        "\n",
        "            # Update scheduler\n",
        "            self.scheduler.step(val_metrics[\"val_loss\"])\n",
        "\n",
        "            # Log metrics\n",
        "            self._log_metrics(epoch, train_metrics, val_metrics)\n",
        "\n",
        "            # Save best model\n",
        "            if val_metrics[\"val_iou\"] > best_val_iou:\n",
        "                best_val_loss = val_metrics[\"val_iou\"]\n",
        "                self._save_model(\"best_model.pth\")\n",
        "\n",
        "        self.run.finish()\n",
        "        return self.model\n",
        "\n",
        "    def _train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:\n",
        "        \"\"\"Training loop for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        epoch_metrics = self._init_metrics()\n",
        "\n",
        "        for step, (inputs, masks) in enumerate(tqdm(train_loader)):\n",
        "            batch_metrics = self._train_step(inputs, masks)\n",
        "            self._update_metrics(epoch_metrics, batch_metrics)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                wandb.log(\n",
        "                    {\n",
        "                        \"train_batch_loss\": batch_metrics[\"loss\"],\n",
        "                        \"train_batch_iou\": batch_metrics[\"iou\"],\n",
        "                        \"learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        return self._finalize_metrics(epoch_metrics, len(train_loader))\n",
        "\n",
        "    def _train_step(\n",
        "        self, inputs: torch.Tensor, masks: torch.Tensor\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"Single training step\"\"\"\n",
        "        inputs = inputs.to(self.device)\n",
        "        masks = masks.to(self.device)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        loss = self.criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            iou = self._calculate_iou(outputs, masks)\n",
        "\n",
        "        return {\"loss\": loss.item(), \"iou\": iou}\n",
        "\n",
        "    @staticmethod\n",
        "    def _calculate_iou(outputs: torch.Tensor, masks: torch.Tensor) -> float:\n",
        "        \"\"\"Calculate IoU score\"\"\"\n",
        "        pred = (torch.sigmoid(outputs) > 0.5)\n",
        "        intersection = (pred & masks.bool()).float().sum((1, 2, 3))\n",
        "        union = (pred | masks.bool()).float().sum((1, 2, 3))\n",
        "        iou = (intersection + 1e-6) / (union + 1e-6)\n",
        "        return iou.mean().item()\n",
        "\n",
        "    def _save_model(self, filename: str):\n",
        "        \"\"\"Save model to WandB\"\"\"\n",
        "        path = os.path.join(wandb.run.dir, filename)\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "        wandb.save(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "96EkIQmutpdS"
      },
      "outputs": [],
      "source": [
        "def predict(model, img_path):\n",
        "    transform = A.Compose([\n",
        "        A.Resize(256, 256),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    transformed = transform(image=img)\n",
        "    input_tensor = transformed['image'].unsqueeze(0)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        pred = torch.sigmoid(output)\n",
        "        pred = (pred > 0.5).float()\n",
        "\n",
        "    # Resize prediction back to original size\n",
        "    pred = F.interpolate(pred, size=(img.shape[0], img.shape[1]), mode='bilinear', align_corners=False)\n",
        "    pred = pred.squeeze().numpy()\n",
        "\n",
        "    return pred\n",
        "\n",
        "def get_model(path):\n",
        "    model = UNet()\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "LzZS9Z2jfROF",
        "outputId": "2abe1a0c-2790-4426-a551-c8c2faa94486"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:pvwpdfiq) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lemon-snowball-3</strong> at: <a href='https://wandb.ai/team-aspisov/bird-segmentation/runs/pvwpdfiq' target=\"_blank\">https://wandb.ai/team-aspisov/bird-segmentation/runs/pvwpdfiq</a><br/> View project at: <a href='https://wandb.ai/team-aspisov/bird-segmentation' target=\"_blank\">https://wandb.ai/team-aspisov/bird-segmentation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241025_113103-pvwpdfiq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:pvwpdfiq). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241025_113317-cmem22mh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/team-aspisov/bird-segmentation/runs/cmem22mh' target=\"_blank\">logical-smoke-4</a></strong> to <a href='https://wandb.ai/team-aspisov/bird-segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/team-aspisov/bird-segmentation' target=\"_blank\">https://wandb.ai/team-aspisov/bird-segmentation</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/team-aspisov/bird-segmentation/runs/cmem22mh' target=\"_blank\">https://wandb.ai/team-aspisov/bird-segmentation/runs/cmem22mh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1048/1048 [03:40<00:00,  4.74it/s]\n",
            "100%|██████████| 176/176 [00:24<00:00,  7.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: {'epoch': 0, 'train_loss': 0.3121429327733189, 'train_iou': 0.695910915974101, 'val_loss': 0.19234024530107324, 'val_iou': 0.7366907467896288, 'learning_rate': 0.0001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "100%|██████████| 1048/1048 [03:29<00:00,  4.99it/s]\n",
            "100%|██████████| 176/176 [00:21<00:00,  8.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: {'epoch': 1, 'train_loss': 0.1465482073587202, 'train_iou': 0.7452046153995827, 'val_loss': 0.1216924786567688, 'val_iou': 0.7555427889932286, 'learning_rate': 0.0001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1048/1048 [03:28<00:00,  5.02it/s]\n",
            "100%|██████████| 176/176 [00:21<00:00,  8.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: {'epoch': 2, 'train_loss': 0.09958001649902977, 'train_iou': 0.7503941268980048, 'val_loss': 0.1069384604184465, 'val_iou': 0.7558964450250972, 'learning_rate': 0.0001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1048/1048 [03:26<00:00,  5.07it/s]\n",
            "100%|██████████| 176/176 [00:24<00:00,  7.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: {'epoch': 3, 'train_loss': 0.08068916497695196, 'train_iou': 0.757062911759806, 'val_loss': 0.08174087467010725, 'val_iou': 0.7483179725029252, 'learning_rate': 0.0001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1048/1048 [03:25<00:00,  5.10it/s]\n",
            "100%|██████████| 176/176 [00:20<00:00,  8.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: {'epoch': 4, 'train_loss': 0.07106913024033526, 'train_iou': 0.7638048601287012, 'val_loss': 0.08236916187557984, 'val_iou': 0.7173599130050703, 'learning_rate': 0.0001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 170/1048 [00:32<06:02,  2.42it/s]"
          ]
        }
      ],
      "source": [
        "# main.py\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Initialize configuration\n",
        "    config = TrainingConfig()\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = BirdsDataset(\"data/train\", config)\n",
        "    val_dataset = BirdsDataset(\"data/val\", config)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=config.batch_size, shuffle=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=config.batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model = UNet(config).to(config.device)\n",
        "\n",
        "    # Initialize trainer and train\n",
        "    trainer = Trainer(model, config)\n",
        "    trained_model = trainer.train(train_loader, val_loader)\n",
        "\n",
        "    return trained_model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWKD09whySKA"
      },
      "source": [
        "You can also experiment with models and write a small report about results. If the report will be meaningful, you will receive an extra point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCHacSHutHo4"
      },
      "source": [
        "### Testing (8 points)\n",
        "Your model will be tested on the new data, similar to validation, so use techniques to prevent overfitting the model.\n",
        "\n",
        "* IoU > 0.85 — 8 points\n",
        "* IoU > 0.80 — 7 points\n",
        "* IoU > 0.75 — 6 points\n",
        "* IoU > 0.70 — 5 points\n",
        "* IoU > 0.60 — 4 points\n",
        "* IoU > 0.50 — 3 points\n",
        "* IoU > 0.40 — 2 points\n",
        "* IoU > 0.30 — 1 points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ6h11Q0tUHN"
      },
      "outputs": [],
      "source": [
        "model = get_model('model_14.pth').to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV9zadusfROF"
      },
      "outputs": [],
      "source": [
        "ious, times = [], []\n",
        "test_dir = 'data/val/'\n",
        "\n",
        "for class_name in tqdm(sorted(os.listdir(os.path.join(test_dir, 'images')))):\n",
        "    for img_name in sorted(os.listdir(os.path.join(test_dir, 'images', class_name))):\n",
        "\n",
        "        t_start = time()\n",
        "        pred = predict(model, os.path.join(test_dir, 'images', class_name, img_name))\n",
        "        times.append(time() - t_start)\n",
        "\n",
        "        gt_name = img_name.replace('jpg', 'png')\n",
        "        gt = np.asarray(Image.open(os.path.join(test_dir, 'gt', class_name, gt_name)), dtype = np.uint8)\n",
        "        if len(gt.shape) > 2:\n",
        "            gt = gt[:, :, 0]\n",
        "\n",
        "        iou = get_iou(gt==255, pred>0.5)\n",
        "        ious.append(iou)\n",
        "\n",
        "np.mean(ious), np.mean(times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47KgrqdpvKWS"
      },
      "source": [
        "### Compression (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kJiLB__vTC3"
      },
      "source": [
        "Try to speed up the model in any way without losing more than 1% in iou score.\n",
        "For example [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQyNHbt0vtMu"
      },
      "outputs": [],
      "source": [
        "def get_fast_model():\n",
        "    # YOUR CODE HERE\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2DedST0v6aF"
      },
      "outputs": [],
      "source": [
        "fast_model = get_fast_model().to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryWUekS2vlv8"
      },
      "outputs": [],
      "source": [
        "ious, times = [], []\n",
        "test_dir = 'data/val/'\n",
        "\n",
        "for class_name in tqdm(sorted(os.listdir(os.path.join(test_dir, 'images')))):\n",
        "    for img_name in sorted(os.listdir(os.path.join(test_dir, 'images', class_name))):\n",
        "\n",
        "        t_start = time()\n",
        "        pred = predict(fast_model, os.path.join(test_dir, 'images', class_name, img_name))\n",
        "        times.append(time() - t_start)\n",
        "\n",
        "        gt_name = img_name.replace('jpg', 'png')\n",
        "        gt = np.asarray(Image.open(os.path.join(test_dir, 'gt', class_name, gt_name)), dtype = np.uint8)\n",
        "        if len(gt.shape) > 2:\n",
        "            gt = gt[:, :, 0]\n",
        "\n",
        "        iou = get_iou(gt==255, pred>0.5)\n",
        "        ious.append(iou)\n",
        "\n",
        "np.mean(ious), np.mean(times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCdMgBoOwXAb"
      },
      "source": [
        "**Bonus:** For the best iou score on test(without compression) in group you will get 1.5, 1, 0.5 extra points(for 1st, 2nd, 3rd places)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daanikNkwo5t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
