{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework (10 points)\n",
    "\n",
    "In this homework we train Sound Event Detection model.\n",
    "\n",
    "Dataset: https://disk.yandex.ru/d/NRpDIp4jg2ODqg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import tqdm.notebook as tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data\n",
    "import torchaudio\n",
    "import urllib\n",
    "\n",
    "# realization of Dataset for given data\n",
    "import dataset\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-17 19:04:21--  https://downloader.disk.yandex.ru/disk/1d04dfa5f0d8f20b3393f2a50f9fdccbf44c4a3e2f42342fca9559bb26c5d2d3/65d11145/gtj3WQiuHGabqHv6W0pVHNHKJOSrL1Ndqw5DdKyJakWvM0_u-aVydaSJ9SFBTCXqkDf5pEJT-jL3E5RuN55B9A%3D%3D?uid=0&filename=sound_event_detection.tar.gz&disposition=attachment&hash=cFvndBq/xOwr18kbjbvCR1M9L3Wj1pzfVE929UmDTjP2u%2BA0B6mbXpTOy5mLcun8q/J6bpmRyOJonT3VoXnDag%3D%3D%3A&limit=0&content_type=application%2Fx-gzip&owner_uid=163052607&fsize=10308787085&hid=51bb9b7fb344746dbead9fd66c87e539&media_type=compressed&tknv=v2\n",
      "Resolving downloader.disk.yandex.ru (downloader.disk.yandex.ru)... 77.88.21.127\n",
      "Connecting to downloader.disk.yandex.ru (downloader.disk.yandex.ru)|77.88.21.127|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://s222vlx.storage.yandex.net/rdisk/1d04dfa5f0d8f20b3393f2a50f9fdccbf44c4a3e2f42342fca9559bb26c5d2d3/65d11145/gtj3WQiuHGabqHv6W0pVHNHKJOSrL1Ndqw5DdKyJakWvM0_u-aVydaSJ9SFBTCXqkDf5pEJT-jL3E5RuN55B9A==?uid=0&filename=sound_event_detection.tar.gz&disposition=attachment&hash=cFvndBq/xOwr18kbjbvCR1M9L3Wj1pzfVE929UmDTjP2u%2BA0B6mbXpTOy5mLcun8q/J6bpmRyOJonT3VoXnDag%3D%3D%3A&limit=0&content_type=application%2Fx-gzip&owner_uid=163052607&fsize=10308787085&hid=51bb9b7fb344746dbead9fd66c87e539&media_type=compressed&tknv=v2&rtoken=GzAyIwxc47Tu&force_default=no&ycrid=na-9553120885d20671ffafbff40c8072e4-downloader3h&ts=611995dc31b40&s=ff15d9219dd560981052a5d412d1ae8d4f44fe225add72f0a52e5d9dd265b8c7&pb=U2FsdGVkX18wS1PHgBByciE0v7BBAaaoP5ETe2N0tlFOoV87VD8ejj61jhA3UPWTF0JJBjr3fxN44debN4vqFp66sMoYMj3lksjDo7REZWY [following]\n",
      "--2024-02-17 19:04:21--  https://s222vlx.storage.yandex.net/rdisk/1d04dfa5f0d8f20b3393f2a50f9fdccbf44c4a3e2f42342fca9559bb26c5d2d3/65d11145/gtj3WQiuHGabqHv6W0pVHNHKJOSrL1Ndqw5DdKyJakWvM0_u-aVydaSJ9SFBTCXqkDf5pEJT-jL3E5RuN55B9A==?uid=0&filename=sound_event_detection.tar.gz&disposition=attachment&hash=cFvndBq/xOwr18kbjbvCR1M9L3Wj1pzfVE929UmDTjP2u%2BA0B6mbXpTOy5mLcun8q/J6bpmRyOJonT3VoXnDag%3D%3D%3A&limit=0&content_type=application%2Fx-gzip&owner_uid=163052607&fsize=10308787085&hid=51bb9b7fb344746dbead9fd66c87e539&media_type=compressed&tknv=v2&rtoken=GzAyIwxc47Tu&force_default=no&ycrid=na-9553120885d20671ffafbff40c8072e4-downloader3h&ts=611995dc31b40&s=ff15d9219dd560981052a5d412d1ae8d4f44fe225add72f0a52e5d9dd265b8c7&pb=U2FsdGVkX18wS1PHgBByciE0v7BBAaaoP5ETe2N0tlFOoV87VD8ejj61jhA3UPWTF0JJBjr3fxN44debN4vqFp66sMoYMj3lksjDo7REZWY\n",
      "Resolving s222vlx.storage.yandex.net (s222vlx.storage.yandex.net)... 5.45.238.30\n",
      "Connecting to s222vlx.storage.yandex.net (s222vlx.storage.yandex.net)|5.45.238.30|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10308787085 (9.6G) [application/x-gzip]\n",
      "Saving to: ‘data.tar.gz’\n",
      "\n",
      "data.tar.gz         100%[===================>]   9.60G  5.07MB/s    in 46m 5s  \n",
      "\n",
      "2024-02-17 19:50:26 (3.56 MB/s) - ‘data.tar.gz’ saved [10308787085/10308787085]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
    "public_key = 'https://disk.yandex.ru/d/NRpDIp4jg2ODqg'\n",
    "final_url = base_url + urllib.parse.urlencode(dict(public_key=public_key))\n",
    "response = requests.get(final_url)\n",
    "download_url = response.json()['href']\n",
    "!wget -O data.tar.gz \"{download_url}\"\n",
    "!tar -xf data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu' # also you can use \"cuda\" for gpu and \"mps\" for apple silicon\n",
    "DATADIR = 'data'\n",
    "LOADER_WORKERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FBANK 80 by default, but you can choose something else\n",
    "FEATS = 80\n",
    "transform = torchaudio.transforms.MelSpectrogram(n_mels=FEATS)\n",
    "trainset = dataset.Dataset('train', 'data', transform)\n",
    "testset = dataset.Dataset('eval', 'data', transform)\n",
    "N_CLASSES = trainset.classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval part (1 point)\n",
    "\n",
    "Write balanced accuracy:\n",
    "$$BAcc = \\frac{1}{classes}\\sum_{c = 1}^{classes} \\frac{\\sum_i^n I(y_i = p_i = c)}{\\sum_i^n I(y_i = c)}$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ -- target class for $i$ element\n",
    "- $p_i$ -- predicted class for $i$ element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of pairs (target_class, predicted_class)\n",
    "def balanced_accuracy(items: list[tuple[int, int]]) -> float:\n",
    "    # <YOUR CODE IS HERE>\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(balanced_accuracy([(0, 0), (0, 0), (1, 1)]), 1.0)\n",
    "assert np.isclose(balanced_accuracy([(0, 1), (1, 0)]), 0.0)\n",
    "assert np.isclose(balanced_accuracy([(0, 0), (0, 0), (1, 0)]), 0.5)\n",
    "assert np.isclose(balanced_accuracy([(0, 0), (1, 1), (0, 0), (0, 0), (1, 0), (0, 1)]), 0.625)\n",
    "assert np.isclose(balanced_accuracy([(1, 1), (0, 1), (2, 2)]), 0.66666666666666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train part (9 points)\n",
    "\n",
    "Train some model with test accuracy > 0.5\n",
    "\n",
    "You can train any model you want. The only limitation is that it must be trained from scratch on the data provided in the task. For example you can choose model from:\n",
    "- DNN\n",
    "- CNN 1d\n",
    "- CNN 2d\n",
    "- Transformer\n",
    "- RNN\n",
    "- mixes of given models\n",
    "\n",
    "Hints:\n",
    "- No need to train large models for this task. 10 million parameters is more than you need.\n",
    "- Watch to overfitting, try to add Augmentation, Dropout, BatchNorm, L1/L2-Regulatization or something else.\n",
    "- Use poolings or strides to reduce time-dimenstion. It is better to reduce the dimension gradually rather than at the end.\n",
    "- Try different features (mel-spec, log-mel-spec, mfcc)\n",
    "\n",
    "P.S. Points can be deducted for unclear training charts\n",
    "\n",
    "PP.S. A partial score will be awarded for a test accuracy < 0.5\n",
    "\n",
    "PPP.S. Add log to Melspectrogram in torchaudio.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage(\n",
    "    model: nn.Module,\n",
    "    data: dataset.Dataset,\n",
    "    opt: optim.Optimizer,\n",
    "    batch_size: int = 256,\n",
    "    train: bool = True\n",
    "):\n",
    "    loader = torch_data.DataLoader(\n",
    "        data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=LOADER_WORKERS,\n",
    "        collate_fn=dataset.collate_fn\n",
    "    )\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    loss_sum, batches = 0.0, 0\n",
    "    pred_pairs = []\n",
    "    for X, Y in tqdm.tqdm(loader):\n",
    "        pred = model.forward(X.to(DEVICE))\n",
    "        loss = F.cross_entropy(pred.squeeze(), Y.squeeze().to(DEVICE))\n",
    "        if train:\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        loss_sum += loss.item()\n",
    "        batches += 1\n",
    "        with torch.no_grad():\n",
    "            pred_pairs.extend(zip(\n",
    "                Y.data.numpy().reshape(-1),\n",
    "                torch.argmax(pred, dim=1).cpu().data.numpy().reshape(-1)\n",
    "            ))\n",
    "    return loss_sum / batches, balanced_accuracy(pred_pairs)\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    opt,\n",
    "    batch_size: int = 256,\n",
    "    epochs: int = 10,\n",
    "):\n",
    "    train_losses, test_losses, train_accs, test_accs = [], [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = stage(model, trainset, opt, batch_size=batch_size)\n",
    "        test_loss, test_acc = stage(model, testset, opt, batch_size=batch_size, train=False)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        clear_output()\n",
    "        fig, axis = plt.subplots(1, 2, figsize=(15, 7))\n",
    "        axis[0].plot(np.arange(1, epoch + 2), train_losses, label='train')\n",
    "        axis[0].plot(np.arange(1, epoch + 2), test_losses, label='test')\n",
    "        axis[1].plot(np.arange(1, epoch + 2), train_accs, label='train')\n",
    "        axis[1].plot(np.arange(1, epoch + 2), test_accs, label='test')\n",
    "        axis[0].set(xlabel='epoch', ylabel='CE Loss')\n",
    "        axis[1].set(xlabel='epoch', ylabel='Accuracy')\n",
    "        fig.legend()\n",
    "        plt.show()\n",
    "        print(f'Epoch {epoch + 1}.')\n",
    "        print(f'Train loss {train_loss}. Train accuracy {train_acc}.')\n",
    "        print(f'Test loss {test_loss}. Test accuracy {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_dim=FEATS, out_dim=N_CLASSES):\n",
    "        super().__init__()\n",
    "        # <YOUR CODE IS HERE>\n",
    "\n",
    "    def forward(self, X):\n",
    "        # input: [batch_size, IN_FEATURES, TIME]\n",
    "        # output: [batch_size, N_CLASSES]\n",
    "        # <YOUR CODE IS HERE>\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(DEVICE)\n",
    "opt = optim.Adam(model.parameters())\n",
    "train(model, opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
