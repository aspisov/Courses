{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework (10 points)\n",
    "\n",
    "In this homework we train Sound Event Detection model.\n",
    "\n",
    "Dataset: https://disk.yandex.ru/d/NRpDIp4jg2ODqg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import tqdm.notebook as tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data\n",
    "import torchaudio\n",
    "import urllib\n",
    "\n",
    "# realization of Dataset for given data\n",
    "import dataset\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
    "# public_key = 'https://disk.yandex.ru/d/NRpDIp4jg2ODqg'\n",
    "# final_url = base_url + urllib.parse.urlencode(dict(public_key=public_key))\n",
    "# response = requests.get(final_url)\n",
    "# download_url = response.json()['href']\n",
    "# !wget -O data.tar.gz \"{download_url}\"\n",
    "# !tar -xf data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "print(f\"using {DEVICE}\")\n",
    "    \n",
    "DATADIR = 'data'\n",
    "LOADER_WORKERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FBANK 80 by default, but you can choose something else\n",
    "FEATS = 80\n",
    "transform = torchaudio.transforms.MelSpectrogram(n_mels=FEATS)\n",
    "trainset = dataset.Dataset('train', 'data', transform)\n",
    "testset = dataset.Dataset('eval', 'data', transform)\n",
    "N_CLASSES = trainset.classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 801])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[124][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval part (1 point)\n",
    "\n",
    "Write balanced accuracy:\n",
    "$$BAcc = \\frac{1}{classes}\\sum_{c = 1}^{classes} \\frac{\\sum_i^n I(y_i = p_i = c)}{\\sum_i^n I(y_i = c)}$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ -- target class for $i$ element\n",
    "- $p_i$ -- predicted class for $i$ element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of pairs (target_class, predicted_class)\n",
    "def balanced_accuracy(items: list[tuple[int, int]]) -> float:\n",
    "    classes = np.unique([item[0] for item in items])\n",
    "    acc = 0\n",
    "    for cls in classes:\n",
    "        acc += sum([item[0] == item[1] == cls for item in items]) / sum([item[0] == cls for item in items])\n",
    "    return acc / len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(balanced_accuracy([(0, 0), (0, 0), (1, 1)]), 1.0)\n",
    "assert np.isclose(balanced_accuracy([(0, 1), (1, 0)]), 0.0)\n",
    "assert np.isclose(balanced_accuracy([(0, 0), (0, 0), (1, 0)]), 0.5)\n",
    "assert np.isclose(balanced_accuracy([(0, 0), (1, 1), (0, 0), (0, 0), (1, 0), (0, 1)]), 0.625)\n",
    "assert np.isclose(balanced_accuracy([(1, 1), (0, 1), (2, 2)]), 0.66666666666666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train part (9 points)\n",
    "\n",
    "Train some model with test accuracy > 0.5\n",
    "\n",
    "You can train any model you want. The only limitation is that it must be trained from scratch on the data provided in the task. For example you can choose model from:\n",
    "- DNN\n",
    "- CNN 1d\n",
    "- CNN 2d\n",
    "- Transformer\n",
    "- RNN\n",
    "- mixes of given models\n",
    "\n",
    "Hints:\n",
    "- No need to train large models for this task. 10 million parameters is more than you need.\n",
    "- Watch to overfitting, try to add Augmentation, Dropout, BatchNorm, L1/L2-Regulatization or something else.\n",
    "- Use poolings or strides to reduce time-dimenstion. It is better to reduce the dimension gradually rather than at the end.\n",
    "- Try different features (mel-spec, log-mel-spec, mfcc)\n",
    "\n",
    "P.S. Points can be deducted for unclear training charts\n",
    "\n",
    "PP.S. A partial score will be awarded for a test accuracy < 0.5\n",
    "\n",
    "PPP.S. Add log to Melspectrogram in torchaudio.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/dimaaspisov/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install wandb\n",
    "# to begin with, you should create an account on WanbB website\n",
    "\n",
    "import wandb\n",
    "\n",
    "# this will ask your for your API key\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage(\n",
    "    model: nn.Module,\n",
    "    data: dataset.Dataset,\n",
    "    opt: optim.Optimizer,\n",
    "    batch_size: int = 256,\n",
    "    train: bool = True,\n",
    "):\n",
    "    loader = torch_data.DataLoader(\n",
    "        data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=LOADER_WORKERS,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "    )\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    loss_sum, batches = 0.0, 0\n",
    "    pred_pairs = []\n",
    "    for X, Y in tqdm.tqdm(loader):\n",
    "        pred = model.forward(X.to(DEVICE))\n",
    "        loss = F.cross_entropy(pred.squeeze(), Y.squeeze().to(DEVICE))\n",
    "        if train:\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        loss_sum += loss.item()\n",
    "        batches += 1\n",
    "        with torch.no_grad():\n",
    "            pred_pairs.extend(\n",
    "                zip(\n",
    "                    Y.data.numpy().reshape(-1),\n",
    "                    torch.argmax(pred, dim=1).cpu().data.numpy().reshape(-1),\n",
    "                )\n",
    "            )\n",
    "    return loss_sum / batches, balanced_accuracy(pred_pairs)\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    opt,\n",
    "    batch_size: int = 64,\n",
    "    epochs: int = 10,\n",
    "):\n",
    "    train_losses, test_losses, train_accs, test_accs = [], [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = stage(\n",
    "            model, trainset, opt, batch_size=batch_size\n",
    "        )\n",
    "        test_loss, test_acc = stage(\n",
    "            model, testset, opt, batch_size=batch_size, train=False\n",
    "        )\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_loss\": test_loss,\n",
    "                \"test_acc\": test_acc,\n",
    "            },\n",
    "            step=epoch + 1,\n",
    "        )\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        clear_output()\n",
    "        fig, axis = plt.subplots(1, 2, figsize=(15, 7))\n",
    "        axis[0].plot(np.arange(1, epoch + 2), train_losses, label=\"train\")\n",
    "        axis[0].plot(np.arange(1, epoch + 2), test_losses, label=\"test\")\n",
    "        axis[1].plot(np.arange(1, epoch + 2), train_accs, label=\"train\")\n",
    "        axis[1].plot(np.arange(1, epoch + 2), test_accs, label=\"test\")\n",
    "        axis[0].set(xlabel=\"epoch\", ylabel=\"CE Loss\")\n",
    "        axis[1].set(xlabel=\"epoch\", ylabel=\"Accuracy\")\n",
    "        fig.legend()\n",
    "        plt.show()\n",
    "        print(f\"Epoch {epoch + 1}.\")\n",
    "        print(f\"Train loss {train_loss}. Train accuracy {train_acc}.\")\n",
    "        print(f\"Test loss {test_loss}. Test accuracy {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SEDModel(nn.Module):\n",
    "    def __init__(self, in_dim=FEATS, out_dim=N_CLASSES, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            self._make_conv_layer(in_dim, hidden_size, 3),\n",
    "            self._make_conv_layer(hidden_size, hidden_size * 2, 3),\n",
    "            self._make_conv_layer(hidden_size * 2, hidden_size * 4, 3),\n",
    "        ])\n",
    "\n",
    "        # Adaptive pooling to handle variable input lengths\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(50)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 4 * 50, hidden_size * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(hidden_size * 4, hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(hidden_size * 2, out_dim)\n",
    "        )\n",
    "\n",
    "        # Add Temporal Attention\n",
    "        self.attention = TemporalAttention(hidden_size * 4)\n",
    "\n",
    "        print(f\"Total parameters: {self.count_parameters()/1e6:.02f}M\")\n",
    "\n",
    "    def _make_conv_layer(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, in_dim, length)\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = conv_layer(x)\n",
    "\n",
    "        # Apply attention\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Adaptive pooling\n",
    "        x = self.adaptive_pool(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        logits = self.fc_layers(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, hidden_size, time_steps)\n",
    "        attention_weights = F.softmax(self.attention(x.transpose(1, 2)).squeeze(-1), dim=1)\n",
    "        attended_x = torch.bmm(x, attention_weights.unsqueeze(-1)).squeeze(-1)\n",
    "        return attended_x.unsqueeze(-1)  # Reshape to (batch_size, hidden_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 3.49M\n"
     ]
    }
   ],
   "source": [
    "model = SEDModel().to(DEVICE)\n",
    "opt = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1df1b5bd754c84b0724f706d0f8db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011160015733265836, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/dimaaspisov/Desktop/GitHub/courses/ysda_speech/week_02_vad_sed/wandb/run-20241016_145635-p37xs4kr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/team-aspisov/ysda_speech_week02_SED/runs/p37xs4kr' target=\"_blank\">conv+attention</a></strong> to <a href='https://wandb.ai/team-aspisov/ysda_speech_week02_SED' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/team-aspisov/ysda_speech_week02_SED' target=\"_blank\">https://wandb.ai/team-aspisov/ysda_speech_week02_SED</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/team-aspisov/ysda_speech_week02_SED/runs/p37xs4kr' target=\"_blank\">https://wandb.ai/team-aspisov/ysda_speech_week02_SED/runs/p37xs4kr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d085d0ceb434f34b267d937480513b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_w/w_4qyk_x2mb0twsw_84xz5880000gn/T/ipykernel_33428/237800860.py\", line 5, in <module>\n",
      "    train(model, opt)\n",
      "  File \"/var/folders/_w/w_4qyk_x2mb0twsw_84xz5880000gn/T/ipykernel_33428/2553845526.py\", line 48, in train\n",
      "    train_loss, train_acc = stage(\n",
      "                            ^^^^^^\n",
      "  File \"/var/folders/_w/w_4qyk_x2mb0twsw_84xz5880000gn/T/ipykernel_33428/2553845526.py\", line 22, in stage\n",
      "    pred = model.forward(X.to(DEVICE))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/_w/w_4qyk_x2mb0twsw_84xz5880000gn/T/ipykernel_33428/1793830011.py\", line 56, in forward\n",
      "    x = self.adaptive_pool(x)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/pooling.py\", line 1228, in forward\n",
      "    return F.adaptive_avg_pool1d(input, self.output_size)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Adaptive pool MPS: input sizes must be divisible by output sizes. Non-divisible input sizes are not implemented on MPS device yet. For now, you can manually transfer tensor to cpu in this case. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/96056)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b7dc9ccb5a473a8cca589107e5d1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">conv+attention</strong> at: <a href='https://wandb.ai/team-aspisov/ysda_speech_week02_SED/runs/p37xs4kr' target=\"_blank\">https://wandb.ai/team-aspisov/ysda_speech_week02_SED/runs/p37xs4kr</a><br/> View project at: <a href='https://wandb.ai/team-aspisov/ysda_speech_week02_SED' target=\"_blank\">https://wandb.ai/team-aspisov/ysda_speech_week02_SED</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241016_145635-p37xs4kr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Adaptive pool MPS: input sizes must be divisible by output sizes. Non-divisible input sizes are not implemented on MPS device yet. For now, you can manually transfer tensor to cpu in this case. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/96056)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m      2\u001b[0m                 project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mysda_speech_week02_SED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                 name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv+attention\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m             ) \u001b[38;5;28;01mas\u001b[39;00m run:\n\u001b[0;32m----> 5\u001b[0m     train(model, opt)\n",
      "Cell \u001b[0;32mIn[23], line 48\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, opt, batch_size, epochs)\u001b[0m\n\u001b[1;32m     46\u001b[0m train_losses, test_losses, train_accs, test_accs \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 48\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m stage(\n\u001b[1;32m     49\u001b[0m         model, trainset, opt, batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m stage(\n\u001b[1;32m     52\u001b[0m         model, testset, opt, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     54\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m     55\u001b[0m         {\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_loss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m         step\u001b[38;5;241m=\u001b[39mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     62\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[23], line 22\u001b[0m, in \u001b[0;36mstage\u001b[0;34m(model, data, opt, batch_size, train)\u001b[0m\n\u001b[1;32m     20\u001b[0m pred_pairs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, Y \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(loader):\n\u001b[0;32m---> 22\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(X\u001b[38;5;241m.\u001b[39mto(DEVICE))\n\u001b[1;32m     23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(pred\u001b[38;5;241m.\u001b[39msqueeze(), Y\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mto(DEVICE))\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train:\n",
      "Cell \u001b[0;32mIn[32], line 56\u001b[0m, in \u001b[0;36mSEDModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(x)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Adaptive pooling\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madaptive_pool(x)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[1;32m     59\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/pooling.py:1228\u001b[0m, in \u001b[0;36mAdaptiveAvgPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39madaptive_avg_pool1d(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Adaptive pool MPS: input sizes must be divisible by output sizes. Non-divisible input sizes are not implemented on MPS device yet. For now, you can manually transfer tensor to cpu in this case. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/96056)"
     ]
    }
   ],
   "source": [
    "with wandb.init(\n",
    "                project=\"ysda_speech_week02_SED\",\n",
    "                name=\"conv+attention\"\n",
    "            ) as run:\n",
    "    train(model, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
