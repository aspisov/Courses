{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBrDXMdDy-Qn"
      },
      "source": [
        "# HSE 2024: Mathematical Methods for Data Analysis\n",
        "\n",
        "## Homework 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXi5K1mf41d"
      },
      "source": [
        "# Attention!\n",
        "\n",
        "* For tasks where <ins>text answer</ins> is required **Russian language** is **allowed**.\n",
        "* If a task asks you to describe something (make coclusions) then **text answer** is **mandatory** and **is** part of the task\n",
        "* **Do not** upload the dataset (titanic.csv) to the grading system (we already have it)\n",
        "* We **only** accept **ipynb** notebooks. If you use Google Colab then you'll have to download the notebook before passing the homework\n",
        "* **Do not** use python loops instead of NumPy vector operations over NumPy vectors - it significantly decreases performance (see why https://blog.paperspace.com/numpy-optimization-vectorization-and-broadcasting/), will be punished with -0.25 for **every** task.\n",
        "Loops are only allowed in part 1 (Tasks 1 - 4).\n",
        "* Some tasks contain tests. They only test you solution on a simple example, thus, passing the test does **not** guarantee you the full grade for the task.\n",
        "\n",
        "If the task asks for an explanation of something, it means that a written answer is required, which is part of the task and is assessed\n",
        "\n",
        "We only accept ipynb notebooks. If you use Google Colab, you need to download the notebook before submitting your homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-26T16:48:20.566549Z",
          "start_time": "2020-09-26T16:48:19.893995Z"
        },
        "id": "mSR-a9vVy-Qp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.regression.linear_model import OLSResults\n",
        "from math import sqrt\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set(style=\"darkgrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUjuv9Qty-Qq"
      },
      "source": [
        "### Data\n",
        "\n",
        "For this homework we will use a dataset of tracks from the streaming service Spotify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHSjHiIDOgHP"
      },
      "source": [
        "**Описание данных**\n",
        "\n",
        "- **track_id:** The Spotify ID for the track\n",
        "- **artists:** The artists' names who performed the track. If there is more than one artist, they are separated by a ;\n",
        "- **album_name:** The album name in which the track appears\n",
        "- **track_name:** Name of the track\n",
        "- **popularity:** The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are. Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past. Duplicate tracks (e.g. the same track from a single and an album) are rated independently. Artist and album popularity is derived mathematically from track popularity.\n",
        "- **duration_ms:** The track length in milliseconds\n",
        "- **explicit:** Whether or not the track has explicit lyrics (true = yes it does; false = no it does not OR unknown)\n",
        "- **danceability:** Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable\n",
        "- **key:** The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1\n",
        "- **loudness:** The overall loudness of a track in decibels (dB)\n",
        "- **mode:** Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0\n",
        "- **speechiness:** Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks\n",
        "- **acousticness:** A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic\n",
        "- **instrumentalness:** Predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content\n",
        "- **liveness:** Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live\n",
        "- **valence:** A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)\n",
        "- **tempo:** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration\n",
        "- **time_signature:** An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of 3/4, to 7/4.\n",
        "- **track_genre:** The genre in which the track belongs\n",
        "\n",
        "**Target variable**\n",
        "- **energy:** Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tHWSWTXDy-Qq"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('dataset.csv')\n",
        "\n",
        "y = data['energy']\n",
        "X = data.drop(['energy'], axis=1)\n",
        "columns = X.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artists</th>\n",
              "      <th>album_name</th>\n",
              "      <th>track_name</th>\n",
              "      <th>popularity</th>\n",
              "      <th>duration_ms</th>\n",
              "      <th>explicit</th>\n",
              "      <th>danceability</th>\n",
              "      <th>energy</th>\n",
              "      <th>key</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>liveness</th>\n",
              "      <th>valence</th>\n",
              "      <th>tempo</th>\n",
              "      <th>time_signature</th>\n",
              "      <th>track_genre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Gen Hoshino</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>73</td>\n",
              "      <td>230666</td>\n",
              "      <td>False</td>\n",
              "      <td>0.676</td>\n",
              "      <td>0.4610</td>\n",
              "      <td>1</td>\n",
              "      <td>-6.746</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1430</td>\n",
              "      <td>0.0322</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.3580</td>\n",
              "      <td>0.715</td>\n",
              "      <td>87.917</td>\n",
              "      <td>4</td>\n",
              "      <td>acoustic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ben Woodward</td>\n",
              "      <td>Ghost (Acoustic)</td>\n",
              "      <td>Ghost - Acoustic</td>\n",
              "      <td>55</td>\n",
              "      <td>149610</td>\n",
              "      <td>False</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.1660</td>\n",
              "      <td>1</td>\n",
              "      <td>-17.235</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0763</td>\n",
              "      <td>0.9240</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>0.267</td>\n",
              "      <td>77.489</td>\n",
              "      <td>4</td>\n",
              "      <td>acoustic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ingrid Michaelson;ZAYN</td>\n",
              "      <td>To Begin Again</td>\n",
              "      <td>To Begin Again</td>\n",
              "      <td>57</td>\n",
              "      <td>210826</td>\n",
              "      <td>False</td>\n",
              "      <td>0.438</td>\n",
              "      <td>0.3590</td>\n",
              "      <td>0</td>\n",
              "      <td>-9.734</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0557</td>\n",
              "      <td>0.2100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.1170</td>\n",
              "      <td>0.120</td>\n",
              "      <td>76.332</td>\n",
              "      <td>4</td>\n",
              "      <td>acoustic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Kina Grannis</td>\n",
              "      <td>Crazy Rich Asians (Original Motion Picture Sou...</td>\n",
              "      <td>Can't Help Falling In Love</td>\n",
              "      <td>71</td>\n",
              "      <td>201933</td>\n",
              "      <td>False</td>\n",
              "      <td>0.266</td>\n",
              "      <td>0.0596</td>\n",
              "      <td>0</td>\n",
              "      <td>-18.515</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0363</td>\n",
              "      <td>0.9050</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>0.1320</td>\n",
              "      <td>0.143</td>\n",
              "      <td>181.740</td>\n",
              "      <td>3</td>\n",
              "      <td>acoustic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Chord Overstreet</td>\n",
              "      <td>Hold On</td>\n",
              "      <td>Hold On</td>\n",
              "      <td>82</td>\n",
              "      <td>198853</td>\n",
              "      <td>False</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.4430</td>\n",
              "      <td>2</td>\n",
              "      <td>-9.681</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0526</td>\n",
              "      <td>0.4690</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0829</td>\n",
              "      <td>0.167</td>\n",
              "      <td>119.949</td>\n",
              "      <td>4</td>\n",
              "      <td>acoustic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  artists                                         album_name  \\\n",
              "0             Gen Hoshino                                             Comedy   \n",
              "1            Ben Woodward                                   Ghost (Acoustic)   \n",
              "2  Ingrid Michaelson;ZAYN                                     To Begin Again   \n",
              "3            Kina Grannis  Crazy Rich Asians (Original Motion Picture Sou...   \n",
              "4        Chord Overstreet                                            Hold On   \n",
              "\n",
              "                   track_name  popularity  duration_ms  explicit  \\\n",
              "0                      Comedy          73       230666     False   \n",
              "1            Ghost - Acoustic          55       149610     False   \n",
              "2              To Begin Again          57       210826     False   \n",
              "3  Can't Help Falling In Love          71       201933     False   \n",
              "4                     Hold On          82       198853     False   \n",
              "\n",
              "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
              "0         0.676  0.4610    1    -6.746     0       0.1430        0.0322   \n",
              "1         0.420  0.1660    1   -17.235     1       0.0763        0.9240   \n",
              "2         0.438  0.3590    0    -9.734     1       0.0557        0.2100   \n",
              "3         0.266  0.0596    0   -18.515     1       0.0363        0.9050   \n",
              "4         0.618  0.4430    2    -9.681     1       0.0526        0.4690   \n",
              "\n",
              "   instrumentalness  liveness  valence    tempo  time_signature track_genre  \n",
              "0          0.000001    0.3580    0.715   87.917               4    acoustic  \n",
              "1          0.000006    0.1010    0.267   77.489               4    acoustic  \n",
              "2          0.000000    0.1170    0.120   76.332               4    acoustic  \n",
              "3          0.000071    0.1320    0.143  181.740               3    acoustic  \n",
              "4          0.000000    0.0829    0.167  119.949               4    acoustic  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "artists             31437\n",
              "album_name          46589\n",
              "track_name          73608\n",
              "popularity            101\n",
              "duration_ms         50697\n",
              "explicit                2\n",
              "danceability         1174\n",
              "energy               2083\n",
              "key                    12\n",
              "loudness            19480\n",
              "mode                    2\n",
              "speechiness          1489\n",
              "acousticness         5061\n",
              "instrumentalness     5346\n",
              "liveness             1722\n",
              "valence              1790\n",
              "tempo               45653\n",
              "time_signature          5\n",
              "track_genre           114\n",
              "dtype: int64"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K81w8s35y-Qq"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYgEN-FMy-Qr"
      },
      "source": [
        "#### 0. [0.25 points] Code the categorical features. Explain the method you have chosen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-IrSlQaWy-Qr"
      },
      "outputs": [],
      "source": [
        "# after reading the description and looking at data these features appear to be categorial\n",
        "categorial_features = [\n",
        "    \"explicit\",\n",
        "    \"key\",\n",
        "    \"mode\",\n",
        "    \"time_signature\",\n",
        "    \"track_genre\"\n",
        "]\n",
        "# these features are also categorical, but they have too many unique values, we could fix this by using word embeddings or other NLP methods\n",
        "# however this is not the goal of this task\n",
        "nlp_features = [\"artists\", \"album_name\", \"track_name\"]\n",
        "\n",
        "# there are numerical features\n",
        "numerical_features = [\n",
        "    \"popularity\",\n",
        "    \"duration_ms\",\n",
        "    \"danceability\",\n",
        "    \"loudness\",\n",
        "    \"speechiness\",\n",
        "    \"acousticness\",\n",
        "    \"instrumentalness\",\n",
        "    \"liveness\",\n",
        "    \"valence\",\n",
        "    \"tempo\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dVwP45Gy-Qr"
      },
      "source": [
        "#### 1. [0.25 points] Split the data into train and test with a ratio of 80:20 and random_state=42."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "U7z8TIh5y-Qs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(91200, 22800)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, y = data.drop(['energy'], axis=1), data['energy']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "len(X_train), len(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7daIQRfKy-Qs",
        "tags": []
      },
      "source": [
        "#### 2. [0.75 points] Train models on train, excluding categorical features, using the StatsModels library and apply it to test; use $RMSE$ and $R^2$ as quality metrics. Try also applying linear regression implementations from sklearn:\n",
        "\n",
        "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
        "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.03$;\n",
        "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.05$\n",
        "\n",
        "Don't forget to scale your data using StandardScaler before training your models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Bkbr5iFCy-Qs"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train[numerical_features])\n",
        "X_test_scaled = scaler.transform(X_test[numerical_features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "StatsModels OLS RMSE: 0.12185169282189574\n",
            "StatsModels OLS R²: 0.7640826589205965\n"
          ]
        }
      ],
      "source": [
        "X_train_sm = sm.add_constant(X_train_scaled)\n",
        "X_test_sm = sm.add_constant(X_test_scaled)\n",
        "\n",
        "model_sm = sm.OLS(y_train, X_train_sm).fit()\n",
        "\n",
        "y_pred_sm = model_sm.predict(X_test_sm)\n",
        "\n",
        "rmse_sm = np.sqrt(mean_squared_error(y_test, y_pred_sm))\n",
        "r2_sm = r2_score(y_test, y_pred_sm)\n",
        "\n",
        "print(\"StatsModels OLS RMSE:\", rmse_sm)\n",
        "print(\"StatsModels OLS R²:\", r2_sm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear RMSE: 0.12185169282189576\n",
            "Linear R²: 0.7640826589205963\n"
          ]
        }
      ],
      "source": [
        "linear_model = LinearRegression()\n",
        "\n",
        "linear_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = linear_model.predict(X_test_scaled)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Linear RMSE:\", rmse)\n",
        "print(\"Linear R²:\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ridge RMSE: 0.12185169384565525\n",
            "Ridge R²: 0.7640826549563902\n"
          ]
        }
      ],
      "source": [
        "ridge_model = Ridge(alpha=0.03)\n",
        "\n",
        "ridge_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = ridge_model.predict(X_test_scaled)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Ridge RMSE:\", rmse)\n",
        "print(\"Ridge R²:\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lasso RMSE: 0.14798373613042762\n",
            "Lasso R²: 0.6520436982694833\n"
          ]
        }
      ],
      "source": [
        "lasso_model = Lasso(alpha=0.05)\n",
        "\n",
        "lasso_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = lasso_model.predict(X_test_scaled)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Lasso RMSE:\", rmse)\n",
        "print(\"Lasso R²:\", r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWWvUdxROgHP"
      },
      "source": [
        "#### 3. [0.25 points] Repeat the steps from the previous point, adding categorical features. Comment on the changes in the quality metrics values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-Opop8b6OgHQ"
      },
      "outputs": [],
      "source": [
        "# numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_numerical = scaler.fit_transform(X_train[numerical_features])\n",
        "X_test_numerical = scaler.transform(X_test[numerical_features])\n",
        "\n",
        "# caterorial features\n",
        "encoder = OneHotEncoder(drop='first', handle_unknown='ignore')\n",
        "X_train_categorial = encoder.fit_transform(X_train[categorial_features]).toarray()\n",
        "X_test_categorial = encoder.transform(X_test[categorial_features]).toarray()\n",
        "\n",
        "X_train_num_cat = np.concatenate([X_train_numerical, X_train_categorial], axis=1)\n",
        "X_test_num_cat = np.concatenate([X_test_numerical, X_test_categorial], axis=1)\n",
        "\n",
        "y_train = pd.DataFrame(y_train).reset_index(drop=True)\n",
        "y_test = pd.DataFrame(y_test).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "StatsModels OLS RMSE: 0.11220622836690176\n",
            "StatsModels OLS R²: 0.7999536370568916\n"
          ]
        }
      ],
      "source": [
        "X_train_sm = sm.add_constant(pd.DataFrame(X_train_num_cat).reset_index(drop=True))\n",
        "X_test_sm = sm.add_constant(pd.DataFrame(X_test_num_cat).reset_index(drop=True))\n",
        "\n",
        "model_sm = sm.OLS(y_train, X_train_sm).fit()\n",
        "\n",
        "y_pred_sm = model_sm.predict(X_test_sm)\n",
        "\n",
        "rmse_sm = np.sqrt(mean_squared_error(y_test, y_pred_sm))\n",
        "r2_sm = r2_score(y_test, y_pred_sm)\n",
        "\n",
        "print(\"StatsModels OLS RMSE:\", rmse_sm)\n",
        "print(\"StatsModels OLS R²:\", r2_sm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear RMSE: 0.11220622836690186\n",
            "Linear R²: 0.7999536370568914\n"
          ]
        }
      ],
      "source": [
        "linear_model = LinearRegression()\n",
        "\n",
        "linear_model.fit(X_train_num_cat, y_train)\n",
        "\n",
        "y_pred = linear_model.predict(X_test_num_cat)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Linear RMSE:\", rmse)\n",
        "print(\"Linear R²:\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ridge RMSE: 0.11220611751017649\n",
            "Ridge R²: 0.7999540323375114\n"
          ]
        }
      ],
      "source": [
        "ridge_model = Ridge(alpha=0.03)\n",
        "\n",
        "ridge_model.fit(X_train_num_cat, y_train)\n",
        "\n",
        "y_pred = ridge_model.predict(X_test_num_cat)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Ridge RMSE:\", rmse)\n",
        "print(\"Ridge R²:\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lasso RMSE: 0.14798373613042762\n",
            "Lasso R²: 0.6520436982694833\n"
          ]
        }
      ],
      "source": [
        "lasso_model = Lasso(alpha=0.05)\n",
        "\n",
        "lasso_model.fit(X_train_num_cat, y_train)\n",
        "\n",
        "y_pred = lasso_model.predict(X_test_num_cat)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Lasso RMSE:\", rmse)\n",
        "print(\"Lasso R²:\", r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69JOftKRy-Qt"
      },
      "source": [
        "#### 4. [1 point] Examine the parameter values ​​of the models obtained from StatsModels and check which weights and in which models turned out to be zero. Comment on the significance of the coefficients, the overall significance of the models and other factors from the resulting tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Np1biYQ7y-Qt"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>         <td>energy</td>      <th>  R-squared:         </th>  <td>   0.804</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.803</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   2664.</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Wed, 23 Oct 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>23:35:04</td>     <th>  Log-Likelihood:    </th>  <td>  70658.</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td> 91200</td>      <th>  AIC:               </th> <td>-1.410e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td> 91059</td>      <th>  BIC:               </th> <td>-1.397e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>   140</td>      <th>                     </th>      <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>const</th> <td>    0.2416</td> <td>    0.011</td> <td>   21.055</td> <td> 0.000</td> <td>    0.219</td> <td>    0.264</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>0</th>     <td>   -0.0033</td> <td>    0.000</td> <td>   -7.714</td> <td> 0.000</td> <td>   -0.004</td> <td>   -0.002</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>1</th>     <td>    0.0011</td> <td>    0.000</td> <td>    2.657</td> <td> 0.008</td> <td>    0.000</td> <td>    0.002</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>2</th>     <td>   -0.0218</td> <td>    0.001</td> <td>  -39.325</td> <td> 0.000</td> <td>   -0.023</td> <td>   -0.021</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>3</th>     <td>    0.1357</td> <td>    0.001</td> <td>  230.204</td> <td> 0.000</td> <td>    0.135</td> <td>    0.137</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>4</th>     <td>    0.0230</td> <td>    0.001</td> <td>   44.744</td> <td> 0.000</td> <td>    0.022</td> <td>    0.024</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>5</th>     <td>   -0.0822</td> <td>    0.001</td> <td> -140.880</td> <td> 0.000</td> <td>   -0.083</td> <td>   -0.081</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>6</th>     <td>    0.0243</td> <td>    0.001</td> <td>   45.646</td> <td> 0.000</td> <td>    0.023</td> <td>    0.025</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>7</th>     <td>    0.0206</td> <td>    0.000</td> <td>   50.378</td> <td> 0.000</td> <td>    0.020</td> <td>    0.021</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>8</th>     <td>    0.0404</td> <td>    0.001</td> <td>   80.674</td> <td> 0.000</td> <td>    0.039</td> <td>    0.041</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>9</th>     <td>    0.0061</td> <td>    0.000</td> <td>   15.117</td> <td> 0.000</td> <td>    0.005</td> <td>    0.007</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>10</th>    <td>   -0.0097</td> <td>    0.001</td> <td>   -6.596</td> <td> 0.000</td> <td>   -0.013</td> <td>   -0.007</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>11</th>    <td>    0.0111</td> <td>    0.002</td> <td>    6.740</td> <td> 0.000</td> <td>    0.008</td> <td>    0.014</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>12</th>    <td>    0.0028</td> <td>    0.002</td> <td>    1.775</td> <td> 0.076</td> <td>   -0.000</td> <td>    0.006</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>13</th>    <td>   -0.0028</td> <td>    0.002</td> <td>   -1.176</td> <td> 0.240</td> <td>   -0.007</td> <td>    0.002</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>14</th>    <td>    0.0039</td> <td>    0.002</td> <td>    2.256</td> <td> 0.024</td> <td>    0.001</td> <td>    0.007</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>15</th>    <td>   -0.0019</td> <td>    0.002</td> <td>   -1.110</td> <td> 0.267</td> <td>   -0.005</td> <td>    0.001</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>16</th>    <td>    0.0049</td> <td>    0.002</td> <td>    2.701</td> <td> 0.007</td> <td>    0.001</td> <td>    0.008</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>17</th>    <td>    0.0035</td> <td>    0.002</td> <td>    2.243</td> <td> 0.025</td> <td>    0.000</td> <td>    0.006</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>18</th>    <td>    0.0052</td> <td>    0.002</td> <td>    2.815</td> <td> 0.005</td> <td>    0.002</td> <td>    0.009</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>19</th>    <td>    0.0044</td> <td>    0.002</td> <td>    2.709</td> <td> 0.007</td> <td>    0.001</td> <td>    0.008</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>20</th>    <td>    0.0102</td> <td>    0.002</td> <td>    5.590</td> <td> 0.000</td> <td>    0.007</td> <td>    0.014</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>21</th>    <td>    0.0079</td> <td>    0.002</td> <td>    4.563</td> <td> 0.000</td> <td>    0.004</td> <td>    0.011</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>22</th>    <td>   -0.0056</td> <td>    0.001</td> <td>   -6.838</td> <td> 0.000</td> <td>   -0.007</td> <td>   -0.004</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>23</th>    <td>    0.3046</td> <td>    0.011</td> <td>   26.940</td> <td> 0.000</td> <td>    0.282</td> <td>    0.327</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>24</th>    <td>    0.2914</td> <td>    0.011</td> <td>   27.184</td> <td> 0.000</td> <td>    0.270</td> <td>    0.312</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>25</th>    <td>    0.3258</td> <td>    0.011</td> <td>   30.493</td> <td> 0.000</td> <td>    0.305</td> <td>    0.347</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>26</th>    <td>    0.2972</td> <td>    0.011</td> <td>   27.047</td> <td> 0.000</td> <td>    0.276</td> <td>    0.319</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>27</th>    <td>    0.0839</td> <td>    0.006</td> <td>   14.758</td> <td> 0.000</td> <td>    0.073</td> <td>    0.095</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>28</th>    <td>    0.0883</td> <td>    0.006</td> <td>   15.554</td> <td> 0.000</td> <td>    0.077</td> <td>    0.099</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>29</th>    <td>    0.0616</td> <td>    0.006</td> <td>   10.937</td> <td> 0.000</td> <td>    0.051</td> <td>    0.073</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>30</th>    <td>    0.0785</td> <td>    0.006</td> <td>   13.675</td> <td> 0.000</td> <td>    0.067</td> <td>    0.090</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>31</th>    <td>    0.0844</td> <td>    0.006</td> <td>   14.971</td> <td> 0.000</td> <td>    0.073</td> <td>    0.095</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>32</th>    <td>    0.1709</td> <td>    0.006</td> <td>   29.388</td> <td> 0.000</td> <td>    0.159</td> <td>    0.182</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>33</th>    <td>    0.0511</td> <td>    0.006</td> <td>    9.050</td> <td> 0.000</td> <td>    0.040</td> <td>    0.062</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>34</th>    <td>    0.0448</td> <td>    0.006</td> <td>    7.926</td> <td> 0.000</td> <td>    0.034</td> <td>    0.056</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>35</th>    <td>    0.0433</td> <td>    0.006</td> <td>    7.688</td> <td> 0.000</td> <td>    0.032</td> <td>    0.054</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>36</th>    <td>    0.1423</td> <td>    0.006</td> <td>   24.759</td> <td> 0.000</td> <td>    0.131</td> <td>    0.154</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>37</th>    <td>    0.0437</td> <td>    0.006</td> <td>    7.730</td> <td> 0.000</td> <td>    0.033</td> <td>    0.055</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>38</th>    <td>    0.0235</td> <td>    0.006</td> <td>    4.193</td> <td> 0.000</td> <td>    0.012</td> <td>    0.034</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>39</th>    <td>    0.1074</td> <td>    0.006</td> <td>   18.434</td> <td> 0.000</td> <td>    0.096</td> <td>    0.119</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>40</th>    <td>    0.0018</td> <td>    0.006</td> <td>    0.321</td> <td> 0.748</td> <td>   -0.009</td> <td>    0.013</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>41</th>    <td>   -0.0012</td> <td>    0.006</td> <td>   -0.205</td> <td> 0.837</td> <td>   -0.012</td> <td>    0.010</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>42</th>    <td>    0.0694</td> <td>    0.006</td> <td>   11.993</td> <td> 0.000</td> <td>    0.058</td> <td>    0.081</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>43</th>    <td>    0.0927</td> <td>    0.006</td> <td>   16.348</td> <td> 0.000</td> <td>    0.082</td> <td>    0.104</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>44</th>    <td>    0.1387</td> <td>    0.007</td> <td>   21.025</td> <td> 0.000</td> <td>    0.126</td> <td>    0.152</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>45</th>    <td>    0.0221</td> <td>    0.006</td> <td>    3.917</td> <td> 0.000</td> <td>    0.011</td> <td>    0.033</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>46</th>    <td>    0.0365</td> <td>    0.006</td> <td>    6.416</td> <td> 0.000</td> <td>    0.025</td> <td>    0.048</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>47</th>    <td>    0.0360</td> <td>    0.006</td> <td>    6.305</td> <td> 0.000</td> <td>    0.025</td> <td>    0.047</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>48</th>    <td>    0.2038</td> <td>    0.006</td> <td>   35.524</td> <td> 0.000</td> <td>    0.193</td> <td>    0.215</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>49</th>    <td>    0.1118</td> <td>    0.006</td> <td>   19.738</td> <td> 0.000</td> <td>    0.101</td> <td>    0.123</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>50</th>    <td>    0.1382</td> <td>    0.006</td> <td>   23.595</td> <td> 0.000</td> <td>    0.127</td> <td>    0.150</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>51</th>    <td>    0.1050</td> <td>    0.006</td> <td>   18.582</td> <td> 0.000</td> <td>    0.094</td> <td>    0.116</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>52</th>    <td>    0.0261</td> <td>    0.006</td> <td>    4.615</td> <td> 0.000</td> <td>    0.015</td> <td>    0.037</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>53</th>    <td>    0.1194</td> <td>    0.006</td> <td>   20.785</td> <td> 0.000</td> <td>    0.108</td> <td>    0.131</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>54</th>    <td>    0.0593</td> <td>    0.006</td> <td>   10.464</td> <td> 0.000</td> <td>    0.048</td> <td>    0.070</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>55</th>    <td>    0.0861</td> <td>    0.006</td> <td>   15.050</td> <td> 0.000</td> <td>    0.075</td> <td>    0.097</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>56</th>    <td>    0.0866</td> <td>    0.006</td> <td>   15.285</td> <td> 0.000</td> <td>    0.075</td> <td>    0.098</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>57</th>    <td>    0.0459</td> <td>    0.006</td> <td>    8.118</td> <td> 0.000</td> <td>    0.035</td> <td>    0.057</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>58</th>    <td>    0.0812</td> <td>    0.006</td> <td>   14.317</td> <td> 0.000</td> <td>    0.070</td> <td>    0.092</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>59</th>    <td>    0.0555</td> <td>    0.006</td> <td>    9.813</td> <td> 0.000</td> <td>    0.044</td> <td>    0.067</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>60</th>    <td>    0.0600</td> <td>    0.006</td> <td>   10.655</td> <td> 0.000</td> <td>    0.049</td> <td>    0.071</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>61</th>    <td>    0.1338</td> <td>    0.006</td> <td>   23.625</td> <td> 0.000</td> <td>    0.123</td> <td>    0.145</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>62</th>    <td>    0.0747</td> <td>    0.006</td> <td>   13.252</td> <td> 0.000</td> <td>    0.064</td> <td>    0.086</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>63</th>    <td>    0.0147</td> <td>    0.006</td> <td>    2.576</td> <td> 0.010</td> <td>    0.004</td> <td>    0.026</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>64</th>    <td>    0.0798</td> <td>    0.006</td> <td>   14.136</td> <td> 0.000</td> <td>    0.069</td> <td>    0.091</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>65</th>    <td>    0.0659</td> <td>    0.006</td> <td>   11.668</td> <td> 0.000</td> <td>    0.055</td> <td>    0.077</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>66</th>    <td>    0.0203</td> <td>    0.006</td> <td>    3.585</td> <td> 0.000</td> <td>    0.009</td> <td>    0.031</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>67</th>    <td>    0.1005</td> <td>    0.006</td> <td>   17.629</td> <td> 0.000</td> <td>    0.089</td> <td>    0.112</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>68</th>    <td>    0.1841</td> <td>    0.006</td> <td>   31.079</td> <td> 0.000</td> <td>    0.173</td> <td>    0.196</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>69</th>    <td>    0.1170</td> <td>    0.006</td> <td>   20.602</td> <td> 0.000</td> <td>    0.106</td> <td>    0.128</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>70</th>    <td>    0.1174</td> <td>    0.006</td> <td>   20.670</td> <td> 0.000</td> <td>    0.106</td> <td>    0.129</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>71</th>    <td>    0.0277</td> <td>    0.006</td> <td>    4.893</td> <td> 0.000</td> <td>    0.017</td> <td>    0.039</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>72</th>    <td>    0.1912</td> <td>    0.006</td> <td>   33.383</td> <td> 0.000</td> <td>    0.180</td> <td>    0.202</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>73</th>    <td>    0.1302</td> <td>    0.006</td> <td>   23.074</td> <td> 0.000</td> <td>    0.119</td> <td>    0.141</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>74</th>    <td>    0.1333</td> <td>    0.006</td> <td>   23.375</td> <td> 0.000</td> <td>    0.122</td> <td>    0.144</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>75</th>    <td>    0.1661</td> <td>    0.006</td> <td>   28.971</td> <td> 0.000</td> <td>    0.155</td> <td>    0.177</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>76</th>    <td>    0.1594</td> <td>    0.006</td> <td>   27.968</td> <td> 0.000</td> <td>    0.148</td> <td>    0.171</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>77</th>    <td>    0.0394</td> <td>    0.006</td> <td>    6.942</td> <td> 0.000</td> <td>    0.028</td> <td>    0.051</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>78</th>    <td>   -0.0025</td> <td>    0.006</td> <td>   -0.447</td> <td> 0.655</td> <td>   -0.014</td> <td>    0.009</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>79</th>    <td>    0.0798</td> <td>    0.006</td> <td>   13.947</td> <td> 0.000</td> <td>    0.069</td> <td>    0.091</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>80</th>    <td>    0.1055</td> <td>    0.006</td> <td>   18.264</td> <td> 0.000</td> <td>    0.094</td> <td>    0.117</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>81</th>    <td>    0.0853</td> <td>    0.006</td> <td>   15.191</td> <td> 0.000</td> <td>    0.074</td> <td>    0.096</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>82</th>    <td>    0.0329</td> <td>    0.006</td> <td>    5.843</td> <td> 0.000</td> <td>    0.022</td> <td>    0.044</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>83</th>    <td>    0.0321</td> <td>    0.006</td> <td>    5.706</td> <td> 0.000</td> <td>    0.021</td> <td>    0.043</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>84</th>    <td>    0.1629</td> <td>    0.006</td> <td>   28.512</td> <td> 0.000</td> <td>    0.152</td> <td>    0.174</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>85</th>    <td>    0.1214</td> <td>    0.006</td> <td>   21.049</td> <td> 0.000</td> <td>    0.110</td> <td>    0.133</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>86</th>    <td>    0.0499</td> <td>    0.006</td> <td>    8.658</td> <td> 0.000</td> <td>    0.039</td> <td>    0.061</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>87</th>    <td>    0.1258</td> <td>    0.006</td> <td>   22.255</td> <td> 0.000</td> <td>    0.115</td> <td>    0.137</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>88</th>    <td>    0.0559</td> <td>    0.006</td> <td>    9.859</td> <td> 0.000</td> <td>    0.045</td> <td>    0.067</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>89</th>    <td>    0.0848</td> <td>    0.006</td> <td>   15.020</td> <td> 0.000</td> <td>    0.074</td> <td>    0.096</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>90</th>    <td>   -0.0159</td> <td>    0.006</td> <td>   -2.812</td> <td> 0.005</td> <td>   -0.027</td> <td>   -0.005</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>91</th>    <td>    0.0734</td> <td>    0.006</td> <td>   12.966</td> <td> 0.000</td> <td>    0.062</td> <td>    0.085</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>92</th>    <td>    0.0195</td> <td>    0.006</td> <td>    3.423</td> <td> 0.001</td> <td>    0.008</td> <td>    0.031</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>93</th>    <td>    0.0515</td> <td>    0.006</td> <td>    9.028</td> <td> 0.000</td> <td>    0.040</td> <td>    0.063</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>94</th>    <td>    0.0580</td> <td>    0.006</td> <td>   10.224</td> <td> 0.000</td> <td>    0.047</td> <td>    0.069</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>95</th>    <td>    0.0633</td> <td>    0.006</td> <td>   11.237</td> <td> 0.000</td> <td>    0.052</td> <td>    0.074</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>96</th>    <td>    0.0142</td> <td>    0.006</td> <td>    2.525</td> <td> 0.012</td> <td>    0.003</td> <td>    0.025</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>97</th>    <td>    0.1375</td> <td>    0.006</td> <td>   24.287</td> <td> 0.000</td> <td>    0.126</td> <td>    0.149</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>98</th>    <td>    0.1771</td> <td>    0.006</td> <td>   31.003</td> <td> 0.000</td> <td>    0.166</td> <td>    0.188</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>99</th>    <td>    0.1051</td> <td>    0.006</td> <td>   17.929</td> <td> 0.000</td> <td>    0.094</td> <td>    0.117</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>100</th>   <td>    0.0575</td> <td>    0.006</td> <td>   10.256</td> <td> 0.000</td> <td>    0.047</td> <td>    0.068</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>101</th>   <td>    0.0463</td> <td>    0.006</td> <td>    8.043</td> <td> 0.000</td> <td>    0.035</td> <td>    0.058</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>102</th>   <td>    0.0423</td> <td>    0.006</td> <td>    7.506</td> <td> 0.000</td> <td>    0.031</td> <td>    0.053</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>103</th>   <td>    0.1247</td> <td>    0.006</td> <td>   21.947</td> <td> 0.000</td> <td>    0.114</td> <td>    0.136</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>104</th>   <td>    0.1250</td> <td>    0.006</td> <td>   21.931</td> <td> 0.000</td> <td>    0.114</td> <td>    0.136</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>105</th>   <td>    0.0945</td> <td>    0.006</td> <td>   16.624</td> <td> 0.000</td> <td>    0.083</td> <td>    0.106</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>106</th>   <td>    0.0338</td> <td>    0.006</td> <td>    6.032</td> <td> 0.000</td> <td>    0.023</td> <td>    0.045</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>107</th>   <td>    0.0822</td> <td>    0.006</td> <td>   14.566</td> <td> 0.000</td> <td>    0.071</td> <td>    0.093</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>108</th>   <td>    0.1127</td> <td>    0.006</td> <td>   19.790</td> <td> 0.000</td> <td>    0.102</td> <td>    0.124</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>109</th>   <td>    0.1335</td> <td>    0.006</td> <td>   23.620</td> <td> 0.000</td> <td>    0.122</td> <td>    0.145</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>110</th>   <td>    0.0391</td> <td>    0.006</td> <td>    6.982</td> <td> 0.000</td> <td>    0.028</td> <td>    0.050</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>111</th>   <td>    0.1133</td> <td>    0.006</td> <td>   19.976</td> <td> 0.000</td> <td>    0.102</td> <td>    0.124</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>112</th>   <td>    0.1138</td> <td>    0.006</td> <td>   20.096</td> <td> 0.000</td> <td>    0.103</td> <td>    0.125</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>113</th>   <td>    0.0528</td> <td>    0.006</td> <td>    9.375</td> <td> 0.000</td> <td>    0.042</td> <td>    0.064</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>114</th>   <td>    0.0487</td> <td>    0.006</td> <td>    8.581</td> <td> 0.000</td> <td>    0.038</td> <td>    0.060</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>115</th>   <td>    0.0527</td> <td>    0.006</td> <td>    9.276</td> <td> 0.000</td> <td>    0.042</td> <td>    0.064</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>116</th>   <td>    0.0526</td> <td>    0.006</td> <td>    9.315</td> <td> 0.000</td> <td>    0.042</td> <td>    0.064</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>117</th>   <td>    0.0390</td> <td>    0.006</td> <td>    6.903</td> <td> 0.000</td> <td>    0.028</td> <td>    0.050</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>118</th>   <td>    0.1001</td> <td>    0.006</td> <td>   17.645</td> <td> 0.000</td> <td>    0.089</td> <td>    0.111</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>119</th>   <td>    0.0109</td> <td>    0.006</td> <td>    1.917</td> <td> 0.055</td> <td>   -0.000</td> <td>    0.022</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>120</th>   <td>    0.0202</td> <td>    0.006</td> <td>    3.593</td> <td> 0.000</td> <td>    0.009</td> <td>    0.031</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>121</th>   <td>    0.1393</td> <td>    0.006</td> <td>   24.717</td> <td> 0.000</td> <td>    0.128</td> <td>    0.150</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>122</th>   <td>    0.1136</td> <td>    0.006</td> <td>   20.070</td> <td> 0.000</td> <td>    0.103</td> <td>    0.125</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>123</th>   <td>    0.0741</td> <td>    0.006</td> <td>   13.083</td> <td> 0.000</td> <td>    0.063</td> <td>    0.085</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>124</th>   <td>   -0.0086</td> <td>    0.006</td> <td>   -1.520</td> <td> 0.128</td> <td>   -0.020</td> <td>    0.002</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>125</th>   <td>    0.0087</td> <td>    0.006</td> <td>    1.562</td> <td> 0.118</td> <td>   -0.002</td> <td>    0.020</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>126</th>   <td>    0.0994</td> <td>    0.006</td> <td>   17.552</td> <td> 0.000</td> <td>    0.088</td> <td>    0.110</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>127</th>   <td>    0.3161</td> <td>    0.006</td> <td>   51.679</td> <td> 0.000</td> <td>    0.304</td> <td>    0.328</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>128</th>   <td>    0.0097</td> <td>    0.006</td> <td>    1.742</td> <td> 0.082</td> <td>   -0.001</td> <td>    0.021</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>129</th>   <td>    0.0097</td> <td>    0.006</td> <td>    1.729</td> <td> 0.084</td> <td>   -0.001</td> <td>    0.021</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>130</th>   <td>    0.0872</td> <td>    0.006</td> <td>   15.460</td> <td> 0.000</td> <td>    0.076</td> <td>    0.098</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>131</th>   <td>   -0.0530</td> <td>    0.006</td> <td>   -9.194</td> <td> 0.000</td> <td>   -0.064</td> <td>   -0.042</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>132</th>   <td>    0.0604</td> <td>    0.006</td> <td>   10.803</td> <td> 0.000</td> <td>    0.049</td> <td>    0.071</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>133</th>   <td>    0.1137</td> <td>    0.006</td> <td>   20.151</td> <td> 0.000</td> <td>    0.103</td> <td>    0.125</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>134</th>   <td>   -0.0277</td> <td>    0.006</td> <td>   -4.879</td> <td> 0.000</td> <td>   -0.039</td> <td>   -0.017</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>135</th>   <td>    0.1354</td> <td>    0.006</td> <td>   23.456</td> <td> 0.000</td> <td>    0.124</td> <td>    0.147</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>136</th>   <td>    0.1624</td> <td>    0.006</td> <td>   28.338</td> <td> 0.000</td> <td>    0.151</td> <td>    0.174</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>137</th>   <td>    0.0505</td> <td>    0.006</td> <td>    8.947</td> <td> 0.000</td> <td>    0.039</td> <td>    0.062</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>138</th>   <td>    0.0633</td> <td>    0.006</td> <td>   11.286</td> <td> 0.000</td> <td>    0.052</td> <td>    0.074</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>139</th>   <td>    0.0240</td> <td>    0.006</td> <td>    4.281</td> <td> 0.000</td> <td>    0.013</td> <td>    0.035</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>4428.716</td> <th>  Durbin-Watson:     </th> <td>   2.002</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>16391.075</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>           <td>-0.050</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>       <td> 5.074</td>  <th>  Cond. No.          </th> <td>    181.</td> \n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/latex": [
              "\\begin{center}\n",
              "\\begin{tabular}{lclc}\n",
              "\\toprule\n",
              "\\textbf{Dep. Variable:}    &      energy      & \\textbf{  R-squared:         } &     0.804   \\\\\n",
              "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.803   \\\\\n",
              "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     2664.   \\\\\n",
              "\\textbf{Date:}             & Wed, 23 Oct 2024 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
              "\\textbf{Time:}             &     23:35:04     & \\textbf{  Log-Likelihood:    } &    70658.   \\\\\n",
              "\\textbf{No. Observations:} &       91200      & \\textbf{  AIC:               } & -1.410e+05  \\\\\n",
              "\\textbf{Df Residuals:}     &       91059      & \\textbf{  BIC:               } & -1.397e+05  \\\\\n",
              "\\textbf{Df Model:}         &         140      & \\textbf{                     } &             \\\\\n",
              "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
              "\\bottomrule\n",
              "\\end{tabular}\n",
              "\\begin{tabular}{lcccccc}\n",
              "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
              "\\midrule\n",
              "\\textbf{const} &       0.2416  &        0.011     &    21.055  &         0.000        &        0.219    &        0.264     \\\\\n",
              "\\textbf{0}     &      -0.0033  &        0.000     &    -7.714  &         0.000        &       -0.004    &       -0.002     \\\\\n",
              "\\textbf{1}     &       0.0011  &        0.000     &     2.657  &         0.008        &        0.000    &        0.002     \\\\\n",
              "\\textbf{2}     &      -0.0218  &        0.001     &   -39.325  &         0.000        &       -0.023    &       -0.021     \\\\\n",
              "\\textbf{3}     &       0.1357  &        0.001     &   230.204  &         0.000        &        0.135    &        0.137     \\\\\n",
              "\\textbf{4}     &       0.0230  &        0.001     &    44.744  &         0.000        &        0.022    &        0.024     \\\\\n",
              "\\textbf{5}     &      -0.0822  &        0.001     &  -140.880  &         0.000        &       -0.083    &       -0.081     \\\\\n",
              "\\textbf{6}     &       0.0243  &        0.001     &    45.646  &         0.000        &        0.023    &        0.025     \\\\\n",
              "\\textbf{7}     &       0.0206  &        0.000     &    50.378  &         0.000        &        0.020    &        0.021     \\\\\n",
              "\\textbf{8}     &       0.0404  &        0.001     &    80.674  &         0.000        &        0.039    &        0.041     \\\\\n",
              "\\textbf{9}     &       0.0061  &        0.000     &    15.117  &         0.000        &        0.005    &        0.007     \\\\\n",
              "\\textbf{10}    &      -0.0097  &        0.001     &    -6.596  &         0.000        &       -0.013    &       -0.007     \\\\\n",
              "\\textbf{11}    &       0.0111  &        0.002     &     6.740  &         0.000        &        0.008    &        0.014     \\\\\n",
              "\\textbf{12}    &       0.0028  &        0.002     &     1.775  &         0.076        &       -0.000    &        0.006     \\\\\n",
              "\\textbf{13}    &      -0.0028  &        0.002     &    -1.176  &         0.240        &       -0.007    &        0.002     \\\\\n",
              "\\textbf{14}    &       0.0039  &        0.002     &     2.256  &         0.024        &        0.001    &        0.007     \\\\\n",
              "\\textbf{15}    &      -0.0019  &        0.002     &    -1.110  &         0.267        &       -0.005    &        0.001     \\\\\n",
              "\\textbf{16}    &       0.0049  &        0.002     &     2.701  &         0.007        &        0.001    &        0.008     \\\\\n",
              "\\textbf{17}    &       0.0035  &        0.002     &     2.243  &         0.025        &        0.000    &        0.006     \\\\\n",
              "\\textbf{18}    &       0.0052  &        0.002     &     2.815  &         0.005        &        0.002    &        0.009     \\\\\n",
              "\\textbf{19}    &       0.0044  &        0.002     &     2.709  &         0.007        &        0.001    &        0.008     \\\\\n",
              "\\textbf{20}    &       0.0102  &        0.002     &     5.590  &         0.000        &        0.007    &        0.014     \\\\\n",
              "\\textbf{21}    &       0.0079  &        0.002     &     4.563  &         0.000        &        0.004    &        0.011     \\\\\n",
              "\\textbf{22}    &      -0.0056  &        0.001     &    -6.838  &         0.000        &       -0.007    &       -0.004     \\\\\n",
              "\\textbf{23}    &       0.3046  &        0.011     &    26.940  &         0.000        &        0.282    &        0.327     \\\\\n",
              "\\textbf{24}    &       0.2914  &        0.011     &    27.184  &         0.000        &        0.270    &        0.312     \\\\\n",
              "\\textbf{25}    &       0.3258  &        0.011     &    30.493  &         0.000        &        0.305    &        0.347     \\\\\n",
              "\\textbf{26}    &       0.2972  &        0.011     &    27.047  &         0.000        &        0.276    &        0.319     \\\\\n",
              "\\textbf{27}    &       0.0839  &        0.006     &    14.758  &         0.000        &        0.073    &        0.095     \\\\\n",
              "\\textbf{28}    &       0.0883  &        0.006     &    15.554  &         0.000        &        0.077    &        0.099     \\\\\n",
              "\\textbf{29}    &       0.0616  &        0.006     &    10.937  &         0.000        &        0.051    &        0.073     \\\\\n",
              "\\textbf{30}    &       0.0785  &        0.006     &    13.675  &         0.000        &        0.067    &        0.090     \\\\\n",
              "\\textbf{31}    &       0.0844  &        0.006     &    14.971  &         0.000        &        0.073    &        0.095     \\\\\n",
              "\\textbf{32}    &       0.1709  &        0.006     &    29.388  &         0.000        &        0.159    &        0.182     \\\\\n",
              "\\textbf{33}    &       0.0511  &        0.006     &     9.050  &         0.000        &        0.040    &        0.062     \\\\\n",
              "\\textbf{34}    &       0.0448  &        0.006     &     7.926  &         0.000        &        0.034    &        0.056     \\\\\n",
              "\\textbf{35}    &       0.0433  &        0.006     &     7.688  &         0.000        &        0.032    &        0.054     \\\\\n",
              "\\textbf{36}    &       0.1423  &        0.006     &    24.759  &         0.000        &        0.131    &        0.154     \\\\\n",
              "\\textbf{37}    &       0.0437  &        0.006     &     7.730  &         0.000        &        0.033    &        0.055     \\\\\n",
              "\\textbf{38}    &       0.0235  &        0.006     &     4.193  &         0.000        &        0.012    &        0.034     \\\\\n",
              "\\textbf{39}    &       0.1074  &        0.006     &    18.434  &         0.000        &        0.096    &        0.119     \\\\\n",
              "\\textbf{40}    &       0.0018  &        0.006     &     0.321  &         0.748        &       -0.009    &        0.013     \\\\\n",
              "\\textbf{41}    &      -0.0012  &        0.006     &    -0.205  &         0.837        &       -0.012    &        0.010     \\\\\n",
              "\\textbf{42}    &       0.0694  &        0.006     &    11.993  &         0.000        &        0.058    &        0.081     \\\\\n",
              "\\textbf{43}    &       0.0927  &        0.006     &    16.348  &         0.000        &        0.082    &        0.104     \\\\\n",
              "\\textbf{44}    &       0.1387  &        0.007     &    21.025  &         0.000        &        0.126    &        0.152     \\\\\n",
              "\\textbf{45}    &       0.0221  &        0.006     &     3.917  &         0.000        &        0.011    &        0.033     \\\\\n",
              "\\textbf{46}    &       0.0365  &        0.006     &     6.416  &         0.000        &        0.025    &        0.048     \\\\\n",
              "\\textbf{47}    &       0.0360  &        0.006     &     6.305  &         0.000        &        0.025    &        0.047     \\\\\n",
              "\\textbf{48}    &       0.2038  &        0.006     &    35.524  &         0.000        &        0.193    &        0.215     \\\\\n",
              "\\textbf{49}    &       0.1118  &        0.006     &    19.738  &         0.000        &        0.101    &        0.123     \\\\\n",
              "\\textbf{50}    &       0.1382  &        0.006     &    23.595  &         0.000        &        0.127    &        0.150     \\\\\n",
              "\\textbf{51}    &       0.1050  &        0.006     &    18.582  &         0.000        &        0.094    &        0.116     \\\\\n",
              "\\textbf{52}    &       0.0261  &        0.006     &     4.615  &         0.000        &        0.015    &        0.037     \\\\\n",
              "\\textbf{53}    &       0.1194  &        0.006     &    20.785  &         0.000        &        0.108    &        0.131     \\\\\n",
              "\\textbf{54}    &       0.0593  &        0.006     &    10.464  &         0.000        &        0.048    &        0.070     \\\\\n",
              "\\textbf{55}    &       0.0861  &        0.006     &    15.050  &         0.000        &        0.075    &        0.097     \\\\\n",
              "\\textbf{56}    &       0.0866  &        0.006     &    15.285  &         0.000        &        0.075    &        0.098     \\\\\n",
              "\\textbf{57}    &       0.0459  &        0.006     &     8.118  &         0.000        &        0.035    &        0.057     \\\\\n",
              "\\textbf{58}    &       0.0812  &        0.006     &    14.317  &         0.000        &        0.070    &        0.092     \\\\\n",
              "\\textbf{59}    &       0.0555  &        0.006     &     9.813  &         0.000        &        0.044    &        0.067     \\\\\n",
              "\\textbf{60}    &       0.0600  &        0.006     &    10.655  &         0.000        &        0.049    &        0.071     \\\\\n",
              "\\textbf{61}    &       0.1338  &        0.006     &    23.625  &         0.000        &        0.123    &        0.145     \\\\\n",
              "\\textbf{62}    &       0.0747  &        0.006     &    13.252  &         0.000        &        0.064    &        0.086     \\\\\n",
              "\\textbf{63}    &       0.0147  &        0.006     &     2.576  &         0.010        &        0.004    &        0.026     \\\\\n",
              "\\textbf{64}    &       0.0798  &        0.006     &    14.136  &         0.000        &        0.069    &        0.091     \\\\\n",
              "\\textbf{65}    &       0.0659  &        0.006     &    11.668  &         0.000        &        0.055    &        0.077     \\\\\n",
              "\\textbf{66}    &       0.0203  &        0.006     &     3.585  &         0.000        &        0.009    &        0.031     \\\\\n",
              "\\textbf{67}    &       0.1005  &        0.006     &    17.629  &         0.000        &        0.089    &        0.112     \\\\\n",
              "\\textbf{68}    &       0.1841  &        0.006     &    31.079  &         0.000        &        0.173    &        0.196     \\\\\n",
              "\\textbf{69}    &       0.1170  &        0.006     &    20.602  &         0.000        &        0.106    &        0.128     \\\\\n",
              "\\textbf{70}    &       0.1174  &        0.006     &    20.670  &         0.000        &        0.106    &        0.129     \\\\\n",
              "\\textbf{71}    &       0.0277  &        0.006     &     4.893  &         0.000        &        0.017    &        0.039     \\\\\n",
              "\\textbf{72}    &       0.1912  &        0.006     &    33.383  &         0.000        &        0.180    &        0.202     \\\\\n",
              "\\textbf{73}    &       0.1302  &        0.006     &    23.074  &         0.000        &        0.119    &        0.141     \\\\\n",
              "\\textbf{74}    &       0.1333  &        0.006     &    23.375  &         0.000        &        0.122    &        0.144     \\\\\n",
              "\\textbf{75}    &       0.1661  &        0.006     &    28.971  &         0.000        &        0.155    &        0.177     \\\\\n",
              "\\textbf{76}    &       0.1594  &        0.006     &    27.968  &         0.000        &        0.148    &        0.171     \\\\\n",
              "\\textbf{77}    &       0.0394  &        0.006     &     6.942  &         0.000        &        0.028    &        0.051     \\\\\n",
              "\\textbf{78}    &      -0.0025  &        0.006     &    -0.447  &         0.655        &       -0.014    &        0.009     \\\\\n",
              "\\textbf{79}    &       0.0798  &        0.006     &    13.947  &         0.000        &        0.069    &        0.091     \\\\\n",
              "\\textbf{80}    &       0.1055  &        0.006     &    18.264  &         0.000        &        0.094    &        0.117     \\\\\n",
              "\\textbf{81}    &       0.0853  &        0.006     &    15.191  &         0.000        &        0.074    &        0.096     \\\\\n",
              "\\textbf{82}    &       0.0329  &        0.006     &     5.843  &         0.000        &        0.022    &        0.044     \\\\\n",
              "\\textbf{83}    &       0.0321  &        0.006     &     5.706  &         0.000        &        0.021    &        0.043     \\\\\n",
              "\\textbf{84}    &       0.1629  &        0.006     &    28.512  &         0.000        &        0.152    &        0.174     \\\\\n",
              "\\textbf{85}    &       0.1214  &        0.006     &    21.049  &         0.000        &        0.110    &        0.133     \\\\\n",
              "\\textbf{86}    &       0.0499  &        0.006     &     8.658  &         0.000        &        0.039    &        0.061     \\\\\n",
              "\\textbf{87}    &       0.1258  &        0.006     &    22.255  &         0.000        &        0.115    &        0.137     \\\\\n",
              "\\textbf{88}    &       0.0559  &        0.006     &     9.859  &         0.000        &        0.045    &        0.067     \\\\\n",
              "\\textbf{89}    &       0.0848  &        0.006     &    15.020  &         0.000        &        0.074    &        0.096     \\\\\n",
              "\\textbf{90}    &      -0.0159  &        0.006     &    -2.812  &         0.005        &       -0.027    &       -0.005     \\\\\n",
              "\\textbf{91}    &       0.0734  &        0.006     &    12.966  &         0.000        &        0.062    &        0.085     \\\\\n",
              "\\textbf{92}    &       0.0195  &        0.006     &     3.423  &         0.001        &        0.008    &        0.031     \\\\\n",
              "\\textbf{93}    &       0.0515  &        0.006     &     9.028  &         0.000        &        0.040    &        0.063     \\\\\n",
              "\\textbf{94}    &       0.0580  &        0.006     &    10.224  &         0.000        &        0.047    &        0.069     \\\\\n",
              "\\textbf{95}    &       0.0633  &        0.006     &    11.237  &         0.000        &        0.052    &        0.074     \\\\\n",
              "\\textbf{96}    &       0.0142  &        0.006     &     2.525  &         0.012        &        0.003    &        0.025     \\\\\n",
              "\\textbf{97}    &       0.1375  &        0.006     &    24.287  &         0.000        &        0.126    &        0.149     \\\\\n",
              "\\textbf{98}    &       0.1771  &        0.006     &    31.003  &         0.000        &        0.166    &        0.188     \\\\\n",
              "\\textbf{99}    &       0.1051  &        0.006     &    17.929  &         0.000        &        0.094    &        0.117     \\\\\n",
              "\\textbf{100}   &       0.0575  &        0.006     &    10.256  &         0.000        &        0.047    &        0.068     \\\\\n",
              "\\textbf{101}   &       0.0463  &        0.006     &     8.043  &         0.000        &        0.035    &        0.058     \\\\\n",
              "\\textbf{102}   &       0.0423  &        0.006     &     7.506  &         0.000        &        0.031    &        0.053     \\\\\n",
              "\\textbf{103}   &       0.1247  &        0.006     &    21.947  &         0.000        &        0.114    &        0.136     \\\\\n",
              "\\textbf{104}   &       0.1250  &        0.006     &    21.931  &         0.000        &        0.114    &        0.136     \\\\\n",
              "\\textbf{105}   &       0.0945  &        0.006     &    16.624  &         0.000        &        0.083    &        0.106     \\\\\n",
              "\\textbf{106}   &       0.0338  &        0.006     &     6.032  &         0.000        &        0.023    &        0.045     \\\\\n",
              "\\textbf{107}   &       0.0822  &        0.006     &    14.566  &         0.000        &        0.071    &        0.093     \\\\\n",
              "\\textbf{108}   &       0.1127  &        0.006     &    19.790  &         0.000        &        0.102    &        0.124     \\\\\n",
              "\\textbf{109}   &       0.1335  &        0.006     &    23.620  &         0.000        &        0.122    &        0.145     \\\\\n",
              "\\textbf{110}   &       0.0391  &        0.006     &     6.982  &         0.000        &        0.028    &        0.050     \\\\\n",
              "\\textbf{111}   &       0.1133  &        0.006     &    19.976  &         0.000        &        0.102    &        0.124     \\\\\n",
              "\\textbf{112}   &       0.1138  &        0.006     &    20.096  &         0.000        &        0.103    &        0.125     \\\\\n",
              "\\textbf{113}   &       0.0528  &        0.006     &     9.375  &         0.000        &        0.042    &        0.064     \\\\\n",
              "\\textbf{114}   &       0.0487  &        0.006     &     8.581  &         0.000        &        0.038    &        0.060     \\\\\n",
              "\\textbf{115}   &       0.0527  &        0.006     &     9.276  &         0.000        &        0.042    &        0.064     \\\\\n",
              "\\textbf{116}   &       0.0526  &        0.006     &     9.315  &         0.000        &        0.042    &        0.064     \\\\\n",
              "\\textbf{117}   &       0.0390  &        0.006     &     6.903  &         0.000        &        0.028    &        0.050     \\\\\n",
              "\\textbf{118}   &       0.1001  &        0.006     &    17.645  &         0.000        &        0.089    &        0.111     \\\\\n",
              "\\textbf{119}   &       0.0109  &        0.006     &     1.917  &         0.055        &       -0.000    &        0.022     \\\\\n",
              "\\textbf{120}   &       0.0202  &        0.006     &     3.593  &         0.000        &        0.009    &        0.031     \\\\\n",
              "\\textbf{121}   &       0.1393  &        0.006     &    24.717  &         0.000        &        0.128    &        0.150     \\\\\n",
              "\\textbf{122}   &       0.1136  &        0.006     &    20.070  &         0.000        &        0.103    &        0.125     \\\\\n",
              "\\textbf{123}   &       0.0741  &        0.006     &    13.083  &         0.000        &        0.063    &        0.085     \\\\\n",
              "\\textbf{124}   &      -0.0086  &        0.006     &    -1.520  &         0.128        &       -0.020    &        0.002     \\\\\n",
              "\\textbf{125}   &       0.0087  &        0.006     &     1.562  &         0.118        &       -0.002    &        0.020     \\\\\n",
              "\\textbf{126}   &       0.0994  &        0.006     &    17.552  &         0.000        &        0.088    &        0.110     \\\\\n",
              "\\textbf{127}   &       0.3161  &        0.006     &    51.679  &         0.000        &        0.304    &        0.328     \\\\\n",
              "\\textbf{128}   &       0.0097  &        0.006     &     1.742  &         0.082        &       -0.001    &        0.021     \\\\\n",
              "\\textbf{129}   &       0.0097  &        0.006     &     1.729  &         0.084        &       -0.001    &        0.021     \\\\\n",
              "\\textbf{130}   &       0.0872  &        0.006     &    15.460  &         0.000        &        0.076    &        0.098     \\\\\n",
              "\\textbf{131}   &      -0.0530  &        0.006     &    -9.194  &         0.000        &       -0.064    &       -0.042     \\\\\n",
              "\\textbf{132}   &       0.0604  &        0.006     &    10.803  &         0.000        &        0.049    &        0.071     \\\\\n",
              "\\textbf{133}   &       0.1137  &        0.006     &    20.151  &         0.000        &        0.103    &        0.125     \\\\\n",
              "\\textbf{134}   &      -0.0277  &        0.006     &    -4.879  &         0.000        &       -0.039    &       -0.017     \\\\\n",
              "\\textbf{135}   &       0.1354  &        0.006     &    23.456  &         0.000        &        0.124    &        0.147     \\\\\n",
              "\\textbf{136}   &       0.1624  &        0.006     &    28.338  &         0.000        &        0.151    &        0.174     \\\\\n",
              "\\textbf{137}   &       0.0505  &        0.006     &     8.947  &         0.000        &        0.039    &        0.062     \\\\\n",
              "\\textbf{138}   &       0.0633  &        0.006     &    11.286  &         0.000        &        0.052    &        0.074     \\\\\n",
              "\\textbf{139}   &       0.0240  &        0.006     &     4.281  &         0.000        &        0.013    &        0.035     \\\\\n",
              "\\bottomrule\n",
              "\\end{tabular}\n",
              "\\begin{tabular}{lclc}\n",
              "\\textbf{Omnibus:}       & 4428.716 & \\textbf{  Durbin-Watson:     } &     2.002  \\\\\n",
              "\\textbf{Prob(Omnibus):} &   0.000  & \\textbf{  Jarque-Bera (JB):  } & 16391.075  \\\\\n",
              "\\textbf{Skew:}          &  -0.050  & \\textbf{  Prob(JB):          } &      0.00  \\\\\n",
              "\\textbf{Kurtosis:}      &   5.074  & \\textbf{  Cond. No.          } &      181.  \\\\\n",
              "\\bottomrule\n",
              "\\end{tabular}\n",
              "%\\caption{OLS Regression Results}\n",
              "\\end{center}\n",
              "\n",
              "Notes: \\newline\n",
              " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:                 energy   R-squared:                       0.804\n",
              "Model:                            OLS   Adj. R-squared:                  0.803\n",
              "Method:                 Least Squares   F-statistic:                     2664.\n",
              "Date:                Wed, 23 Oct 2024   Prob (F-statistic):               0.00\n",
              "Time:                        23:35:04   Log-Likelihood:                 70658.\n",
              "No. Observations:               91200   AIC:                        -1.410e+05\n",
              "Df Residuals:                   91059   BIC:                        -1.397e+05\n",
              "Df Model:                         140                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "const          0.2416      0.011     21.055      0.000       0.219       0.264\n",
              "0             -0.0033      0.000     -7.714      0.000      -0.004      -0.002\n",
              "1              0.0011      0.000      2.657      0.008       0.000       0.002\n",
              "2             -0.0218      0.001    -39.325      0.000      -0.023      -0.021\n",
              "3              0.1357      0.001    230.204      0.000       0.135       0.137\n",
              "4              0.0230      0.001     44.744      0.000       0.022       0.024\n",
              "5             -0.0822      0.001   -140.880      0.000      -0.083      -0.081\n",
              "6              0.0243      0.001     45.646      0.000       0.023       0.025\n",
              "7              0.0206      0.000     50.378      0.000       0.020       0.021\n",
              "8              0.0404      0.001     80.674      0.000       0.039       0.041\n",
              "9              0.0061      0.000     15.117      0.000       0.005       0.007\n",
              "10            -0.0097      0.001     -6.596      0.000      -0.013      -0.007\n",
              "11             0.0111      0.002      6.740      0.000       0.008       0.014\n",
              "12             0.0028      0.002      1.775      0.076      -0.000       0.006\n",
              "13            -0.0028      0.002     -1.176      0.240      -0.007       0.002\n",
              "14             0.0039      0.002      2.256      0.024       0.001       0.007\n",
              "15            -0.0019      0.002     -1.110      0.267      -0.005       0.001\n",
              "16             0.0049      0.002      2.701      0.007       0.001       0.008\n",
              "17             0.0035      0.002      2.243      0.025       0.000       0.006\n",
              "18             0.0052      0.002      2.815      0.005       0.002       0.009\n",
              "19             0.0044      0.002      2.709      0.007       0.001       0.008\n",
              "20             0.0102      0.002      5.590      0.000       0.007       0.014\n",
              "21             0.0079      0.002      4.563      0.000       0.004       0.011\n",
              "22            -0.0056      0.001     -6.838      0.000      -0.007      -0.004\n",
              "23             0.3046      0.011     26.940      0.000       0.282       0.327\n",
              "24             0.2914      0.011     27.184      0.000       0.270       0.312\n",
              "25             0.3258      0.011     30.493      0.000       0.305       0.347\n",
              "26             0.2972      0.011     27.047      0.000       0.276       0.319\n",
              "27             0.0839      0.006     14.758      0.000       0.073       0.095\n",
              "28             0.0883      0.006     15.554      0.000       0.077       0.099\n",
              "29             0.0616      0.006     10.937      0.000       0.051       0.073\n",
              "30             0.0785      0.006     13.675      0.000       0.067       0.090\n",
              "31             0.0844      0.006     14.971      0.000       0.073       0.095\n",
              "32             0.1709      0.006     29.388      0.000       0.159       0.182\n",
              "33             0.0511      0.006      9.050      0.000       0.040       0.062\n",
              "34             0.0448      0.006      7.926      0.000       0.034       0.056\n",
              "35             0.0433      0.006      7.688      0.000       0.032       0.054\n",
              "36             0.1423      0.006     24.759      0.000       0.131       0.154\n",
              "37             0.0437      0.006      7.730      0.000       0.033       0.055\n",
              "38             0.0235      0.006      4.193      0.000       0.012       0.034\n",
              "39             0.1074      0.006     18.434      0.000       0.096       0.119\n",
              "40             0.0018      0.006      0.321      0.748      -0.009       0.013\n",
              "41            -0.0012      0.006     -0.205      0.837      -0.012       0.010\n",
              "42             0.0694      0.006     11.993      0.000       0.058       0.081\n",
              "43             0.0927      0.006     16.348      0.000       0.082       0.104\n",
              "44             0.1387      0.007     21.025      0.000       0.126       0.152\n",
              "45             0.0221      0.006      3.917      0.000       0.011       0.033\n",
              "46             0.0365      0.006      6.416      0.000       0.025       0.048\n",
              "47             0.0360      0.006      6.305      0.000       0.025       0.047\n",
              "48             0.2038      0.006     35.524      0.000       0.193       0.215\n",
              "49             0.1118      0.006     19.738      0.000       0.101       0.123\n",
              "50             0.1382      0.006     23.595      0.000       0.127       0.150\n",
              "51             0.1050      0.006     18.582      0.000       0.094       0.116\n",
              "52             0.0261      0.006      4.615      0.000       0.015       0.037\n",
              "53             0.1194      0.006     20.785      0.000       0.108       0.131\n",
              "54             0.0593      0.006     10.464      0.000       0.048       0.070\n",
              "55             0.0861      0.006     15.050      0.000       0.075       0.097\n",
              "56             0.0866      0.006     15.285      0.000       0.075       0.098\n",
              "57             0.0459      0.006      8.118      0.000       0.035       0.057\n",
              "58             0.0812      0.006     14.317      0.000       0.070       0.092\n",
              "59             0.0555      0.006      9.813      0.000       0.044       0.067\n",
              "60             0.0600      0.006     10.655      0.000       0.049       0.071\n",
              "61             0.1338      0.006     23.625      0.000       0.123       0.145\n",
              "62             0.0747      0.006     13.252      0.000       0.064       0.086\n",
              "63             0.0147      0.006      2.576      0.010       0.004       0.026\n",
              "64             0.0798      0.006     14.136      0.000       0.069       0.091\n",
              "65             0.0659      0.006     11.668      0.000       0.055       0.077\n",
              "66             0.0203      0.006      3.585      0.000       0.009       0.031\n",
              "67             0.1005      0.006     17.629      0.000       0.089       0.112\n",
              "68             0.1841      0.006     31.079      0.000       0.173       0.196\n",
              "69             0.1170      0.006     20.602      0.000       0.106       0.128\n",
              "70             0.1174      0.006     20.670      0.000       0.106       0.129\n",
              "71             0.0277      0.006      4.893      0.000       0.017       0.039\n",
              "72             0.1912      0.006     33.383      0.000       0.180       0.202\n",
              "73             0.1302      0.006     23.074      0.000       0.119       0.141\n",
              "74             0.1333      0.006     23.375      0.000       0.122       0.144\n",
              "75             0.1661      0.006     28.971      0.000       0.155       0.177\n",
              "76             0.1594      0.006     27.968      0.000       0.148       0.171\n",
              "77             0.0394      0.006      6.942      0.000       0.028       0.051\n",
              "78            -0.0025      0.006     -0.447      0.655      -0.014       0.009\n",
              "79             0.0798      0.006     13.947      0.000       0.069       0.091\n",
              "80             0.1055      0.006     18.264      0.000       0.094       0.117\n",
              "81             0.0853      0.006     15.191      0.000       0.074       0.096\n",
              "82             0.0329      0.006      5.843      0.000       0.022       0.044\n",
              "83             0.0321      0.006      5.706      0.000       0.021       0.043\n",
              "84             0.1629      0.006     28.512      0.000       0.152       0.174\n",
              "85             0.1214      0.006     21.049      0.000       0.110       0.133\n",
              "86             0.0499      0.006      8.658      0.000       0.039       0.061\n",
              "87             0.1258      0.006     22.255      0.000       0.115       0.137\n",
              "88             0.0559      0.006      9.859      0.000       0.045       0.067\n",
              "89             0.0848      0.006     15.020      0.000       0.074       0.096\n",
              "90            -0.0159      0.006     -2.812      0.005      -0.027      -0.005\n",
              "91             0.0734      0.006     12.966      0.000       0.062       0.085\n",
              "92             0.0195      0.006      3.423      0.001       0.008       0.031\n",
              "93             0.0515      0.006      9.028      0.000       0.040       0.063\n",
              "94             0.0580      0.006     10.224      0.000       0.047       0.069\n",
              "95             0.0633      0.006     11.237      0.000       0.052       0.074\n",
              "96             0.0142      0.006      2.525      0.012       0.003       0.025\n",
              "97             0.1375      0.006     24.287      0.000       0.126       0.149\n",
              "98             0.1771      0.006     31.003      0.000       0.166       0.188\n",
              "99             0.1051      0.006     17.929      0.000       0.094       0.117\n",
              "100            0.0575      0.006     10.256      0.000       0.047       0.068\n",
              "101            0.0463      0.006      8.043      0.000       0.035       0.058\n",
              "102            0.0423      0.006      7.506      0.000       0.031       0.053\n",
              "103            0.1247      0.006     21.947      0.000       0.114       0.136\n",
              "104            0.1250      0.006     21.931      0.000       0.114       0.136\n",
              "105            0.0945      0.006     16.624      0.000       0.083       0.106\n",
              "106            0.0338      0.006      6.032      0.000       0.023       0.045\n",
              "107            0.0822      0.006     14.566      0.000       0.071       0.093\n",
              "108            0.1127      0.006     19.790      0.000       0.102       0.124\n",
              "109            0.1335      0.006     23.620      0.000       0.122       0.145\n",
              "110            0.0391      0.006      6.982      0.000       0.028       0.050\n",
              "111            0.1133      0.006     19.976      0.000       0.102       0.124\n",
              "112            0.1138      0.006     20.096      0.000       0.103       0.125\n",
              "113            0.0528      0.006      9.375      0.000       0.042       0.064\n",
              "114            0.0487      0.006      8.581      0.000       0.038       0.060\n",
              "115            0.0527      0.006      9.276      0.000       0.042       0.064\n",
              "116            0.0526      0.006      9.315      0.000       0.042       0.064\n",
              "117            0.0390      0.006      6.903      0.000       0.028       0.050\n",
              "118            0.1001      0.006     17.645      0.000       0.089       0.111\n",
              "119            0.0109      0.006      1.917      0.055      -0.000       0.022\n",
              "120            0.0202      0.006      3.593      0.000       0.009       0.031\n",
              "121            0.1393      0.006     24.717      0.000       0.128       0.150\n",
              "122            0.1136      0.006     20.070      0.000       0.103       0.125\n",
              "123            0.0741      0.006     13.083      0.000       0.063       0.085\n",
              "124           -0.0086      0.006     -1.520      0.128      -0.020       0.002\n",
              "125            0.0087      0.006      1.562      0.118      -0.002       0.020\n",
              "126            0.0994      0.006     17.552      0.000       0.088       0.110\n",
              "127            0.3161      0.006     51.679      0.000       0.304       0.328\n",
              "128            0.0097      0.006      1.742      0.082      -0.001       0.021\n",
              "129            0.0097      0.006      1.729      0.084      -0.001       0.021\n",
              "130            0.0872      0.006     15.460      0.000       0.076       0.098\n",
              "131           -0.0530      0.006     -9.194      0.000      -0.064      -0.042\n",
              "132            0.0604      0.006     10.803      0.000       0.049       0.071\n",
              "133            0.1137      0.006     20.151      0.000       0.103       0.125\n",
              "134           -0.0277      0.006     -4.879      0.000      -0.039      -0.017\n",
              "135            0.1354      0.006     23.456      0.000       0.124       0.147\n",
              "136            0.1624      0.006     28.338      0.000       0.151       0.174\n",
              "137            0.0505      0.006      8.947      0.000       0.039       0.062\n",
              "138            0.0633      0.006     11.286      0.000       0.052       0.074\n",
              "139            0.0240      0.006      4.281      0.000       0.013       0.035\n",
              "==============================================================================\n",
              "Omnibus:                     4428.716   Durbin-Watson:                   2.002\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            16391.075\n",
              "Skew:                          -0.050   Prob(JB):                         0.00\n",
              "Kurtosis:                       5.074   Cond. No.                         181.\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_sm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It doesn't zero out any parameters, however it's not surprising because it isn't using regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.        ,  0.        , -0.        ,  0.09558628,  0.        ,\n",
              "       -0.07828713,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
              "       -0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
              "        0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
              "       -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
              "       -0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
              "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
              "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
              "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
              "       -0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
              "       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
              "        0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
              "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
              "       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
              "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
              "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
              "       -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
              "        0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
              "       -0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
              "       -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
              "        0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
              "        0.        ,  0.        , -0.        , -0.        , -0.        ])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lasso_model.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, if we now look at lasso model, here it zeroed out every single weight except two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.00331484,  0.00108738, -0.02175282,  0.13565726,  0.02297808,\n",
              "        -0.08215616,  0.02431216,  0.020624  ,  0.0404424 ,  0.00606037,\n",
              "        -0.00973059,  0.01107892,  0.00283289, -0.00277682,  0.00391098,\n",
              "        -0.00189005,  0.00485391,  0.00346283,  0.0051597 ,  0.00437175,\n",
              "         0.0102324 ,  0.0078721 , -0.00563272,  0.30422021,  0.29110713,\n",
              "         0.32549687,  0.29688151,  0.08356111,  0.08799837,  0.06123254,\n",
              "         0.07818433,  0.08406089,  0.17052406,  0.05074075,  0.04443394,\n",
              "         0.04291396,  0.14194216,  0.04330844,  0.02312568,  0.10707253,\n",
              "         0.00147943, -0.0014946 ,  0.0690761 ,  0.09235011,  0.13837182,\n",
              "         0.0218029 ,  0.03612693,  0.03567114,  0.20348552,  0.11148549,\n",
              "         0.1378512 ,  0.1046516 ,  0.02579008,  0.11900132,  0.0589406 ,\n",
              "         0.08573355,  0.08621796,  0.04556559,  0.08080069,  0.05515466,\n",
              "         0.0596831 ,  0.13350283,  0.07435159,  0.01434103,  0.07948159,\n",
              "         0.06552226,  0.01994004,  0.10017219,  0.18375314,  0.1166874 ,\n",
              "         0.11702522,  0.02733091,  0.19086647,  0.1298244 ,  0.13296283,\n",
              "         0.16573031,  0.1590625 ,  0.03909606, -0.0028652 ,  0.07942306,\n",
              "         0.10513833,  0.08495501,  0.0325837 ,  0.03177951,  0.16251871,\n",
              "         0.12103221,  0.04953099,  0.12549302,  0.05554129,  0.08440771,\n",
              "        -0.01627738,  0.07308535,  0.01915623,  0.05117339,  0.05769077,\n",
              "         0.06298855,  0.0138511 ,  0.13716756,  0.17676569,  0.10473262,\n",
              "         0.05715473,  0.04591569,  0.04198886,  0.12432479,  0.12461487,\n",
              "         0.09414949,  0.03345766,  0.08189861,  0.11238199,  0.13312562,\n",
              "         0.03879113,  0.11293981,  0.11349862,  0.05249432,  0.04839181,\n",
              "         0.05238626,  0.0522636 ,  0.03861728,  0.09978879,  0.0105853 ,\n",
              "         0.01985301,  0.13892429,  0.11326778,  0.07378051, -0.00891076,\n",
              "         0.00837671,  0.09901103,  0.31570923,  0.00937523,  0.00938396,\n",
              "         0.08680506, -0.05336078,  0.06001401,  0.11332044, -0.02801939,\n",
              "         0.13502456,  0.16205687,  0.05013279,  0.06298792,  0.02368907]])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ridge_model.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And ridge doesn't zero out any weights, which is also not surprising because of the way L2 regularization works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLcvGlUZy-Qt"
      },
      "source": [
        "#### 5. [1 point] Implement one of the feature selection algorithms (Elimination by P-value, Forward elimination, Backward elimination), draw conclusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "TnrbRbkwy-Qt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing feature 41 with p-value 0.8374620143895892\n",
            "Removing feature 78 with p-value 0.6910735190457282\n",
            "Removing feature 40 with p-value 0.5075308805016102\n",
            "Removing feature 15 with p-value 0.26425945619253505\n",
            "Removing feature 13 with p-value 0.3809424380380759\n",
            "Removing feature 124 with p-value 0.0698305027257394\n",
            "StatsModels OLS RMSE: 0.11221312709937135\n",
            "StatsModels OLS R²: 0.7999290375531993\n"
          ]
        }
      ],
      "source": [
        "significance_level = 0.05\n",
        "\n",
        "# P-value elimination\n",
        "while True:\n",
        "    model_sm = sm.OLS(y_train, X_train_sm).fit()  \n",
        "    max_p_value = model_sm.pvalues.max()\n",
        "    \n",
        "    if max_p_value > significance_level:\n",
        "        excluded_feature = model_sm.pvalues.idxmax()\n",
        "        print(f\"Removing feature {excluded_feature} with p-value {max_p_value}\")\n",
        "        \n",
        "\n",
        "        X_train_sm = X_train_sm.drop(columns=[excluded_feature]).reset_index(drop=True)\n",
        "        X_test_sm = X_test_sm.drop(columns=[excluded_feature]).reset_index(drop=True)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "preds = model_sm.predict(X_test_sm)\n",
        "\n",
        "\n",
        "rmse_sm = np.sqrt(mean_squared_error(y_test, preds))\n",
        "r2_sm = r2_score(y_test, preds)\n",
        "\n",
        "print(\"StatsModels OLS RMSE:\", rmse_sm)\n",
        "print(\"StatsModels OLS R²:\", r2_sm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see elemination by p-value didn't effect model quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df0eQLdNy-Qt"
      },
      "source": [
        "#### 6. [1 point] Find the best (RMSE) $\\alpha$ for Lasso regression using 4-fold cross-validation. You should choose a value from the logarithmic range $[10^{-4}, 10^{3}]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "JPoT3YHqy-Qt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best alpha value: 0.0001\n",
            "Best RMSE: 0.33621615124823123\n",
            "Best R²: 0.796103513769233\n",
            "Test RMSE: 0.12185403186872831\n",
            "Test R²: 0.7640736015658242\n"
          ]
        }
      ],
      "source": [
        "alpha_values = np.logspace(-4, 3, 20)\n",
        "\n",
        "model_lasso = Lasso()\n",
        "\n",
        "param_grid = {\"alpha\": alpha_values}\n",
        "\n",
        "grid_search = GridSearchCV(model_lasso, param_grid, cv=4, scoring=\"neg_root_mean_squared_error\")\n",
        "\n",
        "grid_search.fit(X_train_num_cat, y_train)\n",
        "\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "print(f\"Best alpha value: {best_alpha}\")\n",
        "\n",
        "\n",
        "best_rmse = np.sqrt(-grid_search.best_score_)\n",
        "print(f\"Best RMSE: {best_rmse}\")\n",
        "best_r2 = r2_score(y_test, grid_search.predict(X_test_num_cat))\n",
        "print(f\"Best R²: {best_r2}\")\n",
        "\n",
        "\n",
        "lasso_best = Lasso(alpha=best_alpha)\n",
        "lasso_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "y_pred = lasso_best.predict(X_test_scaled)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"Test RMSE: {test_rmse}\")\n",
        "test_r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Test R²: {test_r2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1PKinJUy-Qt"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "#### 7. [3.5 points] Implement Ridge regression for MSE loss trained using gradient descent.\n",
        "\n",
        "All computations must be vectorized, and Python loops can only be used for gradient descent iterations. The stopping criteria must be (simultaneously):\n",
        "\n",
        "* checking the absolute norm of the difference in weights on two adjacent iterations (e.g., less than some small number of the order of $10^{-6}$, specified by the `tolerance` parameter);\n",
        "\n",
        "* reaching the maximum number of iterations (e.g., 10000, specified by the `max_iter` parameter).\n",
        "\n",
        "You need to do:\n",
        "\n",
        "* Full gradient descent:\n",
        "\n",
        "$$\n",
        "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
        "$$\n",
        "\n",
        "* Stochastic Gradient Descent:\n",
        "\n",
        "$$\n",
        "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
        "$$\n",
        "\n",
        "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is an estimate of the gradient over a set of objects chosen at random.\n",
        "\n",
        "* Moment of method:\n",
        "\n",
        "$$\n",
        "h_0 = 0, \\\\\n",
        "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
        "w_{k + 1} = w_{k} - h_{k + 1}.\n",
        "$$\n",
        "\n",
        "* Adagrad method:\n",
        "\n",
        "$$\n",
        "G_0 = 0, \\\\\n",
        "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k}))^2, \\\\\n",
        "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
        "$$\n",
        "\n",
        "To verify that the optimization process is actually running, we will use the `loss_history` class attribute. After calling the fit method, it should contain the loss function values ​​for all iterations starting from the first (up to the first step along the antigradient).\n",
        "\n",
        "You need to initialize the weights with a random vector from a normal distribution. Below is a template that should contain code implementing all the model variants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "oI39UzCLy-Qu"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "\n",
        "class LinReg(BaseEstimator):\n",
        "    def __init__(\n",
        "        self,\n",
        "        delta=0.001,\n",
        "        gd_type=\"Momentum\",\n",
        "        tolerance=1e-4,\n",
        "        max_iter=1000,\n",
        "        w0=None,\n",
        "        eta=1e-2,\n",
        "        alpha=1e-3,\n",
        "        epsilon=1e-8,\n",
        "        reg_cf=1e-4,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        gd_type: str\n",
        "            'GradientDescent', 'StochasticDescent', 'Momentum', 'Adagrad'\n",
        "        delta: float\n",
        "            proportion of object in a batch (for stochastic GD)\n",
        "        tolerance: float\n",
        "            for stopping gradient descent\n",
        "        max_iter: int\n",
        "            maximum number of steps in gradient descent\n",
        "        w0: np.array of shape (d)\n",
        "            init weights\n",
        "        eta: float\n",
        "            learning rate\n",
        "        alpha: float\n",
        "            momentum coefficient\n",
        "        reg_cf: float\n",
        "            regularization coefficient\n",
        "        epsilon: float\n",
        "            numerical stability\n",
        "        \"\"\"\n",
        "\n",
        "        self.delta = delta\n",
        "        self.gd_type = gd_type\n",
        "        self.tolerance = tolerance\n",
        "        self.max_iter = max_iter\n",
        "        self.w0 = w0\n",
        "        self.w = None\n",
        "        self.alpha = alpha\n",
        "        self.eta = eta\n",
        "        self.epsilon = epsilon\n",
        "        self.reg_cf = reg_cf\n",
        "        self.loss_history = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d)\n",
        "        y: np.array of shape (l)\n",
        "        ---\n",
        "        output: self\n",
        "        \"\"\"\n",
        "        self.loss_history = []\n",
        "\n",
        "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "        self.w = self.w0 if self.w0 is not None else np.random.randn(X.shape[1])\n",
        "\n",
        "        self.momentum = np.zeros(X.shape[1])\n",
        "\n",
        "        self.g = np.zeros(X.shape[1])\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.loss_history.append(self.calc_loss(X, y))\n",
        "\n",
        "            if self.gd_type == \"GradientDescent\":\n",
        "                update = self.calc_gradient(X, y)\n",
        "            elif self.gd_type == \"StochasticDescent\":\n",
        "                indices = np.random.choice(\n",
        "                    X.shape[0], int(X.shape[0] * self.delta), replace=False\n",
        "                )\n",
        "                update = self.calc_gradient(X[indices], y[indices])\n",
        "            elif self.gd_type == \"Momentum\":\n",
        "                dw = self.calc_gradient(X, y)\n",
        "                self.momentum = self.alpha * self.momentum + self.eta * dw\n",
        "                update = self.momentum\n",
        "            elif self.gd_type == \"Adagrad\":\n",
        "                dw = self.calc_gradient(X, y)\n",
        "                self.g += dw**2\n",
        "                update = dw / np.sqrt(self.g + self.epsilon)\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    \"Invalid gradient descent type: {self.gd_type}\"\n",
        "                )\n",
        "\n",
        "            self.w -= self.eta * update\n",
        "\n",
        "            if np.linalg.norm(self.eta * update) < self.tolerance:\n",
        "                break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.w is None:\n",
        "            raise Exception(\"Not trained yet\")\n",
        "\n",
        "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "        return X @ self.w\n",
        "\n",
        "    def calc_gradient(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
        "        y: np.array of shape (l)\n",
        "        ---\n",
        "        output: np.array of shape (d)\n",
        "        \"\"\"\n",
        "        return (2 * X.T @ (X @ self.w - y) / X.shape[0]) + (\n",
        "            2 * self.reg_cf * self.w\n",
        "        )\n",
        "\n",
        "    def calc_loss(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d)\n",
        "        y: np.array of shape (l)\n",
        "        ---\n",
        "        output: float\n",
        "        \"\"\"\n",
        "        return np.mean((X @ self.w - y) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QQJEjGVy-Qu"
      },
      "source": [
        "#### 8. [1 point] Train and validate \"manual\" models on the same data, compare the quality with models from Sklearn and StatsModels. Investigate the influence of the `max_iter` and `alpha` parameters on the optimization process. Does it meet your expectations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for GradientDescent model:\n",
            "Test RMSE: 0.12184948867521328\n",
            "Test R²: 0.7640911937502666\n",
            "----------\n",
            "\n",
            "Results for StochasticDescent model:\n",
            "Test RMSE: 0.12296208238081502\n",
            "Test R²: 0.7597634128049362\n",
            "----------\n",
            "\n",
            "Results for Momentum model:\n",
            "Test RMSE: 0.12210076012902739\n",
            "Test R²: 0.763117233688045\n",
            "----------\n",
            "\n",
            "Results for Adagrad model:\n",
            "Test RMSE: 0.12185250235107205\n",
            "Test R²: 0.7640795242476054\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_model(model, X_train, y_train, X_test, y_test):\n",
        "    if not isinstance(y_train, np.ndarray):\n",
        "        y_train = np.array(y_train)\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    print(f\"Results for {model.gd_type} model:\")\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    test_r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"Test R²: {test_r2}\")\n",
        "    print(\"----------\\n\")\n",
        "\n",
        "gd_model = LinReg(gd_type='GradientDescent', eta=1e-1)\n",
        "sgd_model = LinReg(gd_type='StochasticDescent', eta=1e-1)\n",
        "momentum_model = LinReg(gd_type='Momentum', eta=1e-1, alpha=1e-2)\n",
        "adagrad_model = LinReg(gd_type='Adagrad', eta=1)\n",
        "\n",
        "# here I'm using only numerical features because it's allowed and trains faster\n",
        "# so I'm going to be comparing the results to the results of models with only numerical features\n",
        "X, y = data.drop(['energy'], axis=1), data['energy']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train[numerical_features])\n",
        "X_test_scaled = scaler.transform(X_test[numerical_features])\n",
        "\n",
        "for model in [gd_model, sgd_model, momentum_model, adagrad_model]:\n",
        "    test_model(model, X_train_scaled, y_train, X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Models achieve same results as models from StatsModels and sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqYtVqv-y-Qu"
      },
      "source": [
        "#### 9. [1 point] Plot graphs of the loss function values ​​as a function of the iteration number for all models (full gradient descent, stochastic gc, Momentum, and Adagrad). Draw conclusions about the convergence rate of various modifications of gradient descent.\n",
        "\n",
        "Don't forget about what a *nice* graph should look like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Xbwhu8BSy-Qu"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABjAAAAHkCAYAAACQb2ZJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAC5EklEQVR4nOzdeXhTdfbH8U+WpgsQaFkKiiKCreyLoOAgm4M4CP7EZVwAwUFZRREcUBgFdRCcQZQdBARGRFFhkFEZx2XcUQTXcQFZVZCtLKV0T+7vj/YGQrckTZs0eb+ehwe5ubn5ngzTy73nnnMshmEYAgAAAAAAAAAACCPWUC8AAAAAAAAAAADgbCQwAAAAAAAAAABA2CGBAQAAAAAAAAAAwg4JDAAAAAAAAAAAEHZIYAAAAAAAAAAAgLBDAgMAAAAAAAAAAIQdEhgAAAAAAAAAACDskMAAAAAAAAAAAABhhwQGUAUYhhHqJVS6aIwZQOWKhJ8zkRBDReM7AoDKw89cAAAQbCQwosigQYOUmpqqW265pcR97rvvPqWmpuqBBx4o9+d99tlnSk1N1Weffebze3799VelpqZq3bp1Ze574MABzZw5U3379lW7du3Url079e/fX4sXL1ZmZmZ5ll6muXPnKjU11fPnBx54QD179gz656Snp2vixInasmWL12elpqZ6fl188cVq27at+vbtq/nz5ysnJyfo66hMBw4c0PDhw7Vv375QLwWIGpF0fti3b58mT56sbt26qWXLlurUqZOGDx+uTz/91Gu/HTt26NZbbw1o/aU5+/xQkbZu3arhw4d7/uzPOdR05vkkNTVVzZs312WXXaY77rhD77//fkUsu1K98847mjhxYqiXAaCcIuU8Zf47vmvXriXe6J85c6ZSU1M1aNAgv9cdagsXLtSyZctCvQwA8EuknGNMr7zyilJTU3XnnXf6ta7U1FTNnTvXr/eEQiDfH6o+e6gXgMpltVr11Vdf6bffflODBg28XsvKytJ7770XmoX56bPPPtM999wjp9OpAQMGKDU1VW63W5999pkWLVqkN998U6tXr1ZcXFylrGfUqFG6/fbbg37cH374QevXr9f111/vtb1u3bqaN2+eJMntduvkyZP6/PPPtXDhQn388cdavny5YmNjg76eyvDJJ5/ovffe00MPPRTqpQBRJRLOD4cPH9bNN9+sevXq6b777tM555yjo0eP6uWXX9aQIUM0Z84cXXXVVZKkjRs36ssvvwzxisvn5Zdf1o4dOzx/rlevntasWaPzzz/fr+PceOONuummmyRJeXl5Onz4sF555RUNGzZMDz30kAYOHBjUdVemFStWhHoJAIIkEs5TUkEcBw8e1NatW9WhQ4cir2/cuDEEqwqOp59+WnfffXeolwEAfouUc4wkrV27VikpKfr444/1yy+/6Lzzzgv1koByowIjyjRv3lyxsbH697//XeS1d999V7GxsUpOTg7Bynx39OhR3XfffTrvvPP06quvasiQIercubN+97vfady4cVq5cqV+/PFHrVy5stLWdP7556t58+aV9nkOh0Nt27ZV27Zt1b59e3Xr1k3333+/nnrqKW3dulXPPvtspa0FQGSIhPPDSy+9pPT0dK1cuVLXXXedLr30Ul199dV65pln1Lx5c82ePTvUS6xQ5rkhKSnJr/fVr1/fc07p2LGj+vTpo2XLlunKK6/UjBkz9Ouvv1bQigHAd5FwnpKkBg0a6Jxzzik2jq+++koHDhxQSkpKCFYGANErUs4xu3fv1hdffKH7779fNWrU0EsvvRTqJQFBQQIjyiQkJKhbt27FPtnzxhtv6Oqrr5bd7l2Yk5OTo/nz5+vqq69Wq1atdNVVV+mZZ56R2+322u/FF19U79691bp1aw0cOFD79+8v8hn79+/XuHHjdOmll6pNmzYaPHiwvv/+e79iWL16tdLS0jR9+nQlJCQUeb1169YaPHiwqlWr5tnWs2dPPf744xo8eLDat2+vhx9+WJL0448/6u6771anTp3UokULXXHFFfrrX/+q7Oxsr/inT5+u3/3ud2rXrp0efPDBIm2aimsh9fLLL+uaa65Ry5Yt1b17d82dO1f5+fle7xkyZIjWrl2r3r17q2XLlrr22ms9LTs+++wzT1XH7bff7lMZea9evdS6dWu9+OKLXtvffvttXX/99WrVqpV+97vf6a9//atXm62cnBw98sgj6tq1q1q2bKmrr766SBIkLS1NkyZN0uWXX6527dppwIAB2rp1q+d1t9utZ555Rr169VLLli3Vu3dvPffcc17HGDRokCZPnqxnnnlG3bt3V6tWrXTLLbfo66+/liStW7dODz74oCTpyiuvDEp5JgDfRML54ciRI7JYLEU+32azafz48frjH/8oqaDNk1nFdmaptK/xvP7667r++uvVpk0bde/eXX//+9+Vm5vrtc97772na6+9Vq1atVLv3r21fv16r9d9Of988sknuvnmm9WuXTt17NhRo0aN0q5duyQVnEP++c9/at++fZ5y8uJKy3/++Wfdc889uvTSS9WxY0fddddd+umnn8r8Li0Wi8aPH6+8vDy98sornu05OTn629/+5mnR1a9fP73xxhte7/3uu+80ePBgXXLJJWrXrp2GDBni+Tlv+vjjjzVgwAC1a9dOXbp00cMPP6wTJ054Xi/r74MZ68aNG3XPPfd4vqPJkyfr1KlTkgrOOZs3b9bmzZspMwciQCScp0xXX3213nzzzSLreOONN3T55ZerVq1aXttdLpeef/559evXT61bt1b37t01c+ZMr2uSBx54QEOHDtVLL72k3//+92rdurVuueUW7d69W//973/Vr18/tWnTRjfddJN++OEHr+Nv2bJFAwcOVJs2bXTppZdq4sSJOnr0qOf1devWqXnz5vr666918803q1WrVurevbuWLFni2cdsnzhv3jzPf5fUVvHMc6/58/zNN9/UqFGj1LZtW11++eVasGCBMjIyNGnSJF1yySW6/PLL9fe//50ZGwAqRKScY9auXasaNWqoc+fOuvrqq7V27doi1ymStHnzZt18881q06aNevfurU8++aTIPr/++qsmTJigLl26qEWLFurcubMmTJigY8eOefbJy8vTzJkz1bVrV7Vu3VpDhw7V+vXrlZqa6nkI6oEHHtDgwYM1ZcoUdejQQf3791d+fr6OHj2qRx55RD169FDLli116aWXavTo0UUenvLl+0PkI4ERhfr06aOvv/7a6//0GRkZ+uCDD9S3b1+vfQ3D0IgRI7R06VLdeOONWrRoka6++mo9/fTTmjJlime/VatWacqUKbriiiu0YMECtWnTpkgLoKNHj+qWW27Rd999p4ceekhPPvmk3G63BgwYoJ07d/q8/nfeeUepqam66KKLStxn4sSJRVpePP/8855/LP/f//2fDh06pAEDBigrK0szZszQkiVL9Ic//EHPPfecV8uJP//5z1qzZo3uuusuPf300zpx4kSZLSkWL16shx56SJ07d9aiRYs0YMAALVmyxJM4Mf3vf//TsmXLdM8992j+/Pmy2+265557dOLECbVo0cKz/8MPP+z1fZemS5cuOnDggGeGxL/+9S+NHj1aF154oebPn6+7775bGzZs0KhRozwXANOmTdP777+viRMnep66feKJJzw3wTIzM3XLLbfok08+0fjx4zVv3jxVq1ZNd955p+d/u6lTp2rOnDm69tprPX9PHn/8cc2fP99rfW+++abeeecd/eUvf9GsWbN05MgR3XPPPXK5XOrevbtGjhwpqeDiZ9SoUT7FDCA4qvr5oXv37srOztYf//hHLVu2TN9//71cLpck6Xe/+50GDx4sSbrpppt04403SpLWrFmjm266yed4XnzxRY0bN07NmjXTvHnzNHz4cK1evVpTp071WsvDDz+sIUOGaOHChapXr54eeOAB/fjjj5Lk0/nnl19+0ciRI9WiRQstXLhQf/3rX7Vr1y4NGzZMbrdbo0aNUrdu3VS3bl2tWbNG3bt3L/J9HDp0SDfddJN27dqlKVOmaObMmTpx4oSGDBnidWOqJE2aNFGDBg08yWrDMDR69Gi9+OKLuuOOO7Rw4UK1a9dO9913nydBk5GRoTvvvFOJiYmaM2eOnnrqKWVlZWno0KE6efKkJOn999/XnXfeqVq1aumpp57Sn//8Z7377ru65557JPn392HKlCk699xztWDBAt15551au3atFi1a5HmtefPmat68udasWaMWLVqUGTOA8FbVz1NnxnHo0KEiDwP9+9//1jXXXFNk/4cffliPP/64evbsqYULF2rAgAFatWqV17/npYIKjueee04PPPCAHn/8ce3YsUPDhg3T9OnTNXz4cE2fPl2//fab7r//fs97Pv/8cw0ZMkRxcXF6+umnNWnSJG3evFm33367V1Ld7XZr7Nix6tOnj5555hldcsklmjlzpj788ENJBedTqaAtofnf/pg8ebJSUlK0cOFCderUSbNnz9aNN96ouLg4zZ49Wz179tTSpUuLfToaAIKhqp9jXC6XXn31VfXp00cOh0PXX3+90tLS9Pbbb3vt99133+lPf/qTqlevrtmzZ2vw4MEaN26c1z5ZWVm6/fbbtXPnTk2ZMkXLli3TwIED9dprr2nWrFme/R5++GGtXLlSAwcO1Pz581WnTp1i24Fv2bJFe/fu1dy5czV69GjZbDYNHz5cH3/8scaPH69ly5Zp1KhR+uSTT7zum/ny/SFKGIgaAwcONAYOHGhkZWUZbdu2NZYtW+Z5bd26dUbXrl0Nt9tt9OjRw5g4caJhGIbx3nvvGSkpKcarr77qdaz58+cbKSkpxk8//WS43W6jc+fOxpgxY7z2efjhh42UlBTj008/NQzDMGbNmmW0atXK+PXXXz375OTkGFdeeaXnvb/88ouRkpJirF27tsQ42rdvX+SzDMMw8vLyivwy9ejRw+jevbvhcrk82z788ENjwIABxsmTJ72O07dvX+NPf/qTYRiGsX37diMlJcVYtWqV53WXy2X06dPHSElJ8WybOHGi0aNHD8MwDCM9Pd1o06aN8fDDD3sd96WXXjJSUlKM7du3e96TkpJi7N2717PP5s2bjZSUFOPf//63YRiG8emnn3p9h2d/VnFWrVplpKSkGF999ZXhdruNrl27GkOHDvXa55NPPjFSUlKM//73v4ZhGEbv3r2NyZMne+0zb94849133/UcMzU11fjhhx88r2dnZxtXX3218cILLxi7du0yUlNTjcWLF3sd46mnnjJatWplHD161DCMgr+Dbdq08frO//nPfxopKSnGt99+axiGYaxdu9ZISUkxfvnllxJjBBBckXJ+MIyCn1ft27c3UlJSjJSUFKN9+/bG6NGjjQ8//NBrvzlz5nj9HPclHpfLZVx++eXG6NGjvfZZvny5ce211xo5OTme477//vue1/fs2WOkpKQYK1euNAzDt/PPa6+9ZqSkpBgHDhzwvP71118bs2bN8rzv7PPB2d/RjBkzjNatWxuHDh3y7HPw4EGje/fuxjvvvGMYhmGkpKQYc+bMKfH7vOGGG4yrr77aMAzD+Oijj4yUlBTj9ddf99rn/vvvN373u98ZeXl5xpdffmmkpKQYW7Zs8by+d+9e44knnjD2799vGIZhXH/99cZ1113ndYx///vfxlVXXWUcOHDAr78P999/v9dxBg0aZPTt29fzZ/PvNoCqLVLOU2f+3P79739vPProo57XPvvsM6NVq1bGyZMnvX52/fTTT0ZKSoqxYMECr2OtX7/eSElJMd577z3PsVNSUowdO3Z49nnooYeMlJQU45NPPvFsW7ZsmZGSkmKcOHHCMAzDuPnmm42+ffsa+fn5nn127dplNGvWzHMNZP77/KWXXvKKv1WrVl4xnH1OOftcW9x+5vc2duxYz+uHDh0yUlJSjNtuu82zze12G+3btzf++te/lvDtAkBgIuUc8+6773ruBZn69OlT5N/CY8aMMa644gojJyfHs+3111/3+tn8/fffG7feeqvX/SrDMIzhw4cbV111lWEYBf/GT01NNZ599lmvff70pz953dMxz0979uzx7HPgwAFj0KBBxueff+713scee8xo0aKFYRiGz98fogMVGFEoLi5OPXv29CqNe/3119WnTx9ZLBavfTdv3iybzaY+ffp4bb/22mslFbQ52rVrl9LS0nTllVd67fOHP/zB68+bNm1Ss2bNlJycrPz8fOXn58tqtapr167FlquV5OxyPEnKz89XixYtivw6U5MmTWS1nv4r36VLF61atUqxsbGe0upFixbp6NGjnhK7LVu2SJJXbFarVb179y5xfV9++aWysrLUs2dPT5z5+fmeFlMff/yxZ9+kpCSvYav169eXVJDtLi+LxaJdu3bpwIEDRdbSsWNHVa9e3bOWyy67TC+//LLuuusurV69Wvv27dPo0aPVo0cPz/fQsGFDXXzxxZ7jx8bGauPGjbrlllv06aefyjCMYmPOycnxerqsadOmql69uufPZh/JYMQMoHyq+vlBkgYMGKCPPvpI8+bN04ABA9SgQQO99dZbGjp0qGbMmFHi+3yJZ/fu3Tpy5Ih+//vfe+0zZMgQvfrqq3I4HJ5tZw5mNQfnpaenS/Lt/NOmTRvFxsbqxhtv1PTp0/XJJ5/o4osv1n333ef1M7Q0W7duVdu2bVW3bl3Ptnr16um///1vkbaHpTH/t9+0aZMsFou6detW5Gf94cOH9dNPP+miiy5SUlKSRo4cqSlTpujdd99V3bp1NWHCBDVo0EDZ2dn67rvvinyHvXv31ptvvqnk5GS//j60bdvW68/169f3apEIILJEwnnqzM84s43U66+/ru7duxf5Gb9582ZJUr9+/by2X3PNNbLZbF7t8WrWrKkmTZp4/mz+/D/zZ6XZnio9PV1ZWVn6+uuv1a1bNxmG4YntvPPOU5MmTbyuWySpXbt2nv92OBxKSkoK2s/cM49trrtNmzaebRaLRTVr1vRU8wFAsFX1c8zatWvVqFEjNW7cWOnp6UpPT9cf/vAHbd682auSY+vWrbriiiu8rl2uuuoq2Ww2z5+bNWum1atXq2HDhvrll1/04Ycf6tlnn9WuXbuUl5fnidEwDF199dVe6zi7WkUq+G7PvPeVnJysf/zjH+rQoYP279+vTZs2adWqVfriiy88x/f1+0N0sJe9CyLRH/7wB09vuWrVqmnTpk0aO3Zskf1OnDihxMTEIr3+zH9Unjx50tOz+uyhoWfeMJGk48ePa+/evSW2cPD1BnbDhg097ZFMdrvdq0f3Sy+9VGRYUZ06dbz+7Ha7NWvWLD3//PPKzMxUgwYN1Lp1a8XGxnr28TW2Mx0/flySNGzYsGJfP3TokOe/4+PjvV4zT4rFJWl8ZR4/OTnZ0zvwkUce0SOPPFLivpMnT1b9+vW1YcMGz37t2rXTww8/rObNm+v48eOqXbt2iZ9pxlxcybskHTx40PPfZ8dsJpXKEzOA4KnK5wdTfHy8evXqpV69ekmS9u7dq8mTJ2v58uW6/vrrix2O6ks85s+60n4ems6c0WT+nDMK23z4cv5p2LChVq1apWeeeUYvvfSSVqxYIafTqdtuu0333nuvV0K+JMePH1fDhg3L3K80Bw8e9LRsPH78uAzDUPv27Yvd99ChQ2rWrJmef/55LVy4UG+88YZefPFFxcfH69prr9XkyZN14sQJGYZR5jnF178PxZ1TDPqjAxEtEs5TUkGrksWLF2vLli265JJL9J///KdIO0IzjuLWZLfblZiY6HVDv6QE99k/K03p6elyu91asmSJ1zwL05nnJangBtSZgvkzt7i1l7RuAKgoVfUcc/ToUb333nvKy8tTx44di7y+Zs0aTZo0ybP2s9dknlPOtHz5ci1evFjHjh1TnTp11KJFC8XHx3vOO2ZL2rP/XX/2vTdzn7OTQBs2bNCsWbP022+/qVatWrr44ou9zjOB3I9D5CKBEaW6du2qGjVq6M0331SNGjXUsGFDtWzZssh+NWvW1LFjx5Sfn+/1g9m88Z2YmOj5IZeWlub1XvNGj6lGjRq69NJLNWHChGLXdGb2tzQ9e/bU4sWL9csvv3ieapWkVq1aef77vffeK/M4zzzzjFasWKGpU6eqd+/eqlGjhiR5+qJL8sR25MgRnXPOOZ7tZ8d2JqfTKUmaOXOmLrjggiKvF/fDPJg++eQTNWrUSMnJyZ6nfSdMmKBLL720yL41a9aUVPDdjxw5UiNHjtT+/fv13//+VwsWLND48eO1ceNG1ahRo8ggJamg2qR69eqemFeuXOk1PN105ncHILxV1fODy+VSr169dN1113lmKZgaNWqkyZMn67rrrtOOHTuKTWD4Eo/5s+7s+RHHjx/Xd999V6QaoCS+nH8kqXXr1po3b55yc3O1detWrVmzRosWLVJqamqRp72KU6NGjWJnXWzatEkNGzb0OocWZ+fOnTp06JBuu+02z/ESEhL0j3/8o9j9GzVqJEm68MIL9fe//10ul0vffPONXn31Vb3wwgtq2LChBgwYIIvFUmRdubm52rRpk1q3bh20fy8AiExV9Tx1tosvvliNGzfWv//9b+Xl5SknJ6fYeUbmv9cPHz7slZTOy8vTsWPHitxw8ke1atVksVg0ZMiQYh9EKm8CwbxZ5XK5PE/2njp1qlzHBICKVFXPMa+++qry8vI0b948zzWLaf78+Vq/fr3GjRunuLg41apVS0eOHPHaxzAMT8JAKpilOmPGDI0fP1433nijJ4lw77336ttvv5V0uptGWlqaGjRo4Hnv2fEWZ8uWLZ7ZtUOHDvV0I/nb3/7m6eDh6/eH6EALqSjlcDh05ZVX6j//+Y82btxY4pPzl156qVwul9544w2v7Rs2bJAkXXLJJbrgggvUoEGDIgPV/vvf/xY51u7du9W4cWO1atXK82vDhg16+eWXvcrVSjNw4EAlJSVp4sSJysjIKPK6y+XSrl27yjzO1q1b1bRpU914442em0cHDx7U9u3bPdUAnTp1kqQyYztTmzZtFBMTo4MHD3rFGRMToyeffLLYREBJfP1OTO+9956++eYb3XrrrZIKbiLVrl1bv/76q9da6tevryeffFLff/+9srOz1bt3bz377LOSCpINAwYM0DXXXKMDBw5IKmiF8ssvv2jbtm2ez8rNzdWYMWP00ksveTL8x44d8/qc48eP6+mnn/brBOPLU8UAKk5VPT/YbDbVq1dPa9eu1bFjx4q8vnv3bknyJC/O/lnjSzwXXnihEhMT9c4773jt869//Ut33XWXcnJyylyn5Nv5Z8WKFerZs6dyc3PlcDjUuXNnPfbYY5Kk3377rdgYztahQwd99dVXXv/oP3r0qO66664iMRRnzpw5iouLU//+/SUVfEeZmZkyDMPrf6effvpJ8+fPV35+vv7973+rU6dOOnz4sGw2m9q1a6epU6fK6XTqwIEDqlatmpo1a1bk8z/66CMNGzZMBw4cCNq/F3z5jgBUPVX1PFWcPn366K233tLrr7+uXr16Fal4MD9bKjjXnOn111+Xy+XSJZdcEtBnSwVVD82bN9euXbu84rrooos0b948r/ZUvjj7Z65ZVWGetyTpiy++CHi9AFDRquo5Zt26dWrbtq169eqlyy67zOvXrbfeqhMnTnhaY3Xu3FkffPCBV2XHhx9+6GndJBVcr9SoUUPDhg3zJC9OnTqlrVu3eq5XLrnkEtlsNv3nP//xWsvZfy7Ol19+KbfbrXvuuceTvHC5XJ6WWW632+fvD9GBCowo1qdPHw0fPlxWq1V/+ctfit2na9euuuyyyzRlyhQdOnRIzZs31+bNm7VkyRL1799fTZs2lSTdf//9Gj9+vP7yl7/o6quv1ldffaUXXnjB61hmj/AhQ4boT3/6kxITE/XGG2/opZde0oMPPujzuuvWrau5c+fq3nvvVb9+/XTzzTerZcuWslqt+u677/TKK69oz549RfrEnq1169ZasGCBnnnmGbVt21Z79+7V4sWLlZub6/lB3qhRI91888166qmnlJ+fr2bNmunVV1/1upF/tsTERN15552aPXu2MjIydNlll+ngwYOaPXu2LBaL1xyJspg3tt577z3VrFnT897c3Fx99dVXkgoy5enp6dqyZYv+8Y9/6LLLLtPAgQMlFdzQu++++/Twww/LZrOpR48eSk9P14IFC3Tw4EG1aNFCcXFxatGihebNm6eYmBilpqZq9+7d+uc//+mZ9XH99dfrueee08iRI3XvvfcqKSlJzz//vLKzszVo0CCdf/75uvbaa/XQQw9p3759atmypXbv3q2nnnpKDRs2LLYSpSTm0wJvvfWWunbt6tXHF0DlqKrnh7/85S8aNGiQrr/+et1+++1q1qyZ3G63Pv/8c61YsUK33HKLZ13mz5rXXntNbdq08TmeMWPG6NFHH9XUqVPVq1cv7dmzR08//bRuvfXWIuXNJfHl/NOpUyfNnDlTo0eP1sCBA2Wz2fTiiy/K4XB45hM5nU4dOXJE77//vpo1a1bkc4YMGaL169dr6NChGjFihGJjY7V48WLVq1dP1113nWe/AwcOeM4p+fn5OnjwoP75z3/qo48+0qOPPuq5qOjWrZs6duyoUaNGadSoUWrSpIm++eYbzZ07V126dFFSUpLat28vt9ut0aNHa9iwYapWrZo2btyokydP6qqrrpIk3XPPPRo5cqTGjh2r66+/XkePHtWTTz6pHj16qFmzZkpKSgrK3wfzO/ryyy+1adMmNW/e3PMkM4Cqraqep4qLY/78+dqwYYMWLFhQ7D5NmzZV//79NW/ePGVnZ+uyyy7TDz/8oHnz5umyyy7TFVdcEfDnS9K4ceM0bNgwjR8/Xtdee61cLpeeffZZff311xo5cqRfxzJ/5n7++efq0KGDunXrpunTp+uhhx7SXXfdpQMHDmjevHnFVmwDQLioaueYb775Rtu3b9fkyZOLff3KK69UzZo19eKLL6p///4aPXq03n77bQ0dOlR33nmnjh07pqeeekoxMTGe97Ru3VovvPCCZsyYoR49eujQoUNatmyZjhw54vn39HnnnacbbrhBs2bNUl5eni6++GK99dZbngRDaQ8StW7dWpL06KOP6oYbblB6erpWrVqlH3/8UZKUmZmp6tWr+/T9ITqQwIhil19+uZxOpxo0aFDiTWKLxaLFixdrzpw5+sc//qGjR4+qYcOGuu+++3THHXd49uvbt6+sVqsWLFigV199VSkpKXr00Uc1btw4zz7Jycl68cUX9eSTT2rq1KnKycnRBRdcoGnTphVpm1GWSy65RBs2bNALL7ygN998U0uXLlVubq4aNGigTp066amnnlLz5s1LPcbw4cN17Ngx/eMf/9D8+fPVoEED/d///Z8n5hMnTqhmzZqaMmWK6tSpo1WrVunEiRO64oorNGLECD399NMlHnvs2LGqW7euVq9eraVLl6pmzZrq3Lmzxo0b50lK+OKiiy5S37599fzzz+vDDz/Ua6+9JqmghPzmm2+WVPC/UWJios477zxNmDBBN910k9eJ56abblK1atW0dOlSrVmzRgkJCWrfvr1mzpzpaR/y6KOP6umnn9azzz6rw4cPq3bt2rrxxht17733Sip4emrVqlX629/+pmnTpik/P19t2rTRc8895xnENH36dC1evFgvvviiDhw4oNq1a6tPnz4aO3asX0+lXXbZZbr88sv15JNPatOmTXrmmWd8fi+A4Kiq54eWLVtq/fr1Wrx4sVatWuWpAmjatKkmTZrkdayrrrpKr776qh544AHdeOONmjp1qk/xDBgwQAkJCVq2bJleeeUVJScn609/+lOJc4+K48v55+KLL9aiRYs0f/58jRs3Ti6XSy1bttSzzz6rCy+8UFJBcvn999/X6NGjdc899xRpK9WgQQOtXr1af//73/Xggw/K4XDo0ksv1d///nfPEFdJeuWVVzxzpGJiYlSvXj21bNlSq1at8hpGbrVa9cwzz2j27NlavHix0tLSlJycrCFDhmj06NGSCoaEL126VLNnz9bkyZOVlZWliy66SHPnzvVUNfbo0UOLFy/W3LlzNXr0aCUmJuoPf/iD55wTzH8vDBgwQP/73/901113afr06WU+3ACgaqiq56mzNW3aVCkpKTp8+LAuv/zyEvebNm2aGjVqpLVr12rZsmWqV6+eBg0apNGjR5e70qxLly5atmyZ5s2bp3vuuUcxMTFq0aKFli9f7nNrRNOIESO0YMEC3XXXXXrjjTfUuHFjPfHEE1q4cKGGDRumJk2a6LHHHvNUFAJAOKpq55i1a9cWO1Dc5HA49Ic//EEvvviifvjhBzVr1kyrVq3SjBkzdN9996l27dqaOHGiZsyY4XlP//799euvv2rt2rVavXq1kpOT1a1bN91222166KGHtGPHDjVt2lQPPfSQEhIS9OyzzyojI0OdO3fWyJEjNX/+fK+ZgGe77LLL9PDDD2v58uX697//rTp16uiyyy7TvHnzNHr0aG3dulXdunXz6ftDdLAYTDoEAAAAAAAAAPjg+PHj+uCDD3TFFVd4zWN64okntG7dOr/bEAKloQIDAAAAAAAAAOCT+Ph4TZs2Tc2aNdPgwYOVkJCgL774Qs8995xGjBgR6uUhwlCBAQAAAAAAAADw2Q8//KCnn35aX331lbKysnT++efrlltu0YABA2SxWEK9PEQQEhgAAAAAAAAAACDslG/iFwAAAAAAAAAAQAUggQEAAAAAAAAAAMIOCQwAAAAAAAAAABB2SGAAAAAAAAAAAICwYw/1AspiGIbc7sDnjFutlnK9P9wRX9UX6TESX2hZrRZZLJZQL6PSlefcEe7/m1aUaIw7GmOWojPuaIxZCjxuzh3+i9a/YxWJ7zS4+D6Dj+/0NM4bgYmkv0PEEp6IJTxFUixS5VxzhH0Cw+02dPToqYDea7dblZhYTenpmcrPdwd5ZaFHfFVfpMdIfKGXlFRNNlv0XUwEeu6oCv+bVoRojDsaY5aiM+5ojFkqX9ycO/wTrX/HKhLfaXDxfQYf36k3zhv+i6S/Q8QSnoglPEVSLFLlXXPQQgoAAAAAAAAAAIQdEhgAgEqzYMECDRo0yGvbDz/8oIEDB6pt27bq3r27li1bFqLVAQAAAAAAIJyQwAAAVIoVK1Zozpw5XtuOHTumO+64QxdccIHWrl2rMWPGaPbs2Vq7dm2IVgkAAAAAAIBwEfYzMAAAVdvBgwc1efJkbd26VY0bN/Z67aWXXpLD4dDUqVNlt9vVpEkT7d27V0uWLNENN9wQohUDAAAAAAAgHFCBAQCoUN99951q1qypDRs2qE2bNl6vbdmyRR07dpTdfjqf3qlTJ+3evVtpaWmVvVQAAAAAAACEESowAAAVqmfPnurZs2exrx04cEApKSle2+rVqydJ2r9/v2rXrl3h6wMAAAAAAEB4IoEBAAiZ7OxsORwOr22xsbGSpJycnHId2273v8jQZrN6/R4tojHuaIxZis64ozFmKXrjBgAAABBZSGAAAEImLi5Oubm5XtvMxEVCQkLAx7VaLUpMrBbw+53O+IDfW5VFY9zRGLMUnXFHY8xS9MYNAAAAIDKQwAAAhEz9+vV16NAhr23mn5OTkwM+rtttKD090+/32WxWOZ3xSk/PksvlDvjzq5pojDsaY5aiM+5ojFkqX9xOZzyVGwAAAADCAgkMAEDIdOzYUS+++KJcLpdsNpskadOmTWrcuHG551/k5wd+o9Llcpfr/VVVNMYdjTFL0Rl3NMYsRW/cAAAAACIDj1YBAELmhhtuUEZGhiZPnqwdO3Zo3bp1WrlypYYPHx7qpQEAAAAAACDESGAAAEKmdu3aWrp0qXbv3q3+/ftr3rx5mjBhgvr37x/qpQEAAAAAACDEaCEFAKg0M2bMKLKtdevWWrNmTQhWAwAAAAAAgHBGBQYAAAAAAAAAAAg7JDAAAAAAAAAAAEDYidgExpETWVr22vfaeyA91EsBAFQRX/10RAvXfq18lzvUSwEAVBHr3t+pNz/dG+plAACqiBMZOVr22vf66ZdjoV4KAFQJEZvA+Oz7g3r/q/3696Y9oV4KAKCKWP/hLr3xyR5t//l4qJcCAKgCTmTkaP2Hu7Vsw7ehXgoAoIr48qcjev+r/drwwa5QLwUAqoSITWAYRsHvObmu0C4EAFBlmJUX+W4jxCsBAFQlWTkuGQbnDgBA2axWiyTpVHZeiFcCAFVDxCYwbLaCE4KLm1AAAL9x7gAAlM1uP305xXUHAMAXjpiCcwcP3AKAbyI3gWEtCI0+5gAAX1ksllAvAQBQhcTYTl9O5eVz3QEAKFus3SaJBAYA+MrvBEZaWpr+/Oc/q1OnTmrXrp2GDRumHTt2eF7/4YcfNHDgQLVt21bdu3fXsmXLgrpgX9kKS/JIYAAA/EUXEACAL86swOC6AwDgC4ejMIGRRwIDAHzhdwJj5MiR+uWXX7RkyRK98soriouL05AhQ5SVlaVjx47pjjvu0AUXXKC1a9dqzJgxmj17ttauXVsRay+V3Wwh5eIuFADAN9RfAAD8YbVYPA9OUYEBAPCFWYGRnZsf4pUAQNVg92fnY8eOqWHDhho5cqQuuugiSdKoUaP0f//3f/rpp5+0adMmORwOTZ06VXa7XU2aNNHevXu1ZMkS3XDDDRUSQEloIQUAAACgotltVrncLq47AKCCLFiwQJs2bdJzzz0nSRo0aJA2b95c7L5PPPGErrvuumJf69mzp/bt2+e1rV+/fpo5c2ZQ11sWZmAAgH/8SmAkJiZq1qxZnj8fOXJEy5YtU/369dW0aVPNnTtXHTt2lN1++rCdOnXS4sWLlZaWptq1awdv5WWwUYEBAPAXJRgAAD/Z7Vbl5LmowACACrBixQrNmTNHHTt29GybO3eu8vLyvPb7y1/+op9//lm///3viz1ORkaG9u/fr8WLF6tFixae7XFxcRWz8FLExpgVGCQwAMAXfiUwzvTQQw/ppZdeksPh0MKFC5WQkKADBw4oJSXFa7969epJkvbv3x9wAuPM3rK+chSeEPLdbtlskTmr3IyL+KquSI+R+AAAQKRz2K06JSmPCgwACJqDBw9q8uTJ2rp1qxo3buz1Wq1atbz+/Nprr+mjjz7SunXrVL169WKPt337dhmGofbt28vpdFbUsn1i3q/KyXPJYPgeAJQp4ATG4MGDdfPNN+uFF17Q6NGjtXr1amVnZ8vhcHjtFxsbK0nKyckJ6HOsVosSE6v5/b6azoIsustlyOmMD+izqwriq/oiPUbiQ1XDdQQAwFf2wgcZ8vM5eQBAsHz33XeqWbOmNmzYoPnz5xdp/WTKzMzU3/72Nw0ePFipqaklHm/btm2qW7duyJMX0ukKDLfbUL7LoAgcAMoQcAKjadOmkqTHHntMX331lVatWqW4uDjl5uZ67WcmLhISEgL6HLfbUHp6pt/vy8oqWEe+y6309Cy5IvCJKJvNKqcznviqsEiPkfhCz+mMp0LEDxYuHwAAfrIXtq5lBgYABE/Pnj3Vs2fPMvd78cUXderUKY0cObLU/bZv366EhASNGTNGX375pZKSknT99dfr9ttvl9VauddL5gwMqaAKI64woQEAKJ5fCYy0tDRt2rRJf/jDH2SzFfyAtVqtatKkiQ4dOqT69evr0KFDXu8x/5ycnBzwIvMD6CdrKXwAKt/llsvlDugYVQXxVX2RHiPxoaoxxFO0AADfxBS2u2UGBgBULpfLpeeee0633XabatSoUeq+P/30k06ePKk+ffro7rvv1pYtWzRz5kydOHFC9957b8BrCKTlud1ulc1qkcttKN9tBHSMcBJJrZWJJTwRS/iqrHj8SmAcOnRI48ePV+3atdW5c2dJUl5enr7//nv17NlTderU0YsvviiXy+VJcGzatEmNGzeu1AHe0ukvLp8h3gAAH1kowAAA+MnTQooKDACoVJs3b9b+/fv1xz/+scx9ly9frpycHM+MjNTUVJ06dUoLFy7UmDFjAqrCCLTluSTFOWw6lZ2vGIc94GOEm0hqrUws4YlYwldFx+NXAuPiiy9Wly5d9Mgjj+ivf/2rnE6nFi1apPT0dA0ZMkSxsbFaunSpJk+erDvvvFPffPONVq5cqUceeaSi1l8im7XgLlS4tnUBAIQxct8AAB/F2KjAAIBQePvtt9W6dWudd955Ze4bExOjmJgYr20pKSnKzMzUiRMnlJiY6PfnB9ryXCoY5H0qO19Hj2WqWkzVfhK7KrRW9hWxhCdiCV/liceflud+JTAsFouefvppPfnkkxo7dqxOnjypDh066Pnnn9c555wjSVq6dKmmTZum/v37q27dupowYYL69+/vVwDBYDN70bq5CwUAAACgYnhaSEXARSgAVCVbt27VFVdcUeZ+brdbv//973XTTTd5zcr49ttvVadOnYCSF6ZAWwk7Cs8dmdn5EdOOOJJaKxNLeCKW8FXR8fg9xLtGjRqaOnWqpk6dWuzrrVu31po1a8q7rnKzF5b/RUI2CwBQuUh9AwB8ZfYuj6SLUAAIdy6XSzt27NDQoUOLff3kyZPKy8tTUlKSrFarevfuraVLl+qCCy5QixYttGnTJk8HkVCIdRS0Xc/Nc4Xk8wGgKvE7gVFVnG4hxW0oAIBvmIEBAPCXp4UUD04BQKU5fvy48vLyVKtWrWJfnzZtmjZv3qx3331XkjR+/Hg5nU49+eSTOnDggBo2bKjJkyf7ND+jIjhiChIYOSQwAKBMkZvAMFtIcSEBAPCTYZD8BgD4hiHeAFCxZsyYUWRb7dq1tW3bNp/fY7fbNXLkSK8WUqEUG0MFBgD4qmpPCiqFWYHBhQQAwHeUYABAVZCXl6ennnpK3bt3V7t27XTbbbfpiy++CMla7PaCcwdDvAEAvnIUDu6mAgMAyhaxCYzTT0LxFC0AAAAQSRYuXKi1a9fqr3/9q9avX68LL7xQd911lw4ePFjpa/G0kCKBAQDwUaynhRTnDgAoS8QmMDwzMNycDAAAvmEGBgBUDe+884769u2rLl26qFGjRnrggQeUkZGhr776qtLXEmPnwSkAgH9oIQUAvovcBEbhk1CGIbndXEwAAAAAkaJWrVr673//q19//VUul0tr1qyRw+FQs2bNKn0tdjtDvAEA/vFUYOSSwACAskTuEG/r6cdo891uWelrDgDwETO8ASC8TZ48Wffdd5+uvPJK2Ww2Wa1WzZ49W+eff36lr8VsIZVPCykAgI88CYx8EhgAUJaoSGC4XIasNhIYAIDScaYAgKph586dcjqdmj9/vpKTk/Xyyy9r4sSJWrVqlS6++OKAj2tWU/jDUXgTyuU2Ano/ijKr6c3fUT58n8HHd4ry8gzxpgIDAMoUuQmMMxIW+S6358koAADKQgEGAISvffv26c9//rNWrFihDh06SJJatWqlHTt2aO7cuZo/f35Ax7VaLUpMrOb3+6pXiy34D0tg70fJnM74UC8hovB9Bh/fKQIV6zBnYFC9BwBlidgEhvWMSawuZmAAAAAAEeGbb75RXl6eWrVq5bW9TZs2+uCDDwI+rtttKD090+/3uQrbf2Rm5+nYsVMBfz5Os9mscjrjlZ6eJRezRcqN7zP4+E69OZ3xVKP4yWEvbCHFEG8AKFPEJjAsFovsNovyXYZcLhIYAAA/MAQDAMJWgwYNJEnbtm1T69atPdu3b9+uRo0alevYgcyxMFvX5uW5mIMRZC6Xm+80iPg+g4/vFIE6XYFBAgMAyhLRKXKbtSA8l5t/UAAAymaxMAUDAMJd69at1aFDB02cOFGffvqp9uzZo6efflqbNm3SsGHDKn09ZqvaPJ7CBgD4yJyfRAUGAJQtYiswpMI5GHm0kAIA+IezBgCEL6vVqgULFujpp5/Wgw8+qBMnTiglJUUrVqxQ27ZtK3095uBunsIGAPgqlgQGAPgsshMYheXctJACAPiC+gsAqBpq1qypKVOmaMqUKaFeiuy2whZSXHMAAHx0OoFB8hsAyhLRLaTsheXc+bSQAgAAAFABYgorMPLyeYoWAOCb2JiCcwczMACgbBGdwKACAwDgF7MEg9MGAMBH5gyMfK45AAA+Mod400IKAMoWFQkMLiYAAAAAVARmYAAA/OWwFyYwcklgAEBZIjqBYbaQctFCCgDgAwowAAD+Misw8lxccwAAfGNWYLjchvI5fwBAqSI6gWGz0UIKAAAAQMXxVGBwAwoA4CNHzOnbcbkM8gaAUkV2AsNqVmCQwAAA+M4wOG8AAHzjqcCghRQAwEcxNqsKu54rN582UgBQmshOYJgVGLSQAgD4wGKxlL0TAABnsJPAAAD4yWKxMMgbAHwU2QkMhngDAAAAqEAx9tNV324q+AAAPop12CXRQgoAyhLhCYzCiwn60QIAAACoAGYCQ+K6AwDgu9gYKjAAwBcRncCwe1pI8SQUAMB3PEALAPCV2UJKkvLyOYEAAHwTV9hCKpcEBgCUKqITGOYMDFpIAQB8wQgMAIC/zIemJCmfCgwAgI/iCltIUYEBAKWL6ASG3WwhxRBvAAAAABXAYrEwyBsA4DeGeAOAbyI6gWEO8aaFFADANwXnDc4aAAB/OGIKLquowAAA+CrW00KKcwcAlCayExjmDAxaSAEAAACoIOYg7zwSGAAAHzHEGwB8E9kJDCtPQgEAfOeZgcEUbwCAH2JsXHcAAPxjzsBgiDcAlC6yExg2WkgBAAAAqFgx9oKnaJmBAQDw1ekZGJw7AKA0kZ3AMGdg8CQUAMAHngKMkK4CAFDVxJgzMEhgAAB8FOeZgUEFBgCUJqITGPbCUm4qMAAAAABUFEdhH/NcEhgAAB+ZMzBIYABA6SI6gWG2kMongQEA8AenDQCAHxzmEG8SGAAAH8UWzsCghRQAlC6yExiFQ7xpIQUA8Iml7F0AADibw5yBwXUHAMBHcbFUYACALyI6gWG3MsQbAOA/gxIMAIAfzBZSVGAAAHxltpDKIYEBAKWK6ASGp4UUT0IBAHxgoQQDABAAc4g3CQwAgK/iCltIUYEBAKWL7ASGp4UUT9ICAAAAqBieQaz53IQCAPgm1mFWYJD8BoDSRHYCw0YLKQCA7yyFBRgGpw0AgB9iGOINAPCTmcAg+Q0ApYvoBIbdVliB4eZCAgAAAEDFYAYGAMBfzMAAAN9EdALDZg7xpoUUAAAAgArioAIDAOCnOLMCgxZSAFCq6Ehg0EIKAAAAQAWhAgMAKs6CBQs0aNAgr20PPvigUlNTvX517dq11ONs3LhRffr0UatWrdSvXz998MEHFbnsMsUWDvGmAgMAShfZCYzCFlL5Li4kAABlYwYGACAQJDAAoGKsWLFCc+bMKbJ927ZtGjFihD766CPPr/Xr15d4nE8//VR//vOfddttt2n9+vXq0qWLRo8erZ07d1bg6ktnVmDk5bvl5gIEAEoU0QkMOy2kAAAAAFQws4UUg1gBIDgOHjyoO++8U7Nnz1bjxo29XnO5XNqxY4datWqlunXren4lJSWVeLwlS5aoV69eGjhwoJo0aaKJEyeqRYsWWrlyZUWHUiJziLck5VKFAQAliugEhs1WkMDIp4UUAIS1vLw8PfXUU+revbvatWun2267TV988UUIVlJw3jDEeQMA4LsYKjAAIKi+++471axZUxs2bFCbNm28XtuzZ49ycnLUpEkTn47ldrv1xRdfqFOnTl7bL7vsMm3ZsiVoa/aXw35mAoPzBwCUxB7qBVQkm7UgP+NycyIAgHC2cOFCrV27VjNmzNB5552nJUuW6K677tIbb7yh5OTkUC8PAIBSxcYwxBsAgqlnz57q2bNnsa9t375dFotFK1eu1AcffCCr1apu3bpp7NixqlGjRpH909PTlZmZqfr163ttr1evnn777bdyrdNuD+y5YJvNKqvVotgYq3Ly3HK5jYCPFWpm+3bz96qMWMITsYSvyoonohMYdhstpACgKnjnnXfUt29fdenSRZL0wAMP6OWXX9ZXX32l3r17V/6COG0AAPwQY6cCAwAqy08//SSr1apzzz1XixYt0t69e/XEE09o+/btWrlypaxW7xtp2dnZkiSHw+G1PTY2Vjk5OQGvw2q1KDGxWsDvlwoGeefk5So23lHuY4Wa0xkf6iUEDbGEJ2IJXxUdj18JjOPHj2vWrFl67733lJGRodTUVI0fP14dOnSQJD344INat26d13uSk5P1wQcfBG/FfrCZMzBoIQUAYa1WrVr673//q4EDB6pBgwZas2aNHA6HmjVrVqnrMId4AwDgD3MGRp6LBAYAVLQxY8ZoyJAhcjqdkqSUlBTVrVtXN998s7799tsiLadiY2MlSbm5uV7bc3JyFB8f+E03t9tQenpmQO+12axyOuM954/DaRlyxtnKeFd4MmNJT8+Sq4qfB4klPBFL+CpPPE5nvM+VG34lMMaNG6e0tDTNmjVLSUlJWr16tYYOHap169apSZMm2rZtm0aMGKGBAweeEUjofgCbX0Ik/IUAgEg2efJk3Xfffbryyitls9lktVo1e/ZsnX/++aFeGgAAZXIUzsCghzkAVDyLxeJJXphSUlIkSQcOHCiSwKhVq5YSEhJ06NAhr+2HDh0q0lbKX/nlrLwzzx+Z2fnlPlaouVzuKh+DiVjCE7GEr4qOx+cExt69e/Xxxx/rhRdeUPv27SUV3HD64IMP9Nprr+nuu+/Wjh07NGrUKNWtW7fCFuwPswIjnxZSABDWdu7cKafTqfnz5ys5OVkvv/yyJk6cqFWrVuniiy8O6JiB9JC1FpZgWKyWKtuDNhCR1ofTF9EYsxSdcUdjzFL0xh0q5g0oKjAAoOKNHz9ex48f17Jlyzzbvv32W0lS06ZNi+xvsVjUvn17bd68WTfddJNn+2effaZLLrmk4hdcilhPAtwV0nUAQDjzOYGRmJioZ555Ri1btvRss1gsMgxDJ06c0J49e5STk6MmTZpUyEIDYTNnYDDEGwDC1r59+/TnP/9ZK1as8LQkbNWqlXbs2KG5c+dq/vz5fh8z0H609sIe5nFxVb8HbSAirQ+nL6IxZik6447GmKXojbuyOTxDvLkBBQAVrW/fvho5cqQWLlyoa665Rrt379ajjz6qvn37eu5JnTx5Unl5eUpKSpIk3XHHHRo2bJiaN2+url27au3atfrhhx80bdq0UIbiOX/kkMAAgBL5nMBwOp3q1q2b17aNGzfq559/VpcuXbR9+3ZZLBatXLlSH3zwgaxWq7p166axY8eqRo0aQV+4L+yFg5uYgQEA4eubb75RXl6eWrVq5bW9TZs2Ac9QCrQfbb6r4MIhKytXx46dCuizq6JI68Ppi2iMWYrOuKMxZqny+tGigIMh3gBQaXr06KHZs2dr0aJFWrRokWrUqKF+/fpp7Nixnn2mTZumzZs3691335UkdenSRY8//rgWLFigp556Sk2bNtWiRYtC/hBuLC0IAaBMfs3AONPWrVs1adIkXXnllerZs6fmzJkjq9Wqc889V4sWLdLevXv1xBNPaPv27Vq5cqWs1sAvggJt4xFTeCJwuYyIbAUS6a0BIj0+KfJjJD74okGDBpKkbdu2qXXr1p7t27dvV6NGjQI+biD9Fw2jIOHtdhsR1Y/SV5HWh9MX0RizFJ1xR2PMUvTGXdliPBUYfNcAEGwzZswosq13797q3bu3X++57rrrdN111wVzaeVmJjCowACAkgWUwHj77bd1//33q02bNpo1a5YkacyYMRoyZIhnkFJKSorq1q2rm2++Wd9++22RIUq+CrQNiCTlFF4/uAwjoluBRHprgEiPT4r8GIkPpWndurU6dOigiRMnasqUKapfv77Wr1+vTZs2afXq1ZW6Fosshf9F5R4AwHfmDSgSGAAAf5gzlHJpQQgAJfI7gbFq1SpNmzZNvXr10syZM+VwOCQVzMMwkxemlJQUSdKBAwcCTmAE2gZEKmgBIhU8hRuJrUAivSVCpMcnRX6MxBd6VaENiNVq1YIFC/T000/rwQcf1IkTJ5SSkqIVK1aobdu2oV4eAABliilsIeVyG3K53bKVo/ocABA9Yh2FFRi5JDAAoCR+JTBWr16txx57TIMGDdKkSZO82kKNHz9ex48f17Jlyzzbvv32W0lS06ZNy7XIQMveLYWtQFxuQ3l5LlksljLeUTVFemuASI9PivwYiQ9lqVmzpqZMmaIpU6aEdB3macKgAAMA4AdzCKsk5ecbsjlCuBgAQJURW3j+yOV6EgBK5POjQbt379bjjz+uXr16afjw4UpLS9Phw4d1+PBhnTx5Un379tXHH3+shQsX6ueff9b777+vSZMmqW/fviEbinTmU8cM8gYAAABQEcwKDIk2IAAA3zEDAwDK5nMFxptvvqm8vDy99dZbeuutt7xe69+/v2bMmKHZs2dr0aJFWrRokWrUqKF+/fpp7NixwV6zz2zW0xUXLrehM64rAAAAACAobFaLbFZLQeU3T9ECAHzkmYFBCykAKJHPCYwRI0ZoxIgRpe7Tu3dv9e7du9yLChab7YwEhsuQYkK4GAAAAAARyxFjVVaOiwQGAMBnngoMzh0AUKKIni5nt57ZQoqTAQAAAICKYbaRIoEBAPCVOUMplxZSAFCiiE5gWK0WmV2kmIEBACiLZ4h3aJcBAKiCYgrn7+W5SGAAAHxjVmCQwACAkkV0AkM6Pcjb5eJ2FAAAAICKwVO0AAB/McQbAMoW8QkMe+EcjHxaSAEAymBRwTnDMEh6AwD8E2OnAgMA4J/TFRicOwCgJBGfwLBZqcAAAAAAULE8MzC4CQUA8FGsgwoMAChLxCcw7GYLKWZgAADKYgn1AgAAVZWDCgwAgJ8czMAAgDJFfALDVthCykULKQAAAAAVxNNCKp/rDgCAb2IL5yflUL0HACWKggQGLaQAAL4xCzAYgQEA8JeZwMglgQEA8NGZFRjM4QOA4kV8AiPGU4HBiQAAAABAxaACAwDgL3OItyHOHwBQkohPYJyuwOBEAAAog6Ug6W2IpDcAwD8Oc4h3Pn3MAQC+MRMYEoO8AaAkEZ/AsFsLQsynAgMAAABABfG0kKKPOQDAR1arRXYb5w8AKE3EJzA8Q7yZgQEAKIM5A4MCDACAv8w+5rQAAQD44/QgbyowAKA4EZ/AMDPZLjcXEgAAAAAqhnkDKpcWUgAAP3gGeXP+AIBiRXwCw8YQbwAAAAAVLKZwBgYtQAAA/jATGDm5JDAAoDgRn8AwZ2DQQgoAUBazhRRnDACAv6jAAAAE4vT5gwQ4ABQn8hMYdnOINycCAAAAABXD0wKECgwAgB9iqcAAgFJFfALDZmWINwDAR4UlGAanDACAnzwtQBjCCgDwAzMwAKB0EZ/AOD3Em7tRAAAAACoGLaQAAIHwVGBQwQcAxYr4BIZniLeLEwEAoHQWpmAAAAJkDvHO4wYUAMAPjsIEOC2kAKB4EZ/AoAIDAAAAQEUzKzByGMIKAPBDLC2kAKBUEZ/AMGdg5JPAAACUwcIMDABAgE4P8eYGFADAdw47M5QAoDQRn8DwVGDQQgoAAABABSGBAQAIRKyjcIYSLQgBoFiRn8Cw00IKAAAAQMWKtZtDvLkBBQDwHRUYAFC6iE9gmC2kXC4SGAAAAAAqhlmBkZfvlptehAAAH8VSwQcApYr4BIbZQirfzZNQAIDSmTMwAADwlyPm9KVVHlUYAAAfxTrMBAbnDgAoTsQnMGy2wgoMWkgBAHzEg7MAAH+ZLUAknqIFAPjOUdiCkBZSAFC8iE9gnB7izd0oAAAAABXDarV4rj14ihYA4CtaSAFA6SI+geGZgUELKQBAmQrOGYZIegMA/BcbYw7y5iYUAMA35gwlKjAAoHgRn8DwVGDQQgoAAABABXLE0MccAOCfWM4dAFCq6Elg0EIKAFAGzxBvThkAEPbWr1+vPn36qFWrVrrmmmu0cePGUC/J08ecCgwAgK8cMczAAIDSREECo+BuVL6LTDYAAAAQCV599VVNmjRJN998s1577TX16dNH48aN05dffhnSdcXYeYoWAOAfTwUGyW8AKFbEJzBstJACAPiIAgwACH+GYWj27NkaPHiwBg8erEaNGmn06NG6/PLLtXnz5pCuzTMDg6doAQA+8szAyHXLMLgSAYCz2UO9gIpmVmCQwAAAAACqvl27dmnfvn3q16+f1/Zly5aFaEWneW5C8RQtAATNggULtGnTJj333HOebe+++67mz5+vXbt2KTExUb1799a9996ruLi4Eo/Ts2dP7du3z2tbv379NHPmzApbuy/M5LfbMORyG577WACAAhGfwLBZzRkYlHEDAMpQeK3Ag08AEL727NkjScrMzNTQoUP1/fffq2HDhho5cqR69uxZrmPb7f4XqJsV3zabVbGOggSGy2UEdCwUOPM7RfnxfQYf32nlWbFihebMmaOOHTt6tm3ZskV33323xo4dq969e2vv3r16+OGHdfz4cU2fPr3Y42RkZGj//v1avHixWrRo4dleWsKjspjJb6lgDoadv1cA4CXiExh2WkgBAAAAESMjI0OSNHHiRN199926//779eabb2rUqFFavny5OnfuHNBxrVaLEhOrBbwupzNe1RMckiRbjL1cx0IBpzM+1EuIKHyfwcd3WnEOHjyoyZMna+vWrWrcuLHXay+++KI6deqkYcOGSZIaNWqk++67T5MmTdIjjzwih8NR5Hjbt2+XYRhq3769nE5npcTgK7vNKpvVIpfbUG6eW9VCn1MBgLAS8QkMGy2kAAA+sohybQAIdzExMZKkoUOHqn///pKkZs2a6fvvvy9XAsPtNpSenun3+2w2q5zOeKWnZ3lK+E6kZ+nYsVMBrQPe3ymV9OXH9xl8fKfenM74oFejfPfdd6pZs6Y2bNig+fPne7V++tOf/iSrtejn5efnKyMjQ0lJSUVe27Ztm+rWrRt2yQuTI8amrJx85TBDCQCKiPgEhlmBkc8/KgAAPiPpDQDhqn79+pKklJQUr+1NmzbVe++9V65j5+cHfs3gcrkVU3jtkZWTX65joYDL5eZ7DCK+z+DjO604PXv2LLEtYPPmzb3+nJubq+XLl6tFixbFJi+kggqMhIQEjRkzRl9++aWSkpJ0/fXX6/bbby82GeKrQNsFnt2GLDbGqqycgodvq1oLwkhqqUYs4YlYwldlxRMFCQwqMAAAvrFQgAEAYa958+aqVq2avv76a3Xo0MGzffv27Tr//PNDuDLJUTiINZcbmgBQKfLz8zVhwgTt2LFDzz//fIn7/fTTTzp58qT69Omju+++W1u2bNHMmTN14sQJ3XvvvQF9dnlbD0qn25DFx8XoeEauHHExVbYFYSS1VCOW8EQs4aui44n4BIaZAXK5SGAAAHzDEG8ACF9xcXG68847NX/+fCUnJ6t169Z6/fXX9fHHH2vFihUhXZvDXjCINZcWIABQ4TIyMjR27Fh99tlnmjNnjtq0aVPivsuXL1dOTo6qV68uSUpNTdWpU6e0cOFCjRkzJqAqjEBbD0pF25DZrQVPUh1JO6VjtarWEIxIaqlGLOGJWMJXeeLxp/1gxCcw7FZziHfV/0sBAAAAQBo1apTi4+P11FNP6eDBg2rSpInmzp2ryy67LKTr8lRg5HHtAQAV6dChQ7rrrrv066+/asmSJerUqVOp+8fExHhmKJlSUlKUmZmpEydOKDExMaB1lLeFmNmGzDx/VOUWhJHUUo1YwhOxhK+KjifiExgM8QYAAAAizx133KE77rgj1Mvw4ogprMDIpwIDACrKiRMnNHjwYGVkZGj16tVKTU0tdX+3263f//73uummmzRy5EjP9m+//VZ16tQJOHkRTGYFH0O8AaCoiE9g2GkhBQAAAKASOOxUYABARZs+fbp++eUXLV26VElJSTp8+LDntaSkJNlsNp08eVJ5eXlKSkqS1WpV7969tXTpUl1wwQVq0aKFNm3apKVLl2ry5MkhjOS02BhaEAJASSI+gUEFBgDAV5bCKd4GQzAAAAGgAgMAKpbb7dYbb7yhvLw8DR48uMjr77zzjho2bKhp06Zp8+bNevfddyVJ48ePl9Pp1JNPPqkDBw6oYcOGmjx5sv74xz9WdgjFMltI5ZAAB4AiIj6BEVNYgZEfAYNRAAAAAIQvWoAAQPDNmDHD899Wq1XffPONX++RJLvdrpEjR3q1kAonVGAAQMl8G/VdhZnTzKnAAACUxVL4O2cMAEAgYh2FT9Dm8vAUAMB3ZgKDBDgAFBXxCQy72UKKGRgAAAAAKlBcTEGBO0/QAgD84WlBSAspACgi4hMYNmtBiG7DoKc5AKB0lrJ3AQCgJKd7mJPAAAD4LpbzBwCUyK8ExvHjx/Xwww+ra9euat++vW699VZt2bLF8/oPP/yggQMHqm3bturevbuWLVsW9AX7y6zAkGgjBQDwEacLAEAA4hwFT9BmcwMKAOAHTwVGPucPADibXwmMcePG6euvv9asWbP0yiuvqEWLFho6dKh27typY8eO6Y477tAFF1ygtWvXasyYMZo9e7bWrl1bUWv3iTkDQ6KNFACgdBRgAADKwzOENddF9TcAwGeeGRi5JDAA4Gx2X3fcu3evPv74Y73wwgtq3769JGny5Mn64IMP9NprrykuLk4Oh0NTp06V3W5XkyZNtHfvXi1ZskQ33HBDhQVQFu8KDLckW8jWAgCoGrjlBAAIRGxhBYYhKTff7bkhBQBAacwWhMxQAoCifK7ASExM1DPPPKOWLVt6tlksFhmGoRMnTmjLli3q2LGj7PbTOZFOnTpp9+7dSktLC+6q/WDOwJCkfCowAAClsVCDAQAInOOMhAV9zAEAvvJUYOQzxBsAzuZzBYbT6VS3bt28tm3cuFE///yzunTpoqeeekopKSler9erV0+StH//ftWuXTvwRdoDmzVus1lltVpktVjkNgzJEvixwpHZHuvMNlmRJNLjkyI/RuJDVUXbDwBAIKwWixwxVuXmuQvagCSEekUAgKrAcUYLQgCAN58TGGfbunWrJk2apCuvvFI9e/bU9OnT5XA4vPaJjY2VJOXk5AS8QKvVosTEagG/XypoI5Wbb6h6jXglJkbeVYTTGR/qJVSoSI9PivwYiQ8AAESL2BhbQQKDCgwAgI+owACAkgWUwHj77bd1//33q02bNpo1a5YkKS4uTrm5uV77mYmLhITAkwZut6H09MyA3muzWeV0xstqLWgJcvTYKTkskfNUrRlfenqWXK7IO8lFenxS5MdIfKHndMZTIeIHGkgBAMorNsamk8pjECsAwGfMwACAkvmdwFi1apWmTZumXr16aebMmZ6qi/r16+vQoUNe+5p/Tk5OLtci88uZgS64eedSTq6r3McKRy6XOyLjMkV6fFLkx0h8AAAgWpiDvKnAAAD4yqzAIIEBAEX59Vju6tWr9dhjj2nAgAF6+umnvVpGdezYUVu3bpXLdfqH7aZNm9S4ceNyzb8IBnthBUa4PiENAAgP5gxvRmAAAAIVZ7YBoQIDAOAjTwspEhgAUITPCYzdu3fr8ccfV69evTR8+HClpaXp8OHDOnz4sE6ePKkbbrhBGRkZmjx5snbs2KF169Zp5cqVGj58eEWu3yc2M4Hh5o4UAAAAgIrj4CYUAMBP5rkj32XI5ebhWwA4k88tpN58803l5eXprbfe0ltvveX1Wv/+/TVjxgwtXbpU06ZNU//+/VW3bl1NmDBB/fv3D/qi/WUv7P/ucpHAAAAAAFBx4gpbSGWTwAAA+Cg25vTzxbl5bsXHMscQAEw+JzBGjBihESNGlLpP69attWbNmnIvKthsNrMCgyw2AKBspLsBAIHy9DGnhRQAwEd2m1UWFVyH5OS5FB/r98haAIhYUZHSNVtI5dNCCgDC1vr169WnTx+1atVK11xzjTZu3Fjpa7CYQzAAAAhQLBUYAAA/WSwWORwM8gaA4kRHAoMWUgAQ1l599VVNmjRJN998s1577TX16dNH48aN05dffhmaBTHFGwAQIAaxAgACEWsvuHeVk0f3EAA4U3QkMKy0kAKAcGUYhmbPnq3Bgwdr8ODBatSokUaPHq3LL79cmzdvrtS1UH8BACgvTwKDFlIAAD+Yg7ypwAAAb1HRVM+TwKACAwDCzq5du7Rv3z7169fPa/uyZctCtCJmYAAAAme2kKICAwDgDyr4AKB4UVGBYS9sIZVPBQYAhJ09e/ZIkjIzMzV06FB17txZN910k959993KXwwlGACAcjp9A4prDwCA705XYHD+AIAzRUcFho0KDAAIVxkZGZKkiRMn6u6779b999+vN998U6NGjdLy5cvVuXPngI5rt/ufo7cWDvG2WCwBvb+qMmdFmb9Hg2iMWYrOuKMxZil64w4HtJACAAQiNsacgcH5AwDOFB0JDGvhEG83CQwACDcxMTGSpKFDh6p///6SpGbNmun7778POIFhtVqUmFjN7/c5HAWnxdhYe0Dvr+qczvhQL6HSRWPMUnTGHY0xS9EbdyjF0UIKABAAZmAAQPGiIoFh91RgUIYHAOGmfv36kqSUlBSv7U2bNtV7770X0DHdbkPp6Zl+vy83N1+SlJ2dr2PHTgX02VWRzWaV0xmv9PSsqDlXRmPMUnTGHY0xS+WL2+mMp3KjHBxUYAAAAsAMDAAoXlQkMGIK24Dk5UfPRSsAVBXNmzdXtWrV9PXXX6tDhw6e7du3b9f5558f8HHzA/iZbxiG5/dA3l/VuVzuqIs7GmOWojPuaIxZit64Q4kKDABAIByFLaRyOW8DgJeoSGA47IVleJwEACDsxMXF6c4779T8+fOVnJys1q1b6/XXX9fHH3+sFStWVO5iLEzxBgCUD0/QAgACwQwlAChedCQwyGIDQFgbNWqU4uPj9dRTT+ngwYNq0qSJ5s6dq8suuywk6zHEzCQAQGBiHdyAAgD4z0ECHACKFRUJjJjCCoy8fE4CABCu7rjjDt1xxx0hXQP1FwCA8qICAwAQCPP8wcO3AOAtKqbzOexUYAAA/EABBgAgQOYNKJfbYAYfAMBnZvcQKvgAwFt0JDAKTwJ5eVxAAABKxggMAEB5xTpOX2JRhQEA8NXpCgzOHQBwpqhIYMTYOQkAAHxHAQYAIFA2q9VTAZ6dkx/i1QAAqgqHnRaEAFCcqEhgmBcQlHADAEpHCQYAoPziCgd5Z9EGBADgo9jCc0cu3UMAwEt0JDBimIEBAPCdQQkGAKAc4mLtkqTsXCowAAC+iTVnYFCBAQBeoiOBUViGl8dJAABQCmZgAACCwazAyKYCAwDgI/PeVS73rgDAS1QkMGKowAAA+IUSDABA4OIcBRUYWczAAAD46HQLKRIYAHCmqEhgeCowSGAAAAAAqGDxVGAAAPxkzm/NYQYGAHiJigRGjN2swOACAgBQMjpIAQCC4fQMDK4/AAC+iY2hAgMAihMVCQyHnRZSAADfMcQbAFAenhkYtJACAPjIYSYw8t1yc0ECAB5RkcCIiTGHeJPAAACUghIMAEAQxDuowAAA+MeswJC4fwUAZ4qKBAYVGAAAf/C8EwCgPDwVGLlUYABAeS1YsECDBg3y2vbDDz9o4MCBatu2rbp3765ly5aVeZyNGzeqT58+atWqlfr166cPPvigopYckJiY07focmgjBQAeUZXAyHdRhgcAKJmFEgwAQBCYCYwsKjAAoFxWrFihOXPmeG07duyY7rjjDl1wwQVau3atxowZo9mzZ2vt2rUlHufTTz/Vn//8Z912221av369unTpotGjR2vnzp0VHYLPrBbL6QdwSWAAgEd0JDDOLMOjCgMAUBaS3QCAcvAM8WYGBgAE5ODBg7rzzjs1e/ZsNW7c2Ou1l156SQ6HQ1OnTlWTJk10ww03aMiQIVqyZEmJx1uyZIl69eqlgQMHqkmTJpo4caJatGihlStXVnQofjHvX1GBAQCnRUUCI8Z+OkwSGACAklB/AQAIhtMtpLgBBQCB+O6771SzZk1t2LBBbdq08Xpty5Yt6tixo+x2u2dbp06dtHv3bqWlpRU5ltvt1hdffKFOnTp5bb/sssu0ZcuWigkgQLExtEAHgLPZy96l6rPbrLJaLHIbRkEZXnxMqJcEAAhj1F8AAMojrnCIdxYzMAAgID179lTPnj2Lfe3AgQNKSUnx2lavXj1J0v79+1W7dm2v19LT05WZman69esXec9vv/1WrnXa7YE9F2yzWb1+N8U67JJylO9yB3zsylZSLFURsYQnYglflRVPVCQwpIJhSDm5LiowAAAlowQDABAE8bFUYABARcnOzpbD4fDaFhsbK0nKyckpdn9Jxb6nuP19ZbValJhYLeD3S5LTGe/154S4gtt0jjhHuY9d2c6OpSojlvBELOGrouOJmgSGw16QwKAMDwBQJkowAADlYFZgkMAAgOCLi4tTbm6u1zYzEZGQkFBkfzO5Udx74uMDv+nmdhtKT88M6L02m1VOZ7zS07Pkcp2+T2WzFjxRlXb0lI4dOxXw2ipTSbFURcQSnoglfJUnHqcz3ufKjahKYEhSbj4XEQCA4lGAAQAIBs8MDIZ4A0DQ1a9fX4cOHfLaZv45OTm5yP61atVSQkJCse85u62Uv/LL+ZCsy+X2OoY5wzUzO7/cx65sZ8dSlRFLeCKW8FXR8URGwy0fxNgLLiLy8iLnLwcAAACA8BMfW/CcWG6+Wy431x8AEEwdO3bU1q1b5XKdfkB106ZNaty4cZH5F5JksVjUvn17bd682Wv7Z599pksuuaTC1+uP2JiCe1c8fAsAp0VNAuN0BQYXEACA0tFBCgBQHmYFhiTl0EYKAILqhhtuUEZGhiZPnqwdO3Zo3bp1WrlypYYPH+7Z5+TJkzp69Kjnz3fccYdef/11LV++XDt37tTf/vY3/fDDDxo8eHAoQiiRo/Dh25w8zh0AYIqaBEZMTEGoeWSxAQAlsdBECgBQfnabVXZbwTklK4frDwAIptq1a2vp0qXavXu3+vfvr3nz5mnChAnq37+/Z59p06bpxhtv9Py5S5cuevzxx/XCCy+of//++vTTT7Vo0SI1adIkFCGUKLYwAZ5L9xAA8IiiGRhmGR4nAQBA6QyDGgwAQPnEOezKyMpTdi5zMACgPGbMmFFkW+vWrbVmzRq/3nPdddfpuuuuC+bSgs7sHkIFBgCcFj0VGHazAoMEBgCgeNRfAACCxTPImxZSAAAfeWZgkMAAAI+oSWA4SGAAAAAAqCRxjoJi9ywqMAAAPjJbSFGBAQCnRU0CI8bTQoqTAACgeIzAAAAES3xsYQUGMzAAAD4yH75lBgYAnBY1CQyHOcSbkwAAoAyMwAAAlFdCbEEFRmYOFRgAAN+YLaSowACA06ImgWHOwGCINwCgLOQvAADlFR9X2EKKBAYAwEcOZmAAQBFRk8Bw0EIKAFAGq9lDihIMAEA5xZsVGNkkMAAAvjldgcHDtwBgiqIEBkO8AQBlIH8BAAgSs4UUFRgAAF+Z7c+pwACA06ImgRETwyAkAEDpzAoM8hcAgPJiBgYAwF/MwACAoqImgWG2kMqjhRQAoAwGJRgAgHJiBgYAwF/MwACAoqImgcEQbwBAWSxmBQb5CwBAOSUwAwMA4KfYwu4hOdy7AgCPqElgMAMDAFCW0zO8yWAAAMqHGRgAAH+ZLaRyc11ckwBAoXIlMBYsWKBBgwZ5bXvwwQeVmprq9atr167lWmQwxBS2kMqlhRQAoASeBEZolwEAiADxzMAAAPjJbCFlSMp38QAuAEiSPdA3rlixQnPmzFHHjh29tm/btk0jRozQwIEDPdtsNlvgKwwSR2EZXh5DvAEAJbDIbCFFCgMAUD4JzMAAAPjJvHclSTl5bs/DuAAQzfxOYBw8eFCTJ0/W1q1b1bhxY6/XXC6XduzYoVGjRqlu3bpBW2QwOJiBAQAow+kWUqFdBwCg6juzAsMwDM+cJQAASmKzWmW3WZTvMgoGecfHhHpJABByfreQ+u6771SzZk1t2LBBbdq08Xptz549ysnJUZMmTYK2wGAxs9Z5tJACAJSAe0sAgGAxExiGIWXncg0CAPCNOQcjJ49zBwBIAVRg9OzZUz179iz2te3bt8tisWjlypX64IMPZLVa1a1bN40dO1Y1atQo92LLgwoMAEDZCjIYbkowAADl5LBbZbNa5HIbysrJ9yQ0AAAojSPGplPZ+SQwAKBQUP8V/dNPP8lqtercc8/VokWLtHfvXj3xxBPavn27Vq5cKas1sJnhdntg77PZrJ7f4wt70ObluwM+Xrg5M75IFOnxSZEfI/GhqrGaFRjkLwAA5WSxWBQfa1dGVp4yc/KVFOoFAQCqBHOQdy4zXAFAUpATGGPGjNGQIUPkdDolSSkpKapbt65uvvlmffvtt0VaTvnCarUoMbFaudbldMYrr/BmVG6+u9zHCzdOZ3yol1ChIj0+KfJjJD5UGYUJDC4VAADBkBBXkMBgkDcAwFexhQ/dUoEBAAWCmsCwWCye5IUpJSVFknTgwIGAEhhut6H09MyA1mOzWeV0xis9PUuZp3I8xzt85KTsEfDE9JnxuVyRd7st0uOTIj9G4gs9pzOeChE/WJniDQAIIs8g72wSGAAA3zgcZgUGCQwAkIKcwBg/fryOHz+uZcuWebZ9++23kqSmTZsGfNz8cs6tcLncXtPKs7Ijqwety+Uu93cUziI9PinyYyQ+VDXkLwCg6ti9e7euv/56PfTQQ7r++utDvRwvCYXXHFRgAAB8RQUGAHgL6mO5ffv21ccff6yFCxfq559/1vvvv69Jkyapb9++atKkSTA/ym8xZ8y9YJA3AKA4ZgWGwRAMAKgS8vLydP/99yszM7CK7YpmJjAySWAAAHzEDAwA8BbUMoQePXpo9uzZWrRokRYtWqQaNWqoX79+Gjt2bDA/JiAWi0Uxdqvy8t3KI4sNACgFFRgAUDXMnTtX1aqF73y7+DgqMAAA/oktbCFFBQYAFChXAmPGjBlFtvXu3Vu9e/cuz2ErjKMwgUEFBgCgOIzAAICq4/PPP9eaNWu0fv16de/ePdTLKVYCMzAAAH5y2JmBAQBnipxBED4w20jlkcAAABTDQgspAKgS0tPTNWHCBP3lL39RgwYNgnZcu93/Drs2m9Xr9zNVj4+RJGXnugI6drQq7TuF//g+g4/vFBUpNsaswODeFQBIUZbA8GSx88liAwCKKizAoAIDAMLc1KlT1bZtW/Xr1y9ox7RaLUpMDLwdldMZX2Rb3doFx8t1G+U6drQq7jtF4Pg+g4/vFBXBEVOQGKMCAwAKRFUCI8Y8CVCBAQAohqcCgwQGAISt9evXa8uWLfrXv/4V1OO63YbS0/0fBm6zWeV0xis9PUsul/d1hqXwhHLsRLaOHTsVlHVGg9K+U/iP7zP4+E69OZ3xVKME0ekKDBIYACBFWQLDYbaQogwPAFCM0zMwyGAAQLhau3at0tLSisy9mDJlipYtW6bXX3894GPnl+NBJ5fLXeT98YWDWDOycst17GhV3HeKwPF9Bh/fKSqCgwQGAHiJqgRGDC2kAAClMBMYAIDwNXPmTGVnZ3ttu+qqq3TPPfeoT58+IVpV8RLiCi63TmUxxBsA4JtYTwspkmMAIEVZAsPBEG8AQCkshVMw3FRgAEDYSk5OLnZ77dq1de6551byakpXPa5giPep7LwQrwQAUFVQgQEA3qKqSWGMnRkYAICSeSowyF8AAIKgWnxBAiM716V8+uQDAHxgzsCgewgAFIiuCozCk0AeWWwAQHEsZgVGiNcBAPDLtm3bQr2EYiXEnr7cyszOl7OaI4SrAQBUBZ4h3rkkvgFAogIDAACP0yMwyGAAAMrParV4khi0kQIA+MJhzsCgAgMAJEVZAsNBAgMAUAqzhRQjMAAAwVItnkHeAADfxTIDAwC8RFkCo7CFFFlsAEAxLIUZDIMMBgAgSKoVDvLOoAIDAOADs/15bh4P3wKAFGUJDFpIAQBK46nACO0yAAARxBzkfSqLBAYAoGyeId5UYACApChLYJh9BPPIYgMAimHOwKAAAwAQLNXiClpIZWbTQgoAUDbz3pXLbSjfxf0rAIiqBEZMYQspBiEBAIpDCykAQLB5KjBoIQUA8IFZgSFRhQEAkmQP9QIqkznEO48WUgCAYtBCCgAQbGYFBkO8ASB4PvvsM91+++3FvtawYUO98847Rbb/85//1AMPPFBk+3/+8x81atQo6GsMlM1qkdVikdswlJPnVkJcqFcEAKEVVQmMGBIYAIBSnK7ACPFCAAARwxziTQUGAARPu3bt9NFHH3lt2759u4YNG6YRI0YU+55t27bp0ksv1axZs7y2JyUlVdg6A2GxWBTrsCorx0UFBgAo6lpIMcQbAMLZ7t271a5dO61bty4kn396BgYZDABAcJgJjAwSGAAQNA6HQ3Xr1vX8qlWrlqZPn66rrrpKN910U7Hv2b59uy6++GKv99WtW1c2m63Y/UPJUdgCPYcEBgBEVwLDPAHkMQMDAMJOXl6e7r//fmVmZoZsDWYFBgAAwVItnhZSAFDRnn/+ef3222968MEHS9xn27Ztatq0aSWuKnDmHIzcPB7ABYCoSmDExFCBAQDhau7cuapWrVpI12DmL9xUYAAAgsTTQiqLCgwAqAg5OTlatGiRBg8erHr16hW7z9GjR3XkyBF9/vnn6tu3r7p06aLRo0dr9+7dlbxa3zhiqMAAAFNUzcDwDPEmgw0AYeXzzz/XmjVrtH79enXv3j3Uy2GKNwAgaGokFCQwTpLAAIAK8eqrryonJ0eDBg0qcZ/t27dLkmw2m5544gllZmZqwYIFuu222/Svf/1LderUCeiz7fbAngu22axev58t1lGQwMh3uwP+jMpSVixVCbGEJ2IJX5UVT5QlMApL8GghBQBhIz09XRMmTNBf/vIXNWjQIKRr8QzxDukqAACRpHp8QQIjKydf+S637BFywQoA4WL9+vW66qqrlJiYWOI+nTp10ubNm1WzZk3Ptvnz56tHjx5at26dhg0b5vfnWq0WJSaWr4Lc6Ywvdnv1wuS33RFT7s+oLCXFUhURS3gilvBV0fFEVQLDHOKdRwspAAgbU6dOVdu2bdWvX7+gHjeQJ5VstsIEhhH401RVUaQ9BeKLaIxZis64ozFmKXrjDkfV4mNksRScWzKy8lSremyolwQAEePo0aP68ssvNXz48DL3PTN5IUkJCQlq2LChDh48GNBnu92G0tMDm99ns1nldMYrPT1LLlfRe1Tm2fvY8UwdO3YqoM+oLGXFUpUQS3gilvBVnnicznifr1WiKoFhtpBiBgYAhIf169dry5Yt+te//hXU4wb6NFT1w6fK9f6qLtKeAvFFNMYsRWfc0RizFL1xhxOrxaLq8TE6mZmnjEwSGAAQTF988YUsFosuvfTSUvdbvXq1Zs+erffff19xcXGSpIyMDO3Zs0c33nhjwJ+fX877Sy6Xu9hjmA/gZmXnl/szKktJsVRFxBKeiCV8VXQ8UZXAiCkcgpSX75ZhGJ5WIQCA0Fi7dq3S0tKKzL2YMmWKli1bptdffz2g4wb6NFRmZq4kKd/lDvsnnYIp0p4C8UU0xixFZ9zRGLNUeU9DwTdmAuNk4XkGABAcP/74o8477zzFx3sn7F0ul44ePaoaNWooLi5OPXr00NNPP60JEyZozJgxys7O1qxZs5SUlKT+/fuHaPUlY4g3AJwWVQkMxxntQPLy3Z4TAgAgNGbOnKns7GyvbVdddZXuuece9enTp1zHDiT7bxiG5/dIehrCV5H2FIgvojFmKTrjjsaYpeiNO9zUSHDot7RMBnkDQJAdOXJEtWrVKrL9t99+05VXXqnp06fr+uuvV4MGDbRy5UrNnDlTt956qwzD0O9+9zv94x//8FRkhJPYGGa4AoApqhIYMWckMHJJYABAyCUnJxe7vXbt2jr33HMreTWSWZdnMMUbABBENQoHeZ/MJIEBAME0derUYrc3bNhQ27Zt89rWrFkzLVu2rBJWVX6OmIL7Vzm5PIQAAFFVG263WWUtbBvFIG8AwNnM1oIGGQwAQBDVSDATGLSQAgCUjQoMADgtqiowJCkmxqqcXBcnAQAIU2c/KVWZzNFI5C8AAMFUvTCBkUELKQCAD2KZgQEAHlFVgSGdnoORl0cFBgDgbIUVGCFeBQAgstSId0iihRQAwDdmy/Nc7l0BQPQmMHJpIQUAOIuVIRgAgApABQYAwB+x5gwMKjAAIPoSGHGOgq5Z2bn5IV4JACDs0EIKAFABmIEBAPCHw25WYJDAAIDoS2DEFpwEsnJIYAAAvFkttJACAAQfLaQAAP6IdTADAwBMUZfAiI8tqMDIyuEkAAAonkEJBgAgiGqc0UKKcwwAoCxm+/McZmAAQPQlMBI8CQwqMAAA3ixmBQb3lgAAQVQ9viCB4XIbXIcAAMpkVmDQQgoAojCBEU8CAwBQAotnBgYZDABA8DhibIqNKbgZlU4bKQBAGZiBAQCnRV8Co3CIdyYJDADAWTwJjNAuAwAQgWpWK5iDkX6KQd4AgNKZSW9aSAFANCYwCod4Z+eSwAAAeDOHeLupwAAABFnN6gUJjBMkMAAAZXDEFNyuy3e55XZzbQIgukVhAsOswKAMDwDgzZPA4CIBABBkZgXGiYycEK8EABDuzAoMScqhjRSAKBe1CQxmYAAAzmaxmhUYIV4IACDi1KwWK4kKDABA2WLsVhV2t2UOBoCoF3UJjAQSGACAEljNGRhkMAAAQeakhRQAwEcWi0UOcw5GPnMwAES3qEtgxJHAAACUgBkYAICKwhBvAIA/YgvnYOTmUoEBILpFXQKDCgwAQElIYAAAKsrpGRgkMAAAZTtdgUECA0B0i7oERnxswQkgiyHeAICzmDMwDKq0AQBBVrOwhdTxUwzxBgCUzRzkTQUGgGgXhQmMggqMnDyXXG7uUAEATjNnYFCBAQAINnOI98lTeXIzawkAUAZHYQupnDzuXQGIblGbwJCowgAAeLMWZjC4sQQACLYaCTGyqCBJnpGVF+rlAADCnKcCgxZSAKJc1CUw7DarYuwFYWczBwMAcAZzBoYhyaAKAwAQRHabVdUTYiRJJxjkDQAog2cGBi2kAES5qEtgSKerMDJJYAAAzlCYv5Akkb8AAASbZ5A3czAAAGVweCowaCEFILqVK4GxYMECDRo0yGvbDz/8oIEDB6pt27bq3r27li1bVq4FVgQzgZFFAgMAcAazhZTEHAwAQPB5EhgZVGAAAEoX65mBQQUGgOgWcAJjxYoVmjNnjte2Y8eO6Y477tAFF1ygtWvXasyYMZo9e7bWrl1b7oUGU7yjIIvNDAwAwJmsZ5RgMAcDABBszsJB3um0kAIAlMFTgUECA0CUs5e9i7eDBw9q8uTJ2rp1qxo3buz12ksvvSSHw6GpU6fKbrerSZMm2rt3r5YsWaIbbrghaIsuL08FRi4VGACA06jAAABUpJrVzRZSJDAAAKUzh3hTgQEg2vldgfHdd9+pZs2a2rBhg9q0aeP12pYtW9SxY0fZ7afzIp06ddLu3buVlpZW/tUGSQItpAAAxTizAoP8BQAg2E7PwCCBAQAoncNecMsuN48ZGACim98VGD179lTPnj2Lfe3AgQNKSUnx2lavXj1J0v79+1W7du0AlijZ7YF1urLZrF6/mxLiC8LOyXMFfOxwUFJ8kSLS45MiP0biQ1Vz5hBvKjAAAMF2egYGQ7wBAKWLdVCBAQBSAAmM0mRnZ8vhcHhti40t6POakxPYP9KtVosSE6uVa11OZ7zXnxNrFvzZrfIfOxycHV+kifT4pMiPkfhQVXi1kGIGBgAgyGpWL7g2OsYQbwBAGRx2EhgAIAU5gREXF6fcXO9/jJuJi4SEhICO6XYbSk/PDOi9NptVTme80tOz5HKdLrmzFj5Veyw9W8eOnQro2OGgpPgiRaTHJ0V+jMQXek5nPBUifvAa4k3+AgAQZEk1ChMYJ7NlGIYsZ5b+AQBwhljPEO/wvNYEgMoS1ARG/fr1dejQIa9t5p+Tk5MDPm5+fvl+WLtcbq9jmCeBzKy8ch87HJwdX6SJ9PikyI+R+FCVWK0Wud0GFRgAgKBLLExg5Oa5dSo7X9XjY0K8IgBAuHLEFDyIRgUGgGgX1MdyO3bsqK1bt8rlOv3DddOmTWrcuHHA8y8qQnzhEO9MhngDAM5idpEymIEBAAgyR4zNk7Q4mp4d4tUAAMLZ6QoMEhgAoltQExg33HCDMjIyNHnyZO3YsUPr1q3TypUrNXz48GB+TLklFCYwskhgAADOYraRYog3AKAiJDnNNlIM8gYAlMxMYFCBASDaBTWBUbt2bS1dulS7d+9W//79NW/ePE2YMEH9+/cP5seUW1xswUkgK4eTAADAmznImw5SAICKkFQjTpJ0lAQGAKAUjsIERnYu964ARLdyzcCYMWNGkW2tW7fWmjVrynPYChdPBQYAoARmAsMggwEAqACJhRUYtJACAJSmds2ChPfxkznKyXN5KjIAINoEtQKjqqCFFACgJLSQAgBUpKQaZgKDCgwAQMlqVnOoRkKMDEn7j5wK9XIAIGSiMoHhqcDIzWdIKwDAi8VCCykAQMVJchY8UXvsJBUYAIDSnVunmiRp32ESGACiV3QmMBwFCQzDoJcgAMCbjRZSAIAKRAUGAMBX59atLknadyQjxCsBgNCJygSGI8bqaRFCAgMAcCZr4ZmRFlIAgIpgVmAcPZlDNTgAoFTn1qUCAwCiMoFhsVgUH1sw/CiTORgAgDMwAwMAUJESa8TKIinf5dbJrLxQLwcAEMYaeiowSGAAiF5RmcCQzpiDQQIDAHAGi9lCivwFAKAC2G1WOas5JEnHaCMFAAHZt2+fUlNTi/x6+eWXi93/2LFjGj9+vDp27KiOHTvqoYceUmZmZiWv2n/mDIxjJ3N0KpukN4DoZA/1AkIlgQQGAKAYngoMZmAAACpIkjNWJ07l6mh6thrVrxHq5QBAlbNt2zbFxsbq7bfflqXw3++SVKNG8T9T77nnHuXk5GjFihVKT0/X5MmT9cgjj+iJJ56orCUHJD7WrtrOWKWl52jf4VNKOa9WqJcEAJUuaisw4khgAACKYbXSQgoAULESa5yegwEA8N/27dvVuHFj1atXT3Xr1vX8iouLK7Lvl19+qc2bN2v69Olq0aKFOnfurEcffVSvvvqqDh48GILV+8czyPswg7wBRKeoTWBQgQEAKA4VGACAipbkjJUkpaVnh3glAFA1bdu2TU2bNvVp3y1btqhu3bpq0qSJZ9ull14qi8WirVu3VtQSg8ZsI/UrczAARKmoTWCYQ7yzclwhXgkAIJxYmYEBAKhgdWvFS5IOH88K8UoAoGravn270tLSdNttt+nyyy/Xrbfeqg8//LDYfQ8ePKgGDRp4bXM4HKpVq5Z+++23ylhuuXgGeR+iAgNAdIraGRjmEO9MKjAAAGcozF/QQgoAUGFIYABA4HJzc7Vnzx7Fx8drwoQJSkhI0IYNG3TXXXdp+fLl6ty5s9f+WVlZcjgcRY4TGxurnJzytfKz2wN7Lthms3r9XprzC2cl7TtySjabxWvmRzjwJ5ZwRyzhiVjCV2XFE/UJjGwSGACAMzADAwBQ0c5MYBiGEXY3owAgnDkcDn3++eey2+2exETLli21c+dOLVu2rEgCIy4uTrm5uUWOk5OTo4SEhIDXYbValJhYLeD3S5LTGV/mPtWqx8lqkU5l58uw2ZRUs+z3hIIvsVQVxBKeiCV8VXQ8UZ/AYAYGAOBMngSGO8QLAQBErDo1C4bMZuW4dCo7X9XjY0K8IgCoWopLPKSkpOijjz4qsr1+/fp6++23vbbl5ubq+PHjSk5ODngNbreh9PTMgN5rs1nldMYrPT1LLlfZFx7JSQn6LS1T3/10WK2a1A7oMyuKv7GEM2IJT8QSvsoTj9MZ73PlRtQnMGghBQA4k2eINxUYAIAKEhtjU83qDp3IyNXh41kkMADADz/++KNuvfVWLVmyRB06dPBs/9///lfsYO+OHTtq5syZ2rt3rxo1aiRJ+uyzzyRJ7du3L9da8vPLdwPS5XL7dIxz6lTTb2mZ+vngSTVrlFiuz6wovsZSFRBLeCKW8FXR8URGw60AnB7iTQIDAHDa6SHeJDAAABWnHnMwACAgKSkpuuiii/TII49oy5Yt2rlzp6ZPn66vvvpKI0aMkMvl0uHDh5WdnS1JatOmjdq3b6/77rtP33zzjT799FNNmTJF1113XbkqMCqTOcj718MM8gYQfaI3geEobCGV6wrxSgAA4cRTgRE5D0MAAMIQg7wBIDBWq1WLFi1Sq1atNHbsWPXv319ff/21li9frtTUVP3222/q0qWL3njjDUmSxWLRvHnz1LBhQw0ePFhjx45V165dNXXq1NAG4odz6xTM2th3+FSIVwIAlS/qW0hRgQEAOBMVGACAykACAwACl5SUpMcff7zY1xo2bKht27Z5batdu7bmzJlTGUurEOfWLUhg7D9ySm7D8Dx0BQDRIGorMBJIYAAAisEMDABAZahbq2CQ96FjJDAAAKWrlxgvu82q3Hy3jpD4BhBlojaBQQUGAKA41sIzo9tNAgMAUHHq1UqQJB0+nh3ilQAAwp3NatU5dQrOG7SRAhBtoj6Bke8ylJvHHAwAQAGLxWwhFeKFAAAimlmBcfRktvJdDF4CAJTu3DoM8gYQnaI4gWGTrbDP+cnMvBCvBgAQLswZGLSQAgBUJGc1hxx2qwxDSjtBFQYAoHQNC+dg7DtCBQaA6BK1CQyLxaKa1R2SpBOnckO8GgBAuPDMwKCFFACErePHj+vhhx9W165d1b59e916663asmVLqJflF4vFonqJBYO8Dx7LDPFqAADhzhzkTQspANEmahMYkuRMKEhgpJPAAAAUslGBAQBhb9y4cfr66681a9YsvfLKK2rRooWGDh2qnTt3hnppfqlfu+Bm1P4jJDAAAKUzW0gdOJpJ60EAUSWqExg1q5kVGDkhXgkAIFyYLaTIXwBAeNq7d68+/vhjTZkyRR06dNCFF16oyZMnKzk5Wa+99lqol+eXc2oXDGQ9cJSnaQEApUtyxio+1iaX29CBoyS+AUSPqE5gOKtRgQEA8FbYQYoKDAAIU4mJiXrmmWfUsmVLzzaLxSLDMHTixIkQrsx/9QsTGPvTuBEFACidxWJhkDeAqBTVCQxmYAAAzsYMDAAIb06nU926dZPD4fBs27hxo37++Wd16dIlhCvz3zmFLaQOkMAAAPiAORgAopE91AsIJWZgAADOZvXMwAjxQgAAPtm6dasmTZqkK6+8Uj179izXsex2/5/vstmsXr/749x61WWRlJGVp8ycfE+FeLQrz3eKovg+g4/vFKFybh0SGACiT1QnMGpWj5VEBQYA4DRPAoMMBgCEvbffflv333+/2rRpo1mzZpXrWFarRYmJ1QJ+v9MZH9D76iYl6NDRTGXkutWoYeCfH4kC/U5RPL7P4OM7RWU7t25BC6l9R2ghBSB6RHUCw5kQI4kKDADAaWYLKUMkMAAgnK1atUrTpk1Tr169NHPmTK+WUoFwuw2lp/vfyslms8rpjFd6epZcLrff76+fGK9DRzO1bXeazkmM8/v9kai83ym88X0GH9+pN6cznmqUSmK2kDp8PFvZufmKc0T1bT0AUSKqf9JRgQEAOBszMAAg/K1evVqPPfaYBg0apEmTJslqDc6Ns/z8wG9EulzugN5fPylB3+xM077DGeX6/EgU6HeK4vF9Bh/fKSqbM8EhZzWH0k/lav+RTF14jjPUSwKAChfVKXJzBkZ2rks5ea4QrwYAEA6YgQEA4W337t16/PHH1atXLw0fPlxpaWk6fPiwDh8+rJMnT4Z6eX5rUDtBkvQbg7wBAD44PQeDNlIAokNUV2DEx9oUY7cqL9+t9FO5qluL/pUAEArHjx/XrFmz9N577ykjI0OpqakaP368OnToUOlrMRMYBhkMAAhLb775pvLy8vTWW2/prbfe8nqtf//+mjFjRohWFpgGtQtuRO0/wkBWAEDZzq1bTT/sPaZ9nDcARImoTmBYLBY5ExxKS8/WCRIYABAy48aNU1pammbNmqWkpCStXr1aQ4cO1bp169SkSZNKXUth/kJugwQGAISjESNGaMSIEaFeRtCY/czT0rOVmZ2vhLiovkQDAJShoTnImwoMAFEiqltISVLN6gVtpBjkDQChsXfvXn388ceaMmWKOnTooAsvvFCTJ09WcnKyXnvttUpfj2cGBvkLAEAlqBYXo9rOgtl8v3IzCgBQBjPx/ethKjAARAcSGNUKEhgM8gaA0EhMTNQzzzyjli1berZZLBYZhqETJ05U+no8LaSowAAAVJLz6tWQJP18sOrN8AAAVK5zClsPnjiVq5OZ3MsCEPmiPoHhrEYFBgCEktPpVLdu3eRwODzbNm7cqJ9//lldunSp9PV4hnhTggEAqCTn1StoB/LzISowAACli4+1q07NOEnMTwIQHaK+wSoVGAAQXrZu3apJkybpyiuvVM+ePQM+jt3uf47eZrN6WkjJEtgxqiKbzer1ezSIxpil6Iw7GmOWojfuqur85IIExi8HSWAAAMp2bp1qOnIiW78ePqXU8xNDvRwAqFBRn8CgAgMAwsfbb7+t+++/X23atNGsWbMCPo7ValFiYrWA3ytJDkdMwMeoqpzO+FAvodJFY8xSdMYdjTFL0Rt3VXNeckELqX1HTinf5ZadxBMAoBQN61XX1zvTtI8KDABRIOoTGKcrMHJCvBIAiG6rVq3StGnT1KtXL82cOdOrpZS/3G5D6emZfr/PZrPKLMDIysrVsWPRcUFgs1nldMYrPT1LLpc71MupFNEYsxSdcUdjzFL54nY646ncqGR1asYpzmFTdq5LB45mqmHd6qFeEgAgjJ1bxxzkTeUegMgX9QkMKjAAIPRWr16txx57TIMGDdKkSZNktZb/xll+fmA3Ks0KjHyXO+BjVFUuYo4a0Rh3NMYsRW/cVY3VYtF59arrp19P6JeDGSQwAAClOrfwPLHv8CkZhiGL+RQWAESgqH+0ihkYABBau3fv1uOPP65evXpp+PDhSktL0+HDh3X48GGdPHmy0tdjK/zHPzO8AQCV6fQg78o/9wEAqpb6SQmyWizKysnXsZN0FAEQ2ajAKExg5Oa5lZ2brzhH1H8lAFCp3nzzTeXl5emtt97SW2+95fVa//79NWPGjEpdj1mB4SaDAQCoRI3qF8zB2L0/PcQrAQCEuxi7VfVrJ2j/kVPad+SUkpxxoV4SAFSYqL9bH+ewKzbGppw8l06cyiWBAQCVbMSIERoxYkSol+Fhll8bBgkMAEDlaXJOTUnSngMn5XK7ZQtCO0UAQOQ6t061ggTG4VNqdWHtUC8HACoM/yqW5KwWI4k5GACAMyowSGAAACpR/doJio+1KzffrV8PnQr1cgAAYe7cugzyBhAdSGBIqlktVpJ0IoMEBgBEO6s5A4OZtwCASmS1WHRhg4I2Urv2nwjxagAA4e7cOqcHeQNAJCOBodNzMBjkDQAwKzBoIQUAqGwXFraR2sUcDABAGRoWVmDsTzvF/D4AES3oCYx9+/YpNTW1yK+XX3452B8VNDULExi0kAIAmC3HaSEFAKhsTc51SpJ2ksAAAJShbq14OexW5eW7dfh4VqiXAwAVJugTq7dt26bY2Fi9/fbbnkGoklSjRo1gf1TQUIEBADB5WkiRvwAAVLLGDQoSGAeOZupUdp6qxcWEeEUAgHBltVrUoE417T1wUr8ezlByUkKolwQAFSLoFRjbt29X48aNVa9ePdWtW9fzKy4uLtgfFTRUYAAATJ4WUmQwAACVrEaCQ/US4yVJO/dRhQEAKF3DOgVtpJiDASCSBT2BsW3bNjVt2jTYh61QNanAAAAUOl2BQQIDAFD5UhrWkiRt+/lYaBcCAAh759YtGOT96xESGAAiV4VUYKSlpem2227T5ZdfrltvvVUffvhhsD8mqGpWj5UkHTuZHeKVAABCzazAYBAeACAULm5US5L0IwkMAEAZzq1rVmBkhHglAFBxgjoDIzc3V3v27FF8fLwmTJighIQEbdiwQXfddZeWL1+uzp07B7ZIe2B5FpvN6vV7Sc4p/IF/PCNXLrehWIctoM+rbL7GV1VFenxS5MdIfKiKzAoM0hcAgFC4+PxESdKeAyeVmZ2vhLigjy0EAESIhoUVGAePZikv362YAO+fAUA4C+q/hh0Ohz7//HPZ7XY5HAVtmVq2bKmdO3dq2bJlASUwrFaLEhOrlWtdTmd8qa8nJko1EmJ0MjNPWS5D9cv5eZWtrPiqukiPT4r8GIkPVYm18N/8VGAAAEIhyRmneonxOnQsS9t/Pa62TeuEekkAgDBVq7pDCbF2Zebk67e0Uzo/uUaolwQAQRf0x3kSEhKKbEtJSdFHH30U0PHcbkPp6ZkBvddms8rpjFd6epZcLnep+9atFa+TmXn6aU+aasVXjaec/ImvKor0+KTIj5H4Qs/pjKdCxE/MwAAAhNrF5yfq0LEs/bj3GAkMAECJLBaLzq1bTT/9ekL7jpDAABCZgnqn/scff9Stt96qJUuWqEOHDp7t//vf/8o12Ds/v3w3Bl0ud5nHqJcYr13707X/yKlyf15l8yW+qizS45MiP0biQ1XCDAwAQKhd3KiWPvh6P3MwAABlOrdu9YIExmEGeQOITEF9LDclJUUXXXSRHnnkEW3ZskU7d+7U9OnT9dVXX2nEiBHB/KigS04sqBw5dCwrxCsBAISSJ4FB/gIAECLNCudg/HIwQ+mnckO8GgAIP8ePH9fDDz+srl27qn379rr11lu1ZcuWEvf/5z//qdTU1CK/9u7dW4mrrhgNGeQNIMIFtQLDarVq0aJFmjlzpsaOHav09HQ1b95cy5cvV2pqajA/KuiSEwt62B8kgQEAUc1iDvGmhRQAIERqVo/V+cnV9fPBDH27K02/a9Ug1EsCgLAybtw4paWladasWUpKStLq1as1dOhQrVu3Tk2aNCmy/7Zt23TppZdq1qxZXtuTkpIqa8kV5tw6hQmMI1RgAIhMQR/2kJSUpMcffzzYh61w9QorMA4eC2zeBgAgMjADAwAQDlo3qaOfD2bo650kMADgTHv37tXHH3+sF154Qe3bt5ckTZ48WR988IFee+013XvvvUXes337dl188cWqW7duZS+3wp1bt7ok6ciJbB07maPEGrEhXhEABBeTXQslJxVUYJzIyFV2bn6IVwMACBWbZwZGiBcCAIhqbZrWliR9tztN+S5OSgBgSkxM1DPPPKOWLVt6tlksFhmGoRMnThT7nm3btpVrNms4qx4fowvqFwzvfvb173kQC0DEIYFRqFpcjKrHx0hiDgYARLPTMzD4hz8AIHQaN3CqRkKMsnJc2vFr8TfkACAaOZ1OdevWTQ6Hw7Nt48aN+vnnn9WlS5ci+x89elRHjhzR559/rr59+6pLly4aPXq0du/eXZnLrlBD+zaXw27Vd3uOaeOnVX+uBwCcKegtpKqy5MR4ZWTl6dCxLJ2fXCPUywEAhICVGRgAgDBgtVjU6sLa+uR/B/T1ziO6uFFiqJcEAGFp69atmjRpkq688kr17NmzyOvbt2+XJNlsNj3xxBPKzMzUggULdNttt+lf//qX6tSpE/Bn2+2BPRdss1m9fi+vRvVraFDvVC17/Qf984Pdat44SRc1rBWUY5cl2LGEErGEJ2IJX5UVDwmMM9RLTNDO/enMwQCAKGa2kHK5SGAAAEKrbdM6+uR/B7R122H9sUdTWQqT7ACAAm+//bbuv/9+tWnTpsiAblOnTp20efNm1axZ07Nt/vz56tGjh9atW6dhw4YF9NlWq0WJidUCeq/J6Ywv1/vP9H89LtJP+9L1wVf7tOjV7zRnXHdVT3CU/cYgCWYsoUYs4YlYwldFx0MC4wzJiQVf9sGjtJACgGhlPkWVR79xAECItWpSW7ExNh05ka3dv53Uhec4Q70kAAgbq1at0rRp09SrVy/NnDnTq6XU2c5MXkhSQkKCGjZsqIMHDwb8+W63ofT0wB6AtdmscjrjlZ6eJVcQrzsG9LpIP+45qkPHsvTk81s05obWFZ78rqhYQoFYwhOxhK/yxON0xvtcuUEC4wz1Cgd5U4EBANHLEVNwAmVgKgAg1GJjbGrTtLY2/3BIm384SAIDAAqtXr1ajz32mAYNGqRJkybJai35Jtjq1as1e/Zsvf/++4qLi5MkZWRkaM+ePbrxxhvLtY78/PJdM7hc7nIf40wxNquG/18LPf7cVm358bDe+vwX9WzfMGjHL02wYwklYglPxBK+KjqeyGi4FSTJiQmSGOINANEsxm6TVP6LEQAAguHSZsmSpM9/PCQ385kAQLt379bjjz+uXr16afjw4UpLS9Phw4d1+PBhnTx5Ui6XS4cPH1Z2drYkqUePHjIMQxMmTNBPP/2kb7/9VmPGjFFSUpL69+8f4miCr3EDp27q3kSS9OI7O/TzwZMhXhEAlA8JjDOYLaROnMpVVk5+iFcDAAiFGE8LKW4SAQBCr9WFSYqPtenYyRzt+PVEqJcDACH35ptvKi8vT2+99Za6dOni9WvatGn67bff1KVLF73xxhuSpAYNGmjlypU6deqUbr31Vg0ZMkQ1atTQP/7xD09FRqTp1fE8tWlSW/kutxa9+p2yc7nHBaDqooXUGRLiYlQ9PkYZWXk6dCxLjerXCPWSAACVLMZGCykAQPiIsdvU/qK6+vh/B/TJ/35Tynm1Qr0kAAipESNGaMSIEaXus23bNq8/N2vWTMuWLavIZYUVi8WiP13TTFOXf64DRzP1/FvbNfSa5qFeFgAEhAqMsyQzBwMAoppZgZGf75ZBqw4AQBi4os05kqTPvj9EpTgAwCc1Ehwa1q+5LBbp428PaNP/DoR6SQAQEBIYZ2EOBgBENzOBYUhyuUlgAABC76KGNZWclKCcPJc+//FQqJcDAKgiUs9P1LW/ayxJ+seb23TgKA/rAqh6SGCcxZyDcZAf6gAQlez206fGPAZ5AwDCgMViUdfWDSRJH369P8SrAQBUJf0uv0Cp59VSTp5Li179H9c4AKocEhhnObdudUnSnoMnQ7wSAEAoxNhtnv9mDgYAIFxc3qqBbFaLdu5P154D6aFeDgCgirBaLRp2bQtVj4/Rzwcz9PJ/d4R6SQDgFxIYZ2lyjlOStP/wKfrLAkAUslktslktkqR8Fy2kAADhoWY1hzo2qydJ+s/nv4R4NQCAqiSxRqyGXtNMkvT21l+18bO9zPsDUGWQwDhLzeqxqlMzToakXb/xZBP+v707j4+qvvc//p4lk32TBKJssoU1QNAgShFEsNaFKlfqQ68r1mql2tJWkWsVrGtV0F4QvVpQr0tLXcoPvcq9gGJtq5AAgsgSwyaEJQtLQvbMfH9/DBkSCEsymczMmdfz8QjJnDkz8/mcczifc85nzjkAIpHT4S2PdZyBAQAIIZfldJUk5W4q0oGy6iBHAwAIJ0N6p+mqi7pLkt79bKte/2QzZ5wDCAs0MJrRq3OyJGlr4eEgRwIACIaG+2DUc31YAEAIOTcjSX27psjtMVq+enewwwEAhJlrR/XUjeP6yGaTvli/V7MXfq0jVXXBDgsATokGRjMaLiO1bQ9nYABAJIpyNFxCigYGACC0/HB4N0nSp2sLVV5ZG+RoAADhxGazadz5XfXL64YoxuXQ5u8P6Yn/ztO+A5XBDg0ATooGRjMan4HBNQEBIPJwCSkAQKga0ruDunVKUE2tW0tWfR/scAAAYWhwrw76j5vPU4ekGO0/WKXH38jTph0Hgh0WADSLBkYzunZMUJTTrorqeu0/WBXscAAA7SyKS0gBAEKUzWbTNaN6SpKWr96tsgrOwgAAtFyX9AQ9fOv56tU5SZU19Zr913X6/OvCYIcFACeggdEMp8Ou7hmJkrgPBgBEIs7AAACEsiG9OqjH2UmqrfPo//1je7DDAQCEqaR4lx64IVsjBnSS22P0xpIt+svy7+TxcDUSAKGDBsZJ9D6HG3kDQKQ6dgYGG+4AgNBjs9n0k0t6SZJWfF2o7/eXBzkiAEC4inI6dOfVA3TNqB6SpP/L3aU5769XVU19kCMDAC8aGCfR8+iNvLdyI28AiDhRR8/A4CbeAIBQ1bdbqnL6dZQx0jvLvuPefQCAVrPZbJowsofu/vFARTntWre1VE+9tUYHyqqDHRoA0MA4mYYbee8uPkLXGQAijNPJJaQAAKHvJ5f0lstpV/6uQ1q5aX+wwwEAhLnh/TvpgRuzlRTv0u7iI5rz/jeqq3cHOywAEY4GxkmkJkbrrKRoGSPt2Mcp2QAQSRrugcFNvAEAoaxDcoyuuLC7JOmdpd/p8JGaIEcEAAh3vc5J1u9uPk8JsVHaub9c7yz7LtghAYhwNDBOodfR+2Bs+f5gkCMBALSnKKdNEmdgAABC3xUjuqtbxwQdqarTG0u2cCkpAIDf0lJi9bMJA2ST9PnXe/TPb/YGOyQAEYwGxilk9ewgSVqdXxzkSAAA7cnldEiSautoYAAAQpvTYddPrxogh92mrwtK9I/1HGQCAPhvUI8OmvAD74293/zfLdpddCTIEQGIVDQwTiE7M00Ou02FxRUqLKkIdjgAgHYSE+2UJFXXcg8kAEDo69IxQdde3FOS9PbSfH2/n0vgAgD8d/VF52pgj7NUW+/Ri3/7hnvEAggKGhinEB8TpYE9zpIk5W0uCnI0AID2EuvynoFRXcsN6wAA4eHy4d2U1bODaus9mvvBNzpSVRfskAAAYc5ut+lnVw9QamK09h+s0mufbOZShQDaHQ2M08jp11GSlEsDAwAiRoyvgcE3jAAA4cFut+nOqwcoLTlGJYer9dKiDarnXk4AAD8lxrl0zzWD5LDblLe5SMvydgc7JAARhgbGaWT3SZPTYdOekgoVFnO9PwCIBLG+S0hxBgYAIHwkxEbpFxOzFB3l0KadB/WnjzbKwzdlAQB+6tU5WdeP7S1J+utnBSooPBzkiABEEhoYpxEXE6VBPbw38+YsDACIDDEubwOjqoYGBgAgvHTrlKgpE73flF21qUjvLM3nch8AAL9del4X5fTrKLfH6KVFG1RWWRvskABECBoYZ6DxZaTY+AcA64uJ5hJSAIDwNahHB91xZX9J0qdrCvXm/27hTAwAgF9sNptu+1E/ZZwVp4PlNXp18bfyeKgtAAKPBsYZGNonTU6HXXtLK/X9fi4jBQBW13AGBpeQAgCEqxEDM3T7j/rJJmnF13u04H82cU8MAIBfYqOdmnLtILmi7Pp2x0Et/uf2YIcEIALQwDgDsdFOZfdJkyR9/NXOIEcDAAi0WM7AAABYwKgh5+jOqwfIbrPpXxv2adZfvlY5l/wAAPihc3qCbr28nyTpw3/u0PqtJUGOCIDV0cA4Q1dddK4kKW9zkXZzM28AsLRYzsAAAFjEiIEZuu+6LMW4HNqy65Ae/+887SpifwYA0HoXDszQmOzOMpJeWvStig5WBjskABZGA+MMde2YoPP7dZSRtPgfnCIHAFYW4/KegcFNvAEAVjC4V5oeuvk8pSXHqPhQtR57I0/LV+/m/n4AgFa74dI+OjcjURVVdfr9n77SwfKaYIcEwKJoYLTAj0eeK5ukvC3F+n5/ebDDAQAESEy09wyMereH64UDACyhc3qCHr71fA3u1UH1bo/eXpqvP763XqWHq4MdGgAgDEU57brnmkFKTnBp575yPfZ6rvaWVgQ7LAAWRAOjBTqnJyinf0dJ0uJ/7ghuMACAgImLdsp29O+Kau6DAQCwhsQ4l3553WDdOK6PnA671m8t1UN/+kqfrNxJwx4A0GJpKbF65NYcnZMWr5LD1XrqrTXatqcs2GEBsBgaGC00YWQP2SStyS/W199xoyIAsCK73ab42ChJUnkFNzsFAFiHzWbTuPO7asZt5yuzS7Jq6zx697OtmvlartbkF3NZKQBAi6SnxuqZe0epx9lJOlJVp2f/vFYbtpUGOywAFkIDo4XOSYvXuPO7SpLm/89GTrkGAItKjDvawKikgQEAsJ7O6Qma9u/DNPmK/kqIjdKekgrN/eAbPf7fq7VhWymNDADAGUtOiNaDNw3TwHNTVVPn1h/fW68vN+wLdlgALIIGRitMuqSXepydqIrqer38/zZwujUAWFBSnEuSVF5VF+RIAAAIDJvNph8MPltP3zVCV13UXa4ou7bvLdPsv67TjAWr9I/1e1VXz74OAOD0YqOd+uWkIRoxoJPcHqNXP9qoJSu/D3ZYACyABkYrOB123f3jQYqLdmrrnjIt/LSAbygBgMU0nIFRxiWkAAAWFxcTpYkX99If7r5I487vougoh3YXV2jBx5t0/0v/0rufFaiwhBuzAgBOzemw66dXD9BlOd4rl/z1swL99dMCeThmBsAPNDBaKT0lVpOv7C9JWr56t95ami+PhxUyAFhFYsMZGJWcgQEAiAzJ8S7dOC5Tz025SJPG9FJqYrTKKmr1ycrv9fCfVuqxN3K1fPVuHSyvCXaoAIAQZbfZdP3Y3pp0SS9J0pJV32v+R5u4egmAVnMGO4BwNiwzXTddlqm3/y9fn60pVHlFre68eqCinPSFACDccQ8MAECkio+J0o9GdNf4nK5aV1Cqf23Yq/VbS7V9b7m27y3X20vz1ePsRGX3SdeQ3mnqnB4vu80W7LABACHCZrPpRxd0V1KcS699vFlffrtPR6rqdM81gxTtcgQ7PABhhgaGn8YO66KE2Ci9+uFG5W0pVvGh1brtR/3UPSMx2KEBAPyQkhgtSSo+VBXkSAAACA6nw67z+qbrvL7pKquo1cqN+5W7uUhbCw/7mhkf/H2bEuOi1L97qvp1T1X/7qnqmBIrGw0NAIh4I7POVkJslF5atEHfbCvV4/+dp8uGd9Xw/p0UHUUjA8CZoYHRBob376SE2CjN+9sG7dxfrt+/kavx53fVhJHnKi4mKtjhAQBaoefZSZKkbXvL5DGGb5YCACJaUrxL43O6anxOVx0+UqOvC0q09rsSbf7+oMor67RqU5FWbSryjhsXpR5nJ3l/zvH+TohlvwgAItGQ3mm6/4Zs/fG99SosqdBrH2/WX5YXaOSgDI3J7qxz0uKDHSKAEEcDo40MOPcsPXHnBfrz8u+0alOR/i93lz5ft0ejBp+t8ed3VXpKbLBDBAC0QOf0eEVHOVRV49ae4gp16ZgQ7JAAAAgJyQnRGj20s0YP7ax6t0fb9pRp086D2rTzoLYWHlZZZZ3WbS3Vuq2lvteclRStc9Li1TktXp3TEtQ5PV5nd4hTjItdUgCwul6dk/XEnRfoH+v3asXXhSo+VK1lq3dr2erd6ts1RWOyO2tYZjqXZAfQrDbfWvR4PJo7d67effddlZWV6bzzztOMGTPUvXv3tv6okJOcEK27fzxIFw0q0bufbVVhSYWW5e3W8rzd6tU5WcMy05XdJ00dUzmlGgAaC8Xa4bDb1adrsjZsO6B1W0toYABACAnFuhGpnA67MrumKLNrin78gx6qrXNrV9ERbdtbpu17y7R9T5n2H6zSgbIaHSir0YZtB5q8PinepY4psUpPiVF6SqzSU2J1dlq8ehibbB5u+AqgeS2tAwcPHtTjjz+uv//975Kkyy+/XNOnT1dcXFx7hh3REuNc+tGI7vrhBd20cfsBfba2UF8XlGjLrkPasuuQEuOiNGrwORo99By+BAygiTZvYMybN09/+ctf9NRTT6lTp0569tlndeedd+qjjz6Sy+Vq648LSYN7pSmrZwd9u+OA/nfVLn27/YAKCg+roPCw/vpZgZITXMrskqKe5ySpS3qCuqTHKyneRVMDQMQK1doxrE+6Nmw7oJUbi/SjEd25jBQAhIhQrRuQXFEO9eqcrF6dk33DKqrrVFhcoT0lFSosrlBhyRHtKalQWWWdyipqVVZRq4LCw82+X2JclFISopWaGK2UBJdSEqKVnBCtxNgoJTT8xHl/Ox18cxeIFC2tA/fdd59qamr0+uuvq6ysTA899JAeffRR/eEPfwhC9JHNbrNpUM8OGtSzgw6UVevv6/bo7+v26NCRWn381U598tVO9euequ4ZieqcFq9z0uJ1Tod4bv4NRLA2bWDU1tZqwYIFuv/++zV69GhJ0vPPP69Ro0Zp6dKluvLKK9vy40KazWbToB4dNKhHBx0sr9Ga/GKtyS9W/q5DOnykVrmbi5S7ucg3fmy00/vNo9RYnZUY7dtIT4p3KTEuSolxLsXHONkoB2A5oVw7hvVN18LPCrS7+Ij+sX6vLh5yTtBiAQB4hXLdQPPiY6J8Z2k0dqSqTiWHq1R8qFpFBytVfKhaxYeqVHK4WgfLa1Tv9qi8sk7llXXaVXTktJ8T43L4mhrxsVGKjXYq1uVQbLRTMUd/N/nb5f3b5XLI5bTL5XTIFWWXw27jy2VACGtpHVi7dq1WrVqljz/+WL169ZIk/f73v9dPf/pT/frXv1anTp3aPQd4nZUUo2tG9dTVI8/V19+VasXXhfp2+wHfZQkbS0uO8V2K8BwaG0BEadMGxubNm1VRUaERI0b4hiUlJWnAgAHKzc2N2J2J1MRoXXpeF116XhfV1rm1fW+Zvtt9WDv3l2t3cYWKDlSqqqZeO/eXa+f+8lO+V3SUQ3Ex3g3vuBinEuNcctgll9Oh6CjvBnd0lENRTruinN7fTodNUQ67nA67nEcfO+3exw6HTQ770R+HXXa7TQ6bTXa798dht8lu8zZk7Hab7Dab7HZvx5yNegBtIZRrR1KcSxNGnqt3P9uqt/5vi4oPVemCAZ10Tlo8Z2MAQJCEct1AyzQ0G87NSGoy3Om0KyUlTrv2HFLJIW8z49CRGh1q+H2kVkeq63Sksk5HqupUUV0nY6TqWreqa90qOVztV1w2m3zNDNfR/Srv3979q+goh3efymH37Uc5HTY57Ed/O+xy2m1yNBqnYd/L9xq73bdf5bDbZGvY17Kp0X6X7eh+1/HD5PvbdvT19sb7bUdf491UsUk2ye32yOMx8hgjm8S+HMJaS+tAXl6e0tPTfc0LSRo+fLhsNptWr16tK664ot1iR/McdrvO65uu8/qmq+hgpTZsP6A9JUfP3CupUHllnUoOV6vkcLXWN7q/kiS5ouyKj4lSXIxT8TFRio9xHve393d0lOPoMTG795jZ0XV3lPPY8bLYaKfi69yqd3vkMYZ9PiBEtGkDY9++fZKks88+u8nwjh07au/eva1+X2crb+LjOHq2giOEzlpwOu0a2LODBvbs4BtWW+dW0aEqFR+sUtEh7/VhD5ZX61B5je+06iNVdZKkmjq3aurcOlheE6wUfBo2fG02NdpIbvpYtmMb0kcf+jaWj21PHx1Xx8ZTo6aJ8Zhjn9lovMZxyPdejeJrptCcSe052cZ8s0NbUMtszYxss0l2h10et0fGNPOiZsYPNw6HXW63da9f3B75RTns+snY3upz3LcWrSJQtaOtjD+/q7YWlmlNfrH+58ud+p8vd8rpsCk+JkquKHuTAwh2W5AavKf4uDOJxGazyeG0y13vkTmTlZEFRGLOUmTmHYk5S95vNE67NSfYYQREqNcNtA2bzabEOJdiXU51Pc19qDzGqLK6XkeqvA2NI5Xepoa3oVGvqhq3qmrrVV3j/fv4YTX1HtXWuX3b48Yc2++ysuP3xxoaHjabmuyTNezHSd5tHanRPl2jcRr2xxr2CRv2+k61WXT8c032mU547uQvtJ38qeM+7+SvO+XnHffIbm+0H3B8aWltTiFgaJ80TRjZI9hhnFZL68D+/ftPGNflciklJYW6EYI6psZpbGrTe5OUVdZqb6OGRkNzo6yyTrV1HtXW1QTsOFnDWXlNms4N+4BqtB48br3XdP3YeF3q5XvO+xLfaxuznWz1cZKVR+P1jc0m7zbwmR5vOv0oJ8TUXmw2m2+dG+7b81bKRfLeC+2WKweoe3p8YD+nLd+sqqpKkk643mB0dLQOH27+mqanY7fblJrq30RISgr9m/906ph0yufdHqPK6jqVV9aqoqpOldX1qqyuU1XDRnhNvXcju9a7oV1bd+x3Xb3H91Pv9v52ezyqdxvV1XvkOfq32+2R22O8P26PPKf5f2Qk7382I7lP2GoD0JYK9pZr+ODOwQ4jIAJRO6TWNb+ba3w7nXbdN2mw1uYXa3nebn23+7Bq6tw6XFHb6tgAINB27CvXvtJKpSVa734QgaobUtvVDvinNdPUFeVQSmJ0qz/TGO9+UE2dW3V1HtXWu70HxU747W121LuP7UPVH92PqnebRn8fHe42qvc0jNd0fM/Rz/R4jIyR7wwJ7+9GjxsNM03G8b6+tcc/zNF/jPefxkMRoY5U1enai3uG/Bk6La0DVVVVzd4XIzo6WjU1/h30ttIXblurPXI5KylGZyXFNPkysKSjzetaVVTXq6Kqrunv6jrf8bMjVXWqbXRMrL7eozr3scfe42TNr/+8w41k7Z420Gq5G/er59jeAf2MNm1gxMTESPJej7Dhb0mqqalRbGzrmggej1FZWWWrXutw2JWUFKuysirLfAM81mFTbIJLSnAFPD9jvBvDDRvXxhh5PPJtMBtzbMPa+Mb3vsbo2HAdHe/Yc0YN28jm6FZz4287NTxvd9gVF+dSRUWN3G7TZLzGMXrfQU22tU0z4zR9XfPjHjfWKV/XFux2m+LiolVZ2ZBjeDGn2cGx273zsLKyVh6PNf4PNtZe+bmcDvXtlqKDByta/NqkpNiQ3ygORO3wt/ndXON73IgEjRvRQ263R6Vl1TpSWafaOvfRAwcnHnAIJcaYkN8RBdC2OiTHqMc5yacfMQwFom5Igakd8A/T9Mw03i/zGCPTZP9MJ+yTHX3ofV7Hhnk8x++rNd1/85hj+2Me08x4xvj2yzy+z2m6TWRO+uDEfa3G+xqn2w9r0eccN+CE9zZnPq5fn9NG2uobvEZSny4pSk2KOe24wdbSOhATE6Pa2hO/fFRTU6O4uLgThp+pSPnC7ZkKRi6pbfheHo9Rndvju+Seu1HDuKHZfPxjyft/u2Gd2HDszEgyR9epDetHqemxq4bnGp5obp3X3DrmZMdh/FkVhMOZAKEfYWRyOuzK6tVBUc7A3oumTRsYDafkFRUVqVu3br7hRUVF6tevX6vft77evwODbrfH7/cIZYHOzybJ0XD+WzseB3U67UpNjdfBgxWWnX9Wz5H82pDxf10YqgJRO1rb/D7TxnCUpNQ4p9q4jAaNFRv+pxOJOUuRmXck5iwd+xZka/IO9eZ3oPY5Al07cOaYpm2r8fT0HDc9bTp22RB744GNf584IOJZdhl1u8PiS1MtrQMZGRlatmxZk2G1tbU6dOiQXzfw5gu3XlbNpWF9aT/64/3HJjnCY11o1flCLqHF4fDeKyzQ+xxteuSlX79+SkhI0MqVK31FpKysTBs3btRNN93Ulh8FALCIQNUOfxo+Vm98n0wk5h2JOUuRmXck5ixZM+9A7nNQO0IL07RtMT3bHtM0OFpaB3JycvTcc89p586d6t69uyRp5cqVkqRhw4b5FQtfuD2GXEITuYQmK+UiBT6fNm1guFwu3XTTTXruued01llnqXPnznr22WeVkZGh8ePHt+VHAQAsgtoBAGgJ6gYARLbT1QG3260DBw4oMTFRMTExGjJkiIYNG6apU6dq5syZqqys1IwZM3TNNdf4dQYGAKB9tPm1L+677z7V19frd7/7naqrq5WTk6P58+c3e8MkAAAkagcAoGWoGwAQ2U5VB3bv3q1LL71UTz31lCZOnCibzaa5c+fq0Ucf1a233qro6Ghdfvnlmj59erDTAACcAZsJ8Tu1uN0eHTjQ8mswSlx/P9xZPT/J+jmSX/CddVZ8SF/HPFBaWzvCYZ4GQiTmHYk5S5GZdyTmLPmXN7WjZSJ1GQskpmnbYnq2PaZpU9SNlrPSMkQuoYlcQpOVcpHab58j8ioMAAAAAAAAAAAIeTQwAAAAAAAAAABAyKGBAQAAAAAAAAAAQg4NDAAAAAAAAAAAEHJoYAAAAAAAAAAAgJBDAwMAAAAAAAAAAIQcGhgAAAAAAAAAACDk0MAAAAAAAAAAAAAhhwYGAAAAAAAAAAAIOTQwAAAAAAAAAABAyLEZY0ywgzgVY4w8ntaH6HDY5XZ72jCi0EJ+4c/qOZJfcNntNtlstmCH0e78qR2hPk8DJRLzjsScpcjMOxJzllqfN7Wj5SJ1GQskpmnbYnq2PabpMdSN1rHSMkQuoYlcQpOVcpHaZ58j5BsYAAAAAAAAAAAg8nAJKQAAAAAAAAAAEHJoYAAAAAAAAAAAgJBDAwMAAAAAAAAAAIQcGhgAAAAAAAAAACDk0MAAAAAAAAAAAAAhhwYGAAAAAAAAAAAIOTQwAAAAAAAAAABAyKGBAQAAAAAAAAAAQg4NDAAAAAAAAAAAEHJoYAAAAAAAAAAAgJBDAwMAAAAAAAAAAIQcGhgAAAAAAAAAACDkWLKB4fF49J//+Z8aNWqUhgwZosmTJ2vnzp3BDqvVDh06pEceeUQXX3yxhg0bphtuuEF5eXm+5zdt2qSbbrpJQ4cO1ZgxYzR//vwgRuuf7du3Kzs7Wx988IFvmFXyW7Roka644gplZWXpyiuv1CeffOJ7LtxzrKur0/PPP68xY8YoOztbN954o9asWeN7Ppzzmzdvnm6++eYmw06Xj9XWQZHCavPN39phhenRmpoSznn7U2fCMW9/a0+45RyoehTONTrYwm0ZCjbqUmBFWs0LpEirp2hfVlpGCgsL1bdv3xN+3n333WCH1iKt2cYKVc3lMn369BPm0cUXXxykCE/NSsc/T5dLOM0XSSotLdX999+vESNGKDs7Wz/72c9UUFDgez7g88ZY0Jw5c8yFF15oVqxYYTZt2mQmT55sxo8fb2pqaoIdWqvcfvvtZsKECSY3N9ds3brVPPbYY2bw4MGmoKDAHDhwwFxwwQXmoYceMgUFBea9994zWVlZ5r333gt22C1WW1trJk6caDIzM837779vjDGWyW/RokWmf//+5vXXXzc7duwwc+fONf369TNr1qyxRI5//OMfzciRI80XX3xhduzYYR566CEzbNgws2/fvrDO77XXXjN9+/Y1N910k2/YmeRjtXVQpLDafPO3doT79GhtTQnXvP2tM+GYt7+1J5xyDlQ9CucaHQrCaRkKBZFelwIp0mpeIEViPUX7stIysnz5cpOVlWX2799vioqKfD9VVVXBDu2MtXYbKxQ1l4sxxlx77bVm9uzZTeZRaWlpkKI8NSsd/zxVLsaE13wxxphJkyaZ66+/3qxfv94UFBSYe++914wcOdJUVla2y7yxXAOjpqbGZGdnm3feecc37PDhw2bw4MHmo48+CmJkrbNjxw6TmZlpVq9e7Rvm8XjM+PHjzQsvvGBefvllM2rUKFNXV+d7ftasWeaHP/xhMML1y6xZs8zNN9/cZMPbCvl5PB5zySWXmKeffrrJ8MmTJ5uXX37ZEjlOmDDBPPXUU77H5eXlJjMz0yxZsiQs89u3b5+54447zNChQ83ll1/eZAPgdPlYbR0UKaw23/ytHVaYHq2pKeGat791Jlzz9qf2hEvOga5H4VijQ0W4LEOhgroUWJFU8wIpUusp2o/VlpGXXnrJTJgwIdhhtIo/21ih5lS51NfXm6ysLLN06dIgRnhmrHT883S5hNN8Mcbb1Js6darJz8/3Ddu0aZPJzMw069ata5d5Y7lLSG3evFkVFRUaMWKEb1hSUpIGDBig3NzcIEbWOqmpqXrllVc0aNAg3zCbzSZjjA4fPqy8vDzl5OTI6XT6nh8xYoS2b9+u0tLSYITcKrm5uVq4cKH+8Ic/NBluhfy2bdumwsJCXX311U2Gz58/X3fddZclckxJSdFnn32m3bt3y+12a+HChXK5XOrfv39Y5vftt98qOTlZixcv1pAhQ5o8d7p8rLYOihRWm2/+1o5wnx6trSnhmre/dSZc8/an9oRLzoGuR+FYo0NFuCxDoSLS61IgRVrNC6RIradoP1ZbRrZs2aLevXsHO4xW8WcbK9ScKpcdO3aopqZGvXr1ClJ0Z85Kxz9Pl0s4zRfJm8/s2bPVp08fSVJJSYnmz5+vjIwM9e7du13mjeUaGPv27ZMknX322U2Gd+zYUXv37g1GSH5JSkrS6NGj5XK5fMM++eQTff/99/rBD36gffv2KSMjo8lrOnbsKEnas2dPu8baWmVlZXrggQf0u9/97oT5ZoX8duzYIUmqrKzUHXfcoQsvvFCTJk3Sp59+KskaOT700ENyOp269NJLlZWVpeeff14vvPCCunXrFpb5jR07VrNmzVLXrl1PeO50+VhtHRQprDbf/K0d4Tw9/Kkp4Zq3v3UmXPP2p/aES86BrkfhWKNDRbgsQ6EikutSIEVizQukSK2naD9WW0by8/NVWlqqG2+8URdddJFuuOEGffHFF8EO64z4s40Vak6VS35+vmw2m9544w2NHTtW48aN02OPPaby8vIgRHpqVjr+ebpcwmm+HO/hhx/WyJEjtWTJEj3xxBOKi4trl3ljuQZGVVWVJDVZSCQpOjpaNTU1wQipTa1evVr/8R//oUsvvVRjx45VdXV1s7lKCpt8Z86cqaFDh57wTRdJlsjvyJEjkqRp06bpqquu0oIFCzRy5Ejdc889+vLLLy2R49atW5WUlKQXX3xRCxcu1MSJEzVt2jRt3rzZEvk1drp8rL4Osiqrz7eW1o5wnh7+1JRwzdvfOhOueftTe8I158baIker1ej2ZIVlKJgiqS4FUiTWvECK1HqK9mOlZaS2tlY7duzQkSNH9Ktf/UqvvPKKsrKydOedd+rLL78Mdnh+sdL20XfffSe73a7OnTvr5Zdf1rRp0/T555/rnnvukcfjCXZ4p2Sl45/H5xLO8+XWW2/V+++/rwkTJmjKlCn69ttv22XeOE8/SniJiYmR5F2ZNvwteSdYbGxssMJqE8uWLdNvf/tbDRkyRLNnz5bkzbe2trbJeA0LR1xcXLvH2FKLFi1SXl6ePvzww2afD/f8JCkqKkqSdMcdd+jaa6+VJPXv318bN27Ua6+9FvY5FhYW6v7779frr7+u888/X5KUlZWlgoICzZkzJ+zzO97p8rHyOsjKrDzfWlM7wnV6+FtTwjVvf+tMOObtb+0Jx5yP1xY5Wq1GtycrLEPBEkl1KZAiteYFUiTWU7QvKy0jLpdLubm5cjqdvgOXgwYN0tatWzV//nxdeOGFQY6w9ay0fXTvvffqtttuU1JSkiQpMzNT6enpuv766/XNN9+ccMmpUGGl45/N5RKu80WS77Jxjz32mL7++mu99dZb7TJvLHcGRsOpeEVFRU2GFxUVnXA6Szh56623dO+99+riiy/Wq6++6it2GRkZzeYqSZ06dWr3OFvq/fffV2lpqcaMGaPs7GxlZ2dLkmbMmKErr7wy7POT5FvuMjMzmwzv3bu3du/eHfY5rl+/XnV1dcrKymoyfMiQIdqxY0fY53e80+Vj1XWQ1Vl1vrW2doTr9PC3poRr3v7WmXDM29/aE445H68tcrRajW5PVliGgiHS6lIgRWrNC6RIrKdoX1ZbRuLi4k741nVmZqb2798fpIjahpW2j2w2m+8geYOGdVzDJc1CjZWOf54sl3CbL6Wlpfroo4/kdrt9w+x2u3r16uVbfwV63liugdGvXz8lJCRo5cqVvmFlZWXauHGj7xt64eadd97RY489pn//93/XCy+80KRA5OTkaPXq1U0Woi+//FI9evRQhw4dghFuizz33HP6+OOPtWjRIt+PJN1333165ZVXwj4/SRowYIDi4+O1bt26JsPz8/PVrVu3sM+xYSNsy5YtTYbn5+ere/fuYZ/f8U6XjxXXQZHAivPNn9oRrtPD35oSrnn7W2fCMW9/a0845ny8tsjRajW6PVlhGWpvkViXAilSa14gRWI9Rfuy0jKyefNmZWdnKy8vr8nwDRs2hO2NvRtYafvoN7/5je64444mw7755htJCsn5ZKXjn6fKJdzmS1FRkX7zm99o1apVvmF1dXXauHGjevXq1T7zxljQ7NmzzfDhw82yZcvMpk2bzOTJk81ll11mampqgh1ai23bts0MHDjQTJkyxRQVFTX5KSsrMyUlJSYnJ8dMmzbNfPfdd+b99983WVlZ5oMPPgh26K2WmZlp3n//fWOMsUx+L774osnOzjYffvih2blzp5k3b57p16+f+eqrr8I+R7fbbW688UZz+eWXmy+//NJs377dPP/886Z///5m7dq1YZ/ftGnTzE033eR7fCb5WGkdFEmsNN/aonZYZXq0tKaEa97+1plwy7stak+45RyIehTuNTrYwm0ZCibqUvuIlJoXSJFWT9H+rLKMuN1uM2nSJHPVVVeZ3NxcU1BQYJ588kkzaNAgs3nz5mCH1yKt2cYKVcfn8umnn5q+ffuaefPmmZ07d5oVK1aYsWPHml//+tdBjLJ5Vjr+ebpcwmm+GGOMx+MxkydPNj/84Q9Nbm6u2bJli5k6darJyckxhYWF7TJvLNnAqK+vN88884wZMWKEGTp0qLnzzjvNrl27gh1Wq7z00ksmMzOz2Z9p06YZY4xZt26d+clPfmIGDRpkLrnkEvPmm28GOWr/NN7wNsY6+S1YsMCMHTvWDBw40EyYMMEsXbrU91y453jo0CEzc+ZMM2bMGJOdnW2uv/56s3LlSt/z4Zzf8RsAxpw+HyutgyKJleZbW9QOq0yPltaUcM7bnzoTjnn7W3vCLedA1aNwrtHBFm7LUDBRl9pHJNW8QIq0eor2ZaVlpLS01EyfPt2MHDnSZGVlmeuvv97k5uYGO6wWa802VqhqLpclS5aYa665xgwePNiMHDnSPP3006a6ujpIEZ6clY5/nkku4TJfGpSVlZkZM2aYkSNHmsGDB5vJkyeb/Px83/OBnjc2Y4xpm3M5AAAAAAAAAAAA2obl7oEBAAAAAAAAAADCHw0MAAAAAAAAAAAQcmhgAAAAAAAAAACAkEMDAwAAAAAAAAAAhBwaGAAAAAAAAAAAIOTQwAAAAAAAAAAAACGHBgYAAAAAAAAAAAg5NDAAAAAAAAAAIICMMZb+PCBQaGAAAAAAAAAAQBuZM2eO+vbtK0kqKyvTtGnTlJeX126fX1BQoBtuuKHJsL59+2rOnDntFgPQVmhgAAAAAAAAAEAAbNq0SYsWLZLH42m3z/zkk0+0du3aJsMWLlyoSZMmtVsMQFtxBjsAAAAAAAAAAEDgDB06NNghAK3CGRgAAAAAAAAA0MZWrlypW265RZJ0yy236Oabb/Y9t2zZMk2cOFFZWVkaOXKkHn/8cVVWVvqenzNnjsaPH6+5c+fqggsu0Lhx43Tw4EFVV1dr1qxZuuyyyzRo0CANGzZMt99+uzZt2uR73dy5cyU1vWzU8ZeQKioq0vTp0zV69GgNHjxY1113nZYvX94k/r59++rtt9/WQw89pOHDhys7O1v33XefSkpKfOPs2rVLP//5z3XBBRdoyJAhuv766/X555+38ZREJKOBAQAAAAAAAABtbODAgXrkkUckSY888ohmzJghSfrwww81ZcoU9ezZUy+++KJ+8YtfaPHixbrnnnua3Hx7z549Wrp0qWbPnq1f/epXSk1N1QMPPKD33ntPP/vZz7RgwQI9+OCDys/P19SpU2WM0aRJk3TddddJOvllo0pKSnTddddp1apVmjp1qubMmaPOnTtrypQpWrx4cZNxn3/+eXk8Hs2ePVsPPPCAVqxYoSeffFKS5PF4dNddd6myslLPPPOM5s2bp5SUFN1zzz3auXNnQKYpIg+XkAIAAAAAAACANpaQkKDevXtLknr37q3evXvLGKPnnntOo0aN0nPPPecb99xzz9Vtt92mzz//XGPGjJEk1dfXa9q0abroooskSbW1taqoqNDDDz+sK664QpI0fPhwVVRU6Omnn1ZxcbEyMjKUkZEh6eSXjXrttdd04MABffLJJ+rataskafTo0brtttv0zDPP6KqrrpLd7v3ee2Zmpp566infa9evX68lS5ZIkkpLS7V161bdfffdGj16tCRp8ODBmjt3rmpqatpiEgKcgQEAAAAAAAAA7WHbtm3at2+fxo4dq/r6et9PTk6OEhIS9M9//rPJ+JmZmb6/XS6X5s+fryuuuEJFRUXKzc3VwoUL9dlnn0mS6urqziiGVatWKTs729e8aDBhwgQVFxdr27ZtvmHHN0EyMjJUVVUlSUpLS1Pv3r318MMP68EHH9THH38sY4ymT5/eJG7AH5yBAQAAAAAAAADt4NChQ5KkRx99VI8++ugJzxcVFTV5nJaW1uTxF198oSeffFLbtm1TfHy8+vbtq/j4eElqcvmpUzl8+LC6dOlywvCGzyorK/MNi42NbTKO3W73fY7NZtOCBQv00ksvaenSpfrb3/6mqKgojRs3TjNnzlRKSsoZxQOcCg0MAAAAAAAAAGgHSUlJkqQHHnhAw4cPP+H55OTkk772+++/15QpU3TppZfqv/7rv9StWzdJ0ttvv60vvvjijGNITk5uciPuBsXFxZKk1NTUM36vTp06aebMmZoxY4Y2b96sJUuW6NVXX1VycnKzDRqgpbiEFAAAAAAAAAAEgMPhaPK4Z8+e6tChg3bv3q2srCzfT0ZGhmbNmqWNGzee9L02bNigmpoa3XXXXb7mhSRf86LhzIiG+1ecTE5OjtauXatdu3Y1Gb548WKlp6ere/fuZ5Tb2rVrddFFF2n9+vWy2Wzq37+/pk6dqszMTO3bt++M3gM4Hc7AAAAAAAAAAIAASExMlCStWLFCycnJ6tevn6ZOnapHHnlEDodDl1xyicrKyjRv3jzt379fAwcOPOl7DRw4UE6nU88++6wmT56s2tpaffDBB1qxYoUkqbKyUtKxszw++ugjDRky5IR7Xdx+++1avHixbr/9dv3iF79QamqqFi1apK+++kpPPvnkaRsgDQYMGKCYmBg98MADuvfee5WWlqZ//etf2rRpk2655ZaWTiqgWZyBAQAAAAAAAAAB0KdPH1111VV6++239dvf/laSNGnSJM2aNUtr1qzR3XffrZkzZ6pLly568803T2g2NNa9e3fNmjVL+/fv189//nM98sgjkqQ333xTNptNeXl5kqTLLrtMWVlZevDBBzV//vwT3ic9PV1//vOfNWjQID3xxBP65S9/qb1792revHn6t3/7tzPOLTo6WgsWLFCfPn30xBNP6I477tDy5cv1+9//XhMnTmzJZAJOymbO9O4uAAAAAAAAAAAA7YQzMAAAAAAAAAAAQMihgQEAAAAAAAAAAEIODQwAAAAAAAAAABByaGAAAAAAAAAAAICQQwMDAAAAAAAAAACEHBoYAAAAAAAAAAAg5NDAAAAAAAAAAAAAIYcGBgAAAAAAAAAACDk0MAAAAAAAAAAAQMihgQEAAAAAAAAAAEIODQwAAAAAAAAAABByaGAAAAAAAAAAAICQ8/8B3sEgwBST7WIAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1600x500 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "for i, model in enumerate([gd_model, sgd_model, momentum_model, adagrad_model]):\n",
        "    plt.subplot(1, 4, i + 1)\n",
        "    plt.plot(np.arange(0, len(model.loss_history)), model.loss_history)\n",
        "    plt.title(f\"Model {model.gd_type}\")\n",
        "\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adagram converges first, then batch gradient descent, then we have momentum and stochastic doesn't stop early (does all 1000 iterations).\n",
        "\n",
        "However, I find this assignment very weird, because each step of SGD is much smaller in terms of FLOPS and samples.\n",
        "Obviously momentum with batch gradient descent achieved worse results then batch gradient descent without momentum, because \n",
        "the point of momentum is to make gradient  descent on mini-batches (SGD, Adam) more stable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
