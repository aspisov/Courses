{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBrDXMdDy-Qn"
      },
      "source": [
        "# HSE 2024: Mathematical Methods for Data Analysis\n",
        "\n",
        "## Homework 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXi5K1mf41d"
      },
      "source": [
        "# Attention!\n",
        "\n",
        "* For tasks where <ins>text answer</ins> is required **Russian language** is **allowed**.\n",
        "* If a task asks you to describe something (make coclusions) then **text answer** is **mandatory** and **is** part of the task\n",
        "* **Do not** upload the dataset (titanic.csv) to the grading system (we already have it)\n",
        "* We **only** accept **ipynb** notebooks. If you use Google Colab then you'll have to download the notebook before passing the homework\n",
        "* **Do not** use python loops instead of NumPy vector operations over NumPy vectors - it significantly decreases performance (see why https://blog.paperspace.com/numpy-optimization-vectorization-and-broadcasting/), will be punished with -0.25 for **every** task.\n",
        "Loops are only allowed in part 1 (Tasks 1 - 4).\n",
        "* Some tasks contain tests. They only test you solution on a simple example, thus, passing the test does **not** guarantee you the full grade for the task.\n",
        "\n",
        "If the task asks for an explanation of something, it means that a written answer is required, which is part of the task and is assessed\n",
        "\n",
        "We only accept ipynb notebooks. If you use Google Colab, you need to download the notebook before submitting your homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-26T16:48:20.566549Z",
          "start_time": "2020-09-26T16:48:19.893995Z"
        },
        "id": "mSR-a9vVy-Qp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.regression.linear_model import OLSResults\n",
        "from math import sqrt\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set(style=\"darkgrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUjuv9Qty-Qq"
      },
      "source": [
        "### Data\n",
        "\n",
        "For this homework we will use a dataset of tracks from the streaming service Spotify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHSjHiIDOgHP"
      },
      "source": [
        "**Описание данных**\n",
        "\n",
        "- **track_id:** The Spotify ID for the track\n",
        "- **artists:** The artists' names who performed the track. If there is more than one artist, they are separated by a ;\n",
        "- **album_name:** The album name in which the track appears\n",
        "- **track_name:** Name of the track\n",
        "- **popularity:** The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are. Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past. Duplicate tracks (e.g. the same track from a single and an album) are rated independently. Artist and album popularity is derived mathematically from track popularity.\n",
        "- **duration_ms:** The track length in milliseconds\n",
        "- **explicit:** Whether or not the track has explicit lyrics (true = yes it does; false = no it does not OR unknown)\n",
        "- **danceability:** Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable\n",
        "- **key:** The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1\n",
        "- **loudness:** The overall loudness of a track in decibels (dB)\n",
        "- **mode:** Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0\n",
        "- **speechiness:** Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks\n",
        "- **acousticness:** A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic\n",
        "- **instrumentalness:** Predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content\n",
        "- **liveness:** Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live\n",
        "- **valence:** A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)\n",
        "- **tempo:** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration\n",
        "- **time_signature:** An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of 3/4, to 7/4.\n",
        "- **track_genre:** The genre in which the track belongs\n",
        "\n",
        "**Target variable**\n",
        "- **energy:** Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "tHWSWTXDy-Qq"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('dataset.csv')\n",
        "\n",
        "y = data['energy']\n",
        "X = data.drop(['energy'], axis=1)\n",
        "columns = X.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artists</th>\n",
              "      <th>album_name</th>\n",
              "      <th>track_name</th>\n",
              "      <th>popularity</th>\n",
              "      <th>duration_ms</th>\n",
              "      <th>explicit</th>\n",
              "      <th>danceability</th>\n",
              "      <th>energy</th>\n",
              "      <th>key</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>liveness</th>\n",
              "      <th>valence</th>\n",
              "      <th>tempo</th>\n",
              "      <th>time_signature</th>\n",
              "      <th>track_genre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Gen Hoshino</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>73</td>\n",
              "      <td>230666</td>\n",
              "      <td>False</td>\n",
              "      <td>0.676</td>\n",
              "      <td>0.4610</td>\n",
              "      <td>1</td>\n",
              "      <td>-6.746</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1430</td>\n",
              "      <td>0.0322</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.3580</td>\n",
              "      <td>0.715</td>\n",
              "      <td>87.917</td>\n",
              "      <td>4</td>\n",
              "      <td>acoustic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ben Woodward</td>\n",
              "      <td>Ghost (Acoustic)</td>\n",
              "      <td>Ghost - Acoustic</td>\n",
              "      <td>55</td>\n",
              "      <td>149610</td>\n",
              "      <td>False</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.1660</td>\n",
              "      <td>1</td>\n",
              "      <td>-17.235</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0763</td>\n",
              "      <td>0.9240</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>0.267</td>\n",
              "      <td>77.489</td>\n",
              "      <td>4</td>\n",
              "      <td>acoustic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ingrid Michaelson;ZAYN</td>\n",
              "      <td>To Begin Again</td>\n",
              "      <td>To Begin Again</td>\n",
              "      <td>57</td>\n",
              "      <td>210826</td>\n",
              "      <td>False</td>\n",
              "      <td>0.438</td>\n",
              "      <td>0.3590</td>\n",
              "      <td>0</td>\n",
              "      <td>-9.734</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0557</td>\n",
              "      <td>0.2100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.1170</td>\n",
              "      <td>0.120</td>\n",
              "      <td>76.332</td>\n",
              "      <td>4</td>\n",
              "      <td>acoustic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Kina Grannis</td>\n",
              "      <td>Crazy Rich Asians (Original Motion Picture Sou...</td>\n",
              "      <td>Can't Help Falling In Love</td>\n",
              "      <td>71</td>\n",
              "      <td>201933</td>\n",
              "      <td>False</td>\n",
              "      <td>0.266</td>\n",
              "      <td>0.0596</td>\n",
              "      <td>0</td>\n",
              "      <td>-18.515</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0363</td>\n",
              "      <td>0.9050</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>0.1320</td>\n",
              "      <td>0.143</td>\n",
              "      <td>181.740</td>\n",
              "      <td>3</td>\n",
              "      <td>acoustic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Chord Overstreet</td>\n",
              "      <td>Hold On</td>\n",
              "      <td>Hold On</td>\n",
              "      <td>82</td>\n",
              "      <td>198853</td>\n",
              "      <td>False</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.4430</td>\n",
              "      <td>2</td>\n",
              "      <td>-9.681</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0526</td>\n",
              "      <td>0.4690</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0829</td>\n",
              "      <td>0.167</td>\n",
              "      <td>119.949</td>\n",
              "      <td>4</td>\n",
              "      <td>acoustic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  artists                                         album_name  \\\n",
              "0             Gen Hoshino                                             Comedy   \n",
              "1            Ben Woodward                                   Ghost (Acoustic)   \n",
              "2  Ingrid Michaelson;ZAYN                                     To Begin Again   \n",
              "3            Kina Grannis  Crazy Rich Asians (Original Motion Picture Sou...   \n",
              "4        Chord Overstreet                                            Hold On   \n",
              "\n",
              "                   track_name  popularity  duration_ms  explicit  \\\n",
              "0                      Comedy          73       230666     False   \n",
              "1            Ghost - Acoustic          55       149610     False   \n",
              "2              To Begin Again          57       210826     False   \n",
              "3  Can't Help Falling In Love          71       201933     False   \n",
              "4                     Hold On          82       198853     False   \n",
              "\n",
              "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
              "0         0.676  0.4610    1    -6.746     0       0.1430        0.0322   \n",
              "1         0.420  0.1660    1   -17.235     1       0.0763        0.9240   \n",
              "2         0.438  0.3590    0    -9.734     1       0.0557        0.2100   \n",
              "3         0.266  0.0596    0   -18.515     1       0.0363        0.9050   \n",
              "4         0.618  0.4430    2    -9.681     1       0.0526        0.4690   \n",
              "\n",
              "   instrumentalness  liveness  valence    tempo  time_signature track_genre  \n",
              "0          0.000001    0.3580    0.715   87.917               4    acoustic  \n",
              "1          0.000006    0.1010    0.267   77.489               4    acoustic  \n",
              "2          0.000000    0.1170    0.120   76.332               4    acoustic  \n",
              "3          0.000071    0.1320    0.143  181.740               3    acoustic  \n",
              "4          0.000000    0.0829    0.167  119.949               4    acoustic  "
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "artists             31437\n",
              "album_name          46589\n",
              "track_name          73608\n",
              "popularity            101\n",
              "duration_ms         50697\n",
              "explicit                2\n",
              "danceability         1174\n",
              "energy               2083\n",
              "key                    12\n",
              "loudness            19480\n",
              "mode                    2\n",
              "speechiness          1489\n",
              "acousticness         5061\n",
              "instrumentalness     5346\n",
              "liveness             1722\n",
              "valence              1790\n",
              "tempo               45653\n",
              "time_signature          5\n",
              "track_genre           114\n",
              "dtype: int64"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K81w8s35y-Qq"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYgEN-FMy-Qr"
      },
      "source": [
        "#### 0. [0.25 points] Code the categorical features. Explain the method you have chosen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "-IrSlQaWy-Qr"
      },
      "outputs": [],
      "source": [
        "# after reading the description and looking at data these features appear to be categorial\n",
        "categorial_features = [\n",
        "    \"explicit\",\n",
        "    \"key\",\n",
        "    \"mode\",\n",
        "    \"time_signature\",\n",
        "    \"track_genre\"\n",
        "]\n",
        "# these features are also categorical, but they have too many unique values, we could fix this by using word embeddings or other NLP methods\n",
        "# however this is not the goal of this task\n",
        "nlp_features = [\"artists\", \"album_name\", \"track_name\"]\n",
        "\n",
        "# there are numerical features\n",
        "numerical_features = [\n",
        "    \"popularity\",\n",
        "    \"duration_ms\",\n",
        "    \"danceability\",\n",
        "    \"loudness\",\n",
        "    \"speechiness\",\n",
        "    \"acousticness\",\n",
        "    \"instrumentalness\",\n",
        "    \"liveness\",\n",
        "    \"valence\",\n",
        "    \"tempo\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dVwP45Gy-Qr"
      },
      "source": [
        "#### 1. [0.25 points] Split the data into train and test with a ratio of 80:20 and random_state=42."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "U7z8TIh5y-Qs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(91200, 22800)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, y = data.drop(['energy'], axis=1), data['energy']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "len(X_train), len(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7daIQRfKy-Qs",
        "tags": []
      },
      "source": [
        "#### 2. [0.75 points] Train models on train, excluding categorical features, using the StatsModels library and apply it to test; use $RMSE$ and $R^2$ as quality metrics. Try also applying linear regression implementations from sklearn:\n",
        "\n",
        "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
        "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.03$;\n",
        "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.05$\n",
        "\n",
        "Don't forget to scale your data using StandardScaler before training your models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Bkbr5iFCy-Qs"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train[numerical_features])\n",
        "X_test_scaled = scaler.transform(X_test[numerical_features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "StatsModels OLS RMSE: 0.12185169282189574\n",
            "StatsModels OLS R²: 0.7640826589205965\n"
          ]
        }
      ],
      "source": [
        "X_train_sm = sm.add_constant(X_train_scaled)\n",
        "X_test_sm = sm.add_constant(X_test_scaled)\n",
        "\n",
        "model_sm = sm.OLS(y_train, X_train_sm).fit()\n",
        "\n",
        "y_pred_sm = model_sm.predict(X_test_sm)\n",
        "\n",
        "rmse_sm = np.sqrt(mean_squared_error(y_test, y_pred_sm))\n",
        "r2_sm = r2_score(y_test, y_pred_sm)\n",
        "\n",
        "print(\"StatsModels OLS RMSE:\", rmse_sm)\n",
        "print(\"StatsModels OLS R²:\", r2_sm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear RMSE: 0.12185169282189576\n",
            "Linear R²: 0.7640826589205963\n"
          ]
        }
      ],
      "source": [
        "linear_model = LinearRegression()\n",
        "\n",
        "linear_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = linear_model.predict(X_test_scaled)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Linear RMSE:\", rmse)\n",
        "print(\"Linear R²:\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ridge RMSE: 0.12185169384565525\n",
            "Ridge R²: 0.7640826549563902\n"
          ]
        }
      ],
      "source": [
        "ridge_model = Ridge(alpha=0.03)\n",
        "\n",
        "ridge_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = ridge_model.predict(X_test_scaled)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Ridge RMSE:\", rmse)\n",
        "print(\"Ridge R²:\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lasso RMSE: 0.14798373613042762\n",
            "Lasso R²: 0.6520436982694833\n"
          ]
        }
      ],
      "source": [
        "lasso_model = Lasso(alpha=0.05)\n",
        "\n",
        "lasso_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = lasso_model.predict(X_test_scaled)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Lasso RMSE:\", rmse)\n",
        "print(\"Lasso R²:\", r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWWvUdxROgHP"
      },
      "source": [
        "#### 3. [0.25 points] Repeat the steps from the previous point, adding categorical features. Comment on the changes in the quality metrics values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "-Opop8b6OgHQ"
      },
      "outputs": [],
      "source": [
        "# numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_numerical = scaler.fit_transform(X_train[numerical_features])\n",
        "X_test_numerical = scaler.transform(X_test[numerical_features])\n",
        "\n",
        "# caterorial features\n",
        "encoder = OneHotEncoder(drop='first', handle_unknown='ignore')\n",
        "X_train_categorial = encoder.fit_transform(X_train[categorial_features]).toarray()\n",
        "X_test_categorial = encoder.transform(X_test[categorial_features]).toarray()\n",
        "\n",
        "X_train = np.concatenate([X_train_numerical, X_train_categorial], axis=1)\n",
        "X_test = np.concatenate([X_test_numerical, X_test_categorial], axis=1)\n",
        "\n",
        "y_train = pd.DataFrame(y_train).reset_index(drop=True)\n",
        "y_test = pd.DataFrame(y_test).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "StatsModels OLS RMSE: 0.11220622836690176\n",
            "StatsModels OLS R²: 0.7999536370568916\n"
          ]
        }
      ],
      "source": [
        "X_train_sm = sm.add_constant(pd.DataFrame(X_train).reset_index(drop=True))\n",
        "X_test_sm = sm.add_constant(pd.DataFrame(X_test).reset_index(drop=True))\n",
        "\n",
        "model_sm = sm.OLS(y_train, X_train_sm).fit()\n",
        "\n",
        "y_pred_sm = model_sm.predict(X_test_sm)\n",
        "\n",
        "rmse_sm = np.sqrt(mean_squared_error(y_test, y_pred_sm))\n",
        "r2_sm = r2_score(y_test, y_pred_sm)\n",
        "\n",
        "print(\"StatsModels OLS RMSE:\", rmse_sm)\n",
        "print(\"StatsModels OLS R²:\", r2_sm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear RMSE: 0.11220622836690186\n",
            "Linear R²: 0.7999536370568914\n"
          ]
        }
      ],
      "source": [
        "linear_model = LinearRegression()\n",
        "\n",
        "linear_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = linear_model.predict(X_test)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Linear RMSE:\", rmse)\n",
        "print(\"Linear R²:\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ridge RMSE: 0.11220611751017649\n",
            "Ridge R²: 0.7999540323375114\n"
          ]
        }
      ],
      "source": [
        "ridge_model = Ridge(alpha=0.03)\n",
        "\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = ridge_model.predict(X_test)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Ridge RMSE:\", rmse)\n",
        "print(\"Ridge R²:\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lasso RMSE: 0.14798373613042762\n",
            "Lasso R²: 0.6520436982694833\n"
          ]
        }
      ],
      "source": [
        "lasso_model = Lasso(alpha=0.05)\n",
        "\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lasso_model.predict(X_test)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Lasso RMSE:\", rmse)\n",
        "print(\"Lasso R²:\", r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69JOftKRy-Qt"
      },
      "source": [
        "#### 4. [1 point] Examine the parameter values ​​of the models obtained from StatsModels and check which weights and in which models turned out to be zero. Comment on the significance of the coefficients, the overall significance of the models and other factors from the resulting tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Np1biYQ7y-Qt"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>         <td>energy</td>      <th>  R-squared:         </th>  <td>   0.804</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.803</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   2664.</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Wed, 23 Oct 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>20:02:01</td>     <th>  Log-Likelihood:    </th>  <td>  70658.</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td> 91200</td>      <th>  AIC:               </th> <td>-1.410e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td> 91059</td>      <th>  BIC:               </th> <td>-1.397e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>   140</td>      <th>                     </th>      <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>const</th> <td>    0.2416</td> <td>    0.011</td> <td>   21.055</td> <td> 0.000</td> <td>    0.219</td> <td>    0.264</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>0</th>     <td>   -0.0033</td> <td>    0.000</td> <td>   -7.714</td> <td> 0.000</td> <td>   -0.004</td> <td>   -0.002</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>1</th>     <td>    0.0011</td> <td>    0.000</td> <td>    2.657</td> <td> 0.008</td> <td>    0.000</td> <td>    0.002</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>2</th>     <td>   -0.0218</td> <td>    0.001</td> <td>  -39.325</td> <td> 0.000</td> <td>   -0.023</td> <td>   -0.021</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>3</th>     <td>    0.1357</td> <td>    0.001</td> <td>  230.204</td> <td> 0.000</td> <td>    0.135</td> <td>    0.137</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>4</th>     <td>    0.0230</td> <td>    0.001</td> <td>   44.744</td> <td> 0.000</td> <td>    0.022</td> <td>    0.024</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>5</th>     <td>   -0.0822</td> <td>    0.001</td> <td> -140.880</td> <td> 0.000</td> <td>   -0.083</td> <td>   -0.081</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>6</th>     <td>    0.0243</td> <td>    0.001</td> <td>   45.646</td> <td> 0.000</td> <td>    0.023</td> <td>    0.025</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>7</th>     <td>    0.0206</td> <td>    0.000</td> <td>   50.378</td> <td> 0.000</td> <td>    0.020</td> <td>    0.021</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>8</th>     <td>    0.0404</td> <td>    0.001</td> <td>   80.674</td> <td> 0.000</td> <td>    0.039</td> <td>    0.041</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>9</th>     <td>    0.0061</td> <td>    0.000</td> <td>   15.117</td> <td> 0.000</td> <td>    0.005</td> <td>    0.007</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>10</th>    <td>   -0.0097</td> <td>    0.001</td> <td>   -6.596</td> <td> 0.000</td> <td>   -0.013</td> <td>   -0.007</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>11</th>    <td>    0.0111</td> <td>    0.002</td> <td>    6.740</td> <td> 0.000</td> <td>    0.008</td> <td>    0.014</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>12</th>    <td>    0.0028</td> <td>    0.002</td> <td>    1.775</td> <td> 0.076</td> <td>   -0.000</td> <td>    0.006</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>13</th>    <td>   -0.0028</td> <td>    0.002</td> <td>   -1.176</td> <td> 0.240</td> <td>   -0.007</td> <td>    0.002</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>14</th>    <td>    0.0039</td> <td>    0.002</td> <td>    2.256</td> <td> 0.024</td> <td>    0.001</td> <td>    0.007</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>15</th>    <td>   -0.0019</td> <td>    0.002</td> <td>   -1.110</td> <td> 0.267</td> <td>   -0.005</td> <td>    0.001</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>16</th>    <td>    0.0049</td> <td>    0.002</td> <td>    2.701</td> <td> 0.007</td> <td>    0.001</td> <td>    0.008</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>17</th>    <td>    0.0035</td> <td>    0.002</td> <td>    2.243</td> <td> 0.025</td> <td>    0.000</td> <td>    0.006</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>18</th>    <td>    0.0052</td> <td>    0.002</td> <td>    2.815</td> <td> 0.005</td> <td>    0.002</td> <td>    0.009</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>19</th>    <td>    0.0044</td> <td>    0.002</td> <td>    2.709</td> <td> 0.007</td> <td>    0.001</td> <td>    0.008</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>20</th>    <td>    0.0102</td> <td>    0.002</td> <td>    5.590</td> <td> 0.000</td> <td>    0.007</td> <td>    0.014</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>21</th>    <td>    0.0079</td> <td>    0.002</td> <td>    4.563</td> <td> 0.000</td> <td>    0.004</td> <td>    0.011</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>22</th>    <td>   -0.0056</td> <td>    0.001</td> <td>   -6.838</td> <td> 0.000</td> <td>   -0.007</td> <td>   -0.004</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>23</th>    <td>    0.3046</td> <td>    0.011</td> <td>   26.940</td> <td> 0.000</td> <td>    0.282</td> <td>    0.327</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>24</th>    <td>    0.2914</td> <td>    0.011</td> <td>   27.184</td> <td> 0.000</td> <td>    0.270</td> <td>    0.312</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>25</th>    <td>    0.3258</td> <td>    0.011</td> <td>   30.493</td> <td> 0.000</td> <td>    0.305</td> <td>    0.347</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>26</th>    <td>    0.2972</td> <td>    0.011</td> <td>   27.047</td> <td> 0.000</td> <td>    0.276</td> <td>    0.319</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>27</th>    <td>    0.0839</td> <td>    0.006</td> <td>   14.758</td> <td> 0.000</td> <td>    0.073</td> <td>    0.095</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>28</th>    <td>    0.0883</td> <td>    0.006</td> <td>   15.554</td> <td> 0.000</td> <td>    0.077</td> <td>    0.099</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>29</th>    <td>    0.0616</td> <td>    0.006</td> <td>   10.937</td> <td> 0.000</td> <td>    0.051</td> <td>    0.073</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>30</th>    <td>    0.0785</td> <td>    0.006</td> <td>   13.675</td> <td> 0.000</td> <td>    0.067</td> <td>    0.090</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>31</th>    <td>    0.0844</td> <td>    0.006</td> <td>   14.971</td> <td> 0.000</td> <td>    0.073</td> <td>    0.095</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>32</th>    <td>    0.1709</td> <td>    0.006</td> <td>   29.388</td> <td> 0.000</td> <td>    0.159</td> <td>    0.182</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>33</th>    <td>    0.0511</td> <td>    0.006</td> <td>    9.050</td> <td> 0.000</td> <td>    0.040</td> <td>    0.062</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>34</th>    <td>    0.0448</td> <td>    0.006</td> <td>    7.926</td> <td> 0.000</td> <td>    0.034</td> <td>    0.056</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>35</th>    <td>    0.0433</td> <td>    0.006</td> <td>    7.688</td> <td> 0.000</td> <td>    0.032</td> <td>    0.054</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>36</th>    <td>    0.1423</td> <td>    0.006</td> <td>   24.759</td> <td> 0.000</td> <td>    0.131</td> <td>    0.154</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>37</th>    <td>    0.0437</td> <td>    0.006</td> <td>    7.730</td> <td> 0.000</td> <td>    0.033</td> <td>    0.055</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>38</th>    <td>    0.0235</td> <td>    0.006</td> <td>    4.193</td> <td> 0.000</td> <td>    0.012</td> <td>    0.034</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>39</th>    <td>    0.1074</td> <td>    0.006</td> <td>   18.434</td> <td> 0.000</td> <td>    0.096</td> <td>    0.119</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>40</th>    <td>    0.0018</td> <td>    0.006</td> <td>    0.321</td> <td> 0.748</td> <td>   -0.009</td> <td>    0.013</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>41</th>    <td>   -0.0012</td> <td>    0.006</td> <td>   -0.205</td> <td> 0.837</td> <td>   -0.012</td> <td>    0.010</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>42</th>    <td>    0.0694</td> <td>    0.006</td> <td>   11.993</td> <td> 0.000</td> <td>    0.058</td> <td>    0.081</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>43</th>    <td>    0.0927</td> <td>    0.006</td> <td>   16.348</td> <td> 0.000</td> <td>    0.082</td> <td>    0.104</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>44</th>    <td>    0.1387</td> <td>    0.007</td> <td>   21.025</td> <td> 0.000</td> <td>    0.126</td> <td>    0.152</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>45</th>    <td>    0.0221</td> <td>    0.006</td> <td>    3.917</td> <td> 0.000</td> <td>    0.011</td> <td>    0.033</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>46</th>    <td>    0.0365</td> <td>    0.006</td> <td>    6.416</td> <td> 0.000</td> <td>    0.025</td> <td>    0.048</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>47</th>    <td>    0.0360</td> <td>    0.006</td> <td>    6.305</td> <td> 0.000</td> <td>    0.025</td> <td>    0.047</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>48</th>    <td>    0.2038</td> <td>    0.006</td> <td>   35.524</td> <td> 0.000</td> <td>    0.193</td> <td>    0.215</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>49</th>    <td>    0.1118</td> <td>    0.006</td> <td>   19.738</td> <td> 0.000</td> <td>    0.101</td> <td>    0.123</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>50</th>    <td>    0.1382</td> <td>    0.006</td> <td>   23.595</td> <td> 0.000</td> <td>    0.127</td> <td>    0.150</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>51</th>    <td>    0.1050</td> <td>    0.006</td> <td>   18.582</td> <td> 0.000</td> <td>    0.094</td> <td>    0.116</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>52</th>    <td>    0.0261</td> <td>    0.006</td> <td>    4.615</td> <td> 0.000</td> <td>    0.015</td> <td>    0.037</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>53</th>    <td>    0.1194</td> <td>    0.006</td> <td>   20.785</td> <td> 0.000</td> <td>    0.108</td> <td>    0.131</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>54</th>    <td>    0.0593</td> <td>    0.006</td> <td>   10.464</td> <td> 0.000</td> <td>    0.048</td> <td>    0.070</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>55</th>    <td>    0.0861</td> <td>    0.006</td> <td>   15.050</td> <td> 0.000</td> <td>    0.075</td> <td>    0.097</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>56</th>    <td>    0.0866</td> <td>    0.006</td> <td>   15.285</td> <td> 0.000</td> <td>    0.075</td> <td>    0.098</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>57</th>    <td>    0.0459</td> <td>    0.006</td> <td>    8.118</td> <td> 0.000</td> <td>    0.035</td> <td>    0.057</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>58</th>    <td>    0.0812</td> <td>    0.006</td> <td>   14.317</td> <td> 0.000</td> <td>    0.070</td> <td>    0.092</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>59</th>    <td>    0.0555</td> <td>    0.006</td> <td>    9.813</td> <td> 0.000</td> <td>    0.044</td> <td>    0.067</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>60</th>    <td>    0.0600</td> <td>    0.006</td> <td>   10.655</td> <td> 0.000</td> <td>    0.049</td> <td>    0.071</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>61</th>    <td>    0.1338</td> <td>    0.006</td> <td>   23.625</td> <td> 0.000</td> <td>    0.123</td> <td>    0.145</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>62</th>    <td>    0.0747</td> <td>    0.006</td> <td>   13.252</td> <td> 0.000</td> <td>    0.064</td> <td>    0.086</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>63</th>    <td>    0.0147</td> <td>    0.006</td> <td>    2.576</td> <td> 0.010</td> <td>    0.004</td> <td>    0.026</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>64</th>    <td>    0.0798</td> <td>    0.006</td> <td>   14.136</td> <td> 0.000</td> <td>    0.069</td> <td>    0.091</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>65</th>    <td>    0.0659</td> <td>    0.006</td> <td>   11.668</td> <td> 0.000</td> <td>    0.055</td> <td>    0.077</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>66</th>    <td>    0.0203</td> <td>    0.006</td> <td>    3.585</td> <td> 0.000</td> <td>    0.009</td> <td>    0.031</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>67</th>    <td>    0.1005</td> <td>    0.006</td> <td>   17.629</td> <td> 0.000</td> <td>    0.089</td> <td>    0.112</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>68</th>    <td>    0.1841</td> <td>    0.006</td> <td>   31.079</td> <td> 0.000</td> <td>    0.173</td> <td>    0.196</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>69</th>    <td>    0.1170</td> <td>    0.006</td> <td>   20.602</td> <td> 0.000</td> <td>    0.106</td> <td>    0.128</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>70</th>    <td>    0.1174</td> <td>    0.006</td> <td>   20.670</td> <td> 0.000</td> <td>    0.106</td> <td>    0.129</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>71</th>    <td>    0.0277</td> <td>    0.006</td> <td>    4.893</td> <td> 0.000</td> <td>    0.017</td> <td>    0.039</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>72</th>    <td>    0.1912</td> <td>    0.006</td> <td>   33.383</td> <td> 0.000</td> <td>    0.180</td> <td>    0.202</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>73</th>    <td>    0.1302</td> <td>    0.006</td> <td>   23.074</td> <td> 0.000</td> <td>    0.119</td> <td>    0.141</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>74</th>    <td>    0.1333</td> <td>    0.006</td> <td>   23.375</td> <td> 0.000</td> <td>    0.122</td> <td>    0.144</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>75</th>    <td>    0.1661</td> <td>    0.006</td> <td>   28.971</td> <td> 0.000</td> <td>    0.155</td> <td>    0.177</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>76</th>    <td>    0.1594</td> <td>    0.006</td> <td>   27.968</td> <td> 0.000</td> <td>    0.148</td> <td>    0.171</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>77</th>    <td>    0.0394</td> <td>    0.006</td> <td>    6.942</td> <td> 0.000</td> <td>    0.028</td> <td>    0.051</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>78</th>    <td>   -0.0025</td> <td>    0.006</td> <td>   -0.447</td> <td> 0.655</td> <td>   -0.014</td> <td>    0.009</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>79</th>    <td>    0.0798</td> <td>    0.006</td> <td>   13.947</td> <td> 0.000</td> <td>    0.069</td> <td>    0.091</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>80</th>    <td>    0.1055</td> <td>    0.006</td> <td>   18.264</td> <td> 0.000</td> <td>    0.094</td> <td>    0.117</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>81</th>    <td>    0.0853</td> <td>    0.006</td> <td>   15.191</td> <td> 0.000</td> <td>    0.074</td> <td>    0.096</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>82</th>    <td>    0.0329</td> <td>    0.006</td> <td>    5.843</td> <td> 0.000</td> <td>    0.022</td> <td>    0.044</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>83</th>    <td>    0.0321</td> <td>    0.006</td> <td>    5.706</td> <td> 0.000</td> <td>    0.021</td> <td>    0.043</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>84</th>    <td>    0.1629</td> <td>    0.006</td> <td>   28.512</td> <td> 0.000</td> <td>    0.152</td> <td>    0.174</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>85</th>    <td>    0.1214</td> <td>    0.006</td> <td>   21.049</td> <td> 0.000</td> <td>    0.110</td> <td>    0.133</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>86</th>    <td>    0.0499</td> <td>    0.006</td> <td>    8.658</td> <td> 0.000</td> <td>    0.039</td> <td>    0.061</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>87</th>    <td>    0.1258</td> <td>    0.006</td> <td>   22.255</td> <td> 0.000</td> <td>    0.115</td> <td>    0.137</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>88</th>    <td>    0.0559</td> <td>    0.006</td> <td>    9.859</td> <td> 0.000</td> <td>    0.045</td> <td>    0.067</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>89</th>    <td>    0.0848</td> <td>    0.006</td> <td>   15.020</td> <td> 0.000</td> <td>    0.074</td> <td>    0.096</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>90</th>    <td>   -0.0159</td> <td>    0.006</td> <td>   -2.812</td> <td> 0.005</td> <td>   -0.027</td> <td>   -0.005</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>91</th>    <td>    0.0734</td> <td>    0.006</td> <td>   12.966</td> <td> 0.000</td> <td>    0.062</td> <td>    0.085</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>92</th>    <td>    0.0195</td> <td>    0.006</td> <td>    3.423</td> <td> 0.001</td> <td>    0.008</td> <td>    0.031</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>93</th>    <td>    0.0515</td> <td>    0.006</td> <td>    9.028</td> <td> 0.000</td> <td>    0.040</td> <td>    0.063</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>94</th>    <td>    0.0580</td> <td>    0.006</td> <td>   10.224</td> <td> 0.000</td> <td>    0.047</td> <td>    0.069</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>95</th>    <td>    0.0633</td> <td>    0.006</td> <td>   11.237</td> <td> 0.000</td> <td>    0.052</td> <td>    0.074</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>96</th>    <td>    0.0142</td> <td>    0.006</td> <td>    2.525</td> <td> 0.012</td> <td>    0.003</td> <td>    0.025</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>97</th>    <td>    0.1375</td> <td>    0.006</td> <td>   24.287</td> <td> 0.000</td> <td>    0.126</td> <td>    0.149</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>98</th>    <td>    0.1771</td> <td>    0.006</td> <td>   31.003</td> <td> 0.000</td> <td>    0.166</td> <td>    0.188</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>99</th>    <td>    0.1051</td> <td>    0.006</td> <td>   17.929</td> <td> 0.000</td> <td>    0.094</td> <td>    0.117</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>100</th>   <td>    0.0575</td> <td>    0.006</td> <td>   10.256</td> <td> 0.000</td> <td>    0.047</td> <td>    0.068</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>101</th>   <td>    0.0463</td> <td>    0.006</td> <td>    8.043</td> <td> 0.000</td> <td>    0.035</td> <td>    0.058</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>102</th>   <td>    0.0423</td> <td>    0.006</td> <td>    7.506</td> <td> 0.000</td> <td>    0.031</td> <td>    0.053</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>103</th>   <td>    0.1247</td> <td>    0.006</td> <td>   21.947</td> <td> 0.000</td> <td>    0.114</td> <td>    0.136</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>104</th>   <td>    0.1250</td> <td>    0.006</td> <td>   21.931</td> <td> 0.000</td> <td>    0.114</td> <td>    0.136</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>105</th>   <td>    0.0945</td> <td>    0.006</td> <td>   16.624</td> <td> 0.000</td> <td>    0.083</td> <td>    0.106</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>106</th>   <td>    0.0338</td> <td>    0.006</td> <td>    6.032</td> <td> 0.000</td> <td>    0.023</td> <td>    0.045</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>107</th>   <td>    0.0822</td> <td>    0.006</td> <td>   14.566</td> <td> 0.000</td> <td>    0.071</td> <td>    0.093</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>108</th>   <td>    0.1127</td> <td>    0.006</td> <td>   19.790</td> <td> 0.000</td> <td>    0.102</td> <td>    0.124</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>109</th>   <td>    0.1335</td> <td>    0.006</td> <td>   23.620</td> <td> 0.000</td> <td>    0.122</td> <td>    0.145</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>110</th>   <td>    0.0391</td> <td>    0.006</td> <td>    6.982</td> <td> 0.000</td> <td>    0.028</td> <td>    0.050</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>111</th>   <td>    0.1133</td> <td>    0.006</td> <td>   19.976</td> <td> 0.000</td> <td>    0.102</td> <td>    0.124</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>112</th>   <td>    0.1138</td> <td>    0.006</td> <td>   20.096</td> <td> 0.000</td> <td>    0.103</td> <td>    0.125</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>113</th>   <td>    0.0528</td> <td>    0.006</td> <td>    9.375</td> <td> 0.000</td> <td>    0.042</td> <td>    0.064</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>114</th>   <td>    0.0487</td> <td>    0.006</td> <td>    8.581</td> <td> 0.000</td> <td>    0.038</td> <td>    0.060</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>115</th>   <td>    0.0527</td> <td>    0.006</td> <td>    9.276</td> <td> 0.000</td> <td>    0.042</td> <td>    0.064</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>116</th>   <td>    0.0526</td> <td>    0.006</td> <td>    9.315</td> <td> 0.000</td> <td>    0.042</td> <td>    0.064</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>117</th>   <td>    0.0390</td> <td>    0.006</td> <td>    6.903</td> <td> 0.000</td> <td>    0.028</td> <td>    0.050</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>118</th>   <td>    0.1001</td> <td>    0.006</td> <td>   17.645</td> <td> 0.000</td> <td>    0.089</td> <td>    0.111</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>119</th>   <td>    0.0109</td> <td>    0.006</td> <td>    1.917</td> <td> 0.055</td> <td>   -0.000</td> <td>    0.022</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>120</th>   <td>    0.0202</td> <td>    0.006</td> <td>    3.593</td> <td> 0.000</td> <td>    0.009</td> <td>    0.031</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>121</th>   <td>    0.1393</td> <td>    0.006</td> <td>   24.717</td> <td> 0.000</td> <td>    0.128</td> <td>    0.150</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>122</th>   <td>    0.1136</td> <td>    0.006</td> <td>   20.070</td> <td> 0.000</td> <td>    0.103</td> <td>    0.125</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>123</th>   <td>    0.0741</td> <td>    0.006</td> <td>   13.083</td> <td> 0.000</td> <td>    0.063</td> <td>    0.085</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>124</th>   <td>   -0.0086</td> <td>    0.006</td> <td>   -1.520</td> <td> 0.128</td> <td>   -0.020</td> <td>    0.002</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>125</th>   <td>    0.0087</td> <td>    0.006</td> <td>    1.562</td> <td> 0.118</td> <td>   -0.002</td> <td>    0.020</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>126</th>   <td>    0.0994</td> <td>    0.006</td> <td>   17.552</td> <td> 0.000</td> <td>    0.088</td> <td>    0.110</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>127</th>   <td>    0.3161</td> <td>    0.006</td> <td>   51.679</td> <td> 0.000</td> <td>    0.304</td> <td>    0.328</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>128</th>   <td>    0.0097</td> <td>    0.006</td> <td>    1.742</td> <td> 0.082</td> <td>   -0.001</td> <td>    0.021</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>129</th>   <td>    0.0097</td> <td>    0.006</td> <td>    1.729</td> <td> 0.084</td> <td>   -0.001</td> <td>    0.021</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>130</th>   <td>    0.0872</td> <td>    0.006</td> <td>   15.460</td> <td> 0.000</td> <td>    0.076</td> <td>    0.098</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>131</th>   <td>   -0.0530</td> <td>    0.006</td> <td>   -9.194</td> <td> 0.000</td> <td>   -0.064</td> <td>   -0.042</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>132</th>   <td>    0.0604</td> <td>    0.006</td> <td>   10.803</td> <td> 0.000</td> <td>    0.049</td> <td>    0.071</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>133</th>   <td>    0.1137</td> <td>    0.006</td> <td>   20.151</td> <td> 0.000</td> <td>    0.103</td> <td>    0.125</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>134</th>   <td>   -0.0277</td> <td>    0.006</td> <td>   -4.879</td> <td> 0.000</td> <td>   -0.039</td> <td>   -0.017</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>135</th>   <td>    0.1354</td> <td>    0.006</td> <td>   23.456</td> <td> 0.000</td> <td>    0.124</td> <td>    0.147</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>136</th>   <td>    0.1624</td> <td>    0.006</td> <td>   28.338</td> <td> 0.000</td> <td>    0.151</td> <td>    0.174</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>137</th>   <td>    0.0505</td> <td>    0.006</td> <td>    8.947</td> <td> 0.000</td> <td>    0.039</td> <td>    0.062</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>138</th>   <td>    0.0633</td> <td>    0.006</td> <td>   11.286</td> <td> 0.000</td> <td>    0.052</td> <td>    0.074</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>139</th>   <td>    0.0240</td> <td>    0.006</td> <td>    4.281</td> <td> 0.000</td> <td>    0.013</td> <td>    0.035</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>4428.716</td> <th>  Durbin-Watson:     </th> <td>   2.002</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>16391.075</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>           <td>-0.050</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>       <td> 5.074</td>  <th>  Cond. No.          </th> <td>    181.</td> \n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/latex": [
              "\\begin{center}\n",
              "\\begin{tabular}{lclc}\n",
              "\\toprule\n",
              "\\textbf{Dep. Variable:}    &      energy      & \\textbf{  R-squared:         } &     0.804   \\\\\n",
              "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.803   \\\\\n",
              "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     2664.   \\\\\n",
              "\\textbf{Date:}             & Wed, 23 Oct 2024 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
              "\\textbf{Time:}             &     20:02:01     & \\textbf{  Log-Likelihood:    } &    70658.   \\\\\n",
              "\\textbf{No. Observations:} &       91200      & \\textbf{  AIC:               } & -1.410e+05  \\\\\n",
              "\\textbf{Df Residuals:}     &       91059      & \\textbf{  BIC:               } & -1.397e+05  \\\\\n",
              "\\textbf{Df Model:}         &         140      & \\textbf{                     } &             \\\\\n",
              "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
              "\\bottomrule\n",
              "\\end{tabular}\n",
              "\\begin{tabular}{lcccccc}\n",
              "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
              "\\midrule\n",
              "\\textbf{const} &       0.2416  &        0.011     &    21.055  &         0.000        &        0.219    &        0.264     \\\\\n",
              "\\textbf{0}     &      -0.0033  &        0.000     &    -7.714  &         0.000        &       -0.004    &       -0.002     \\\\\n",
              "\\textbf{1}     &       0.0011  &        0.000     &     2.657  &         0.008        &        0.000    &        0.002     \\\\\n",
              "\\textbf{2}     &      -0.0218  &        0.001     &   -39.325  &         0.000        &       -0.023    &       -0.021     \\\\\n",
              "\\textbf{3}     &       0.1357  &        0.001     &   230.204  &         0.000        &        0.135    &        0.137     \\\\\n",
              "\\textbf{4}     &       0.0230  &        0.001     &    44.744  &         0.000        &        0.022    &        0.024     \\\\\n",
              "\\textbf{5}     &      -0.0822  &        0.001     &  -140.880  &         0.000        &       -0.083    &       -0.081     \\\\\n",
              "\\textbf{6}     &       0.0243  &        0.001     &    45.646  &         0.000        &        0.023    &        0.025     \\\\\n",
              "\\textbf{7}     &       0.0206  &        0.000     &    50.378  &         0.000        &        0.020    &        0.021     \\\\\n",
              "\\textbf{8}     &       0.0404  &        0.001     &    80.674  &         0.000        &        0.039    &        0.041     \\\\\n",
              "\\textbf{9}     &       0.0061  &        0.000     &    15.117  &         0.000        &        0.005    &        0.007     \\\\\n",
              "\\textbf{10}    &      -0.0097  &        0.001     &    -6.596  &         0.000        &       -0.013    &       -0.007     \\\\\n",
              "\\textbf{11}    &       0.0111  &        0.002     &     6.740  &         0.000        &        0.008    &        0.014     \\\\\n",
              "\\textbf{12}    &       0.0028  &        0.002     &     1.775  &         0.076        &       -0.000    &        0.006     \\\\\n",
              "\\textbf{13}    &      -0.0028  &        0.002     &    -1.176  &         0.240        &       -0.007    &        0.002     \\\\\n",
              "\\textbf{14}    &       0.0039  &        0.002     &     2.256  &         0.024        &        0.001    &        0.007     \\\\\n",
              "\\textbf{15}    &      -0.0019  &        0.002     &    -1.110  &         0.267        &       -0.005    &        0.001     \\\\\n",
              "\\textbf{16}    &       0.0049  &        0.002     &     2.701  &         0.007        &        0.001    &        0.008     \\\\\n",
              "\\textbf{17}    &       0.0035  &        0.002     &     2.243  &         0.025        &        0.000    &        0.006     \\\\\n",
              "\\textbf{18}    &       0.0052  &        0.002     &     2.815  &         0.005        &        0.002    &        0.009     \\\\\n",
              "\\textbf{19}    &       0.0044  &        0.002     &     2.709  &         0.007        &        0.001    &        0.008     \\\\\n",
              "\\textbf{20}    &       0.0102  &        0.002     &     5.590  &         0.000        &        0.007    &        0.014     \\\\\n",
              "\\textbf{21}    &       0.0079  &        0.002     &     4.563  &         0.000        &        0.004    &        0.011     \\\\\n",
              "\\textbf{22}    &      -0.0056  &        0.001     &    -6.838  &         0.000        &       -0.007    &       -0.004     \\\\\n",
              "\\textbf{23}    &       0.3046  &        0.011     &    26.940  &         0.000        &        0.282    &        0.327     \\\\\n",
              "\\textbf{24}    &       0.2914  &        0.011     &    27.184  &         0.000        &        0.270    &        0.312     \\\\\n",
              "\\textbf{25}    &       0.3258  &        0.011     &    30.493  &         0.000        &        0.305    &        0.347     \\\\\n",
              "\\textbf{26}    &       0.2972  &        0.011     &    27.047  &         0.000        &        0.276    &        0.319     \\\\\n",
              "\\textbf{27}    &       0.0839  &        0.006     &    14.758  &         0.000        &        0.073    &        0.095     \\\\\n",
              "\\textbf{28}    &       0.0883  &        0.006     &    15.554  &         0.000        &        0.077    &        0.099     \\\\\n",
              "\\textbf{29}    &       0.0616  &        0.006     &    10.937  &         0.000        &        0.051    &        0.073     \\\\\n",
              "\\textbf{30}    &       0.0785  &        0.006     &    13.675  &         0.000        &        0.067    &        0.090     \\\\\n",
              "\\textbf{31}    &       0.0844  &        0.006     &    14.971  &         0.000        &        0.073    &        0.095     \\\\\n",
              "\\textbf{32}    &       0.1709  &        0.006     &    29.388  &         0.000        &        0.159    &        0.182     \\\\\n",
              "\\textbf{33}    &       0.0511  &        0.006     &     9.050  &         0.000        &        0.040    &        0.062     \\\\\n",
              "\\textbf{34}    &       0.0448  &        0.006     &     7.926  &         0.000        &        0.034    &        0.056     \\\\\n",
              "\\textbf{35}    &       0.0433  &        0.006     &     7.688  &         0.000        &        0.032    &        0.054     \\\\\n",
              "\\textbf{36}    &       0.1423  &        0.006     &    24.759  &         0.000        &        0.131    &        0.154     \\\\\n",
              "\\textbf{37}    &       0.0437  &        0.006     &     7.730  &         0.000        &        0.033    &        0.055     \\\\\n",
              "\\textbf{38}    &       0.0235  &        0.006     &     4.193  &         0.000        &        0.012    &        0.034     \\\\\n",
              "\\textbf{39}    &       0.1074  &        0.006     &    18.434  &         0.000        &        0.096    &        0.119     \\\\\n",
              "\\textbf{40}    &       0.0018  &        0.006     &     0.321  &         0.748        &       -0.009    &        0.013     \\\\\n",
              "\\textbf{41}    &      -0.0012  &        0.006     &    -0.205  &         0.837        &       -0.012    &        0.010     \\\\\n",
              "\\textbf{42}    &       0.0694  &        0.006     &    11.993  &         0.000        &        0.058    &        0.081     \\\\\n",
              "\\textbf{43}    &       0.0927  &        0.006     &    16.348  &         0.000        &        0.082    &        0.104     \\\\\n",
              "\\textbf{44}    &       0.1387  &        0.007     &    21.025  &         0.000        &        0.126    &        0.152     \\\\\n",
              "\\textbf{45}    &       0.0221  &        0.006     &     3.917  &         0.000        &        0.011    &        0.033     \\\\\n",
              "\\textbf{46}    &       0.0365  &        0.006     &     6.416  &         0.000        &        0.025    &        0.048     \\\\\n",
              "\\textbf{47}    &       0.0360  &        0.006     &     6.305  &         0.000        &        0.025    &        0.047     \\\\\n",
              "\\textbf{48}    &       0.2038  &        0.006     &    35.524  &         0.000        &        0.193    &        0.215     \\\\\n",
              "\\textbf{49}    &       0.1118  &        0.006     &    19.738  &         0.000        &        0.101    &        0.123     \\\\\n",
              "\\textbf{50}    &       0.1382  &        0.006     &    23.595  &         0.000        &        0.127    &        0.150     \\\\\n",
              "\\textbf{51}    &       0.1050  &        0.006     &    18.582  &         0.000        &        0.094    &        0.116     \\\\\n",
              "\\textbf{52}    &       0.0261  &        0.006     &     4.615  &         0.000        &        0.015    &        0.037     \\\\\n",
              "\\textbf{53}    &       0.1194  &        0.006     &    20.785  &         0.000        &        0.108    &        0.131     \\\\\n",
              "\\textbf{54}    &       0.0593  &        0.006     &    10.464  &         0.000        &        0.048    &        0.070     \\\\\n",
              "\\textbf{55}    &       0.0861  &        0.006     &    15.050  &         0.000        &        0.075    &        0.097     \\\\\n",
              "\\textbf{56}    &       0.0866  &        0.006     &    15.285  &         0.000        &        0.075    &        0.098     \\\\\n",
              "\\textbf{57}    &       0.0459  &        0.006     &     8.118  &         0.000        &        0.035    &        0.057     \\\\\n",
              "\\textbf{58}    &       0.0812  &        0.006     &    14.317  &         0.000        &        0.070    &        0.092     \\\\\n",
              "\\textbf{59}    &       0.0555  &        0.006     &     9.813  &         0.000        &        0.044    &        0.067     \\\\\n",
              "\\textbf{60}    &       0.0600  &        0.006     &    10.655  &         0.000        &        0.049    &        0.071     \\\\\n",
              "\\textbf{61}    &       0.1338  &        0.006     &    23.625  &         0.000        &        0.123    &        0.145     \\\\\n",
              "\\textbf{62}    &       0.0747  &        0.006     &    13.252  &         0.000        &        0.064    &        0.086     \\\\\n",
              "\\textbf{63}    &       0.0147  &        0.006     &     2.576  &         0.010        &        0.004    &        0.026     \\\\\n",
              "\\textbf{64}    &       0.0798  &        0.006     &    14.136  &         0.000        &        0.069    &        0.091     \\\\\n",
              "\\textbf{65}    &       0.0659  &        0.006     &    11.668  &         0.000        &        0.055    &        0.077     \\\\\n",
              "\\textbf{66}    &       0.0203  &        0.006     &     3.585  &         0.000        &        0.009    &        0.031     \\\\\n",
              "\\textbf{67}    &       0.1005  &        0.006     &    17.629  &         0.000        &        0.089    &        0.112     \\\\\n",
              "\\textbf{68}    &       0.1841  &        0.006     &    31.079  &         0.000        &        0.173    &        0.196     \\\\\n",
              "\\textbf{69}    &       0.1170  &        0.006     &    20.602  &         0.000        &        0.106    &        0.128     \\\\\n",
              "\\textbf{70}    &       0.1174  &        0.006     &    20.670  &         0.000        &        0.106    &        0.129     \\\\\n",
              "\\textbf{71}    &       0.0277  &        0.006     &     4.893  &         0.000        &        0.017    &        0.039     \\\\\n",
              "\\textbf{72}    &       0.1912  &        0.006     &    33.383  &         0.000        &        0.180    &        0.202     \\\\\n",
              "\\textbf{73}    &       0.1302  &        0.006     &    23.074  &         0.000        &        0.119    &        0.141     \\\\\n",
              "\\textbf{74}    &       0.1333  &        0.006     &    23.375  &         0.000        &        0.122    &        0.144     \\\\\n",
              "\\textbf{75}    &       0.1661  &        0.006     &    28.971  &         0.000        &        0.155    &        0.177     \\\\\n",
              "\\textbf{76}    &       0.1594  &        0.006     &    27.968  &         0.000        &        0.148    &        0.171     \\\\\n",
              "\\textbf{77}    &       0.0394  &        0.006     &     6.942  &         0.000        &        0.028    &        0.051     \\\\\n",
              "\\textbf{78}    &      -0.0025  &        0.006     &    -0.447  &         0.655        &       -0.014    &        0.009     \\\\\n",
              "\\textbf{79}    &       0.0798  &        0.006     &    13.947  &         0.000        &        0.069    &        0.091     \\\\\n",
              "\\textbf{80}    &       0.1055  &        0.006     &    18.264  &         0.000        &        0.094    &        0.117     \\\\\n",
              "\\textbf{81}    &       0.0853  &        0.006     &    15.191  &         0.000        &        0.074    &        0.096     \\\\\n",
              "\\textbf{82}    &       0.0329  &        0.006     &     5.843  &         0.000        &        0.022    &        0.044     \\\\\n",
              "\\textbf{83}    &       0.0321  &        0.006     &     5.706  &         0.000        &        0.021    &        0.043     \\\\\n",
              "\\textbf{84}    &       0.1629  &        0.006     &    28.512  &         0.000        &        0.152    &        0.174     \\\\\n",
              "\\textbf{85}    &       0.1214  &        0.006     &    21.049  &         0.000        &        0.110    &        0.133     \\\\\n",
              "\\textbf{86}    &       0.0499  &        0.006     &     8.658  &         0.000        &        0.039    &        0.061     \\\\\n",
              "\\textbf{87}    &       0.1258  &        0.006     &    22.255  &         0.000        &        0.115    &        0.137     \\\\\n",
              "\\textbf{88}    &       0.0559  &        0.006     &     9.859  &         0.000        &        0.045    &        0.067     \\\\\n",
              "\\textbf{89}    &       0.0848  &        0.006     &    15.020  &         0.000        &        0.074    &        0.096     \\\\\n",
              "\\textbf{90}    &      -0.0159  &        0.006     &    -2.812  &         0.005        &       -0.027    &       -0.005     \\\\\n",
              "\\textbf{91}    &       0.0734  &        0.006     &    12.966  &         0.000        &        0.062    &        0.085     \\\\\n",
              "\\textbf{92}    &       0.0195  &        0.006     &     3.423  &         0.001        &        0.008    &        0.031     \\\\\n",
              "\\textbf{93}    &       0.0515  &        0.006     &     9.028  &         0.000        &        0.040    &        0.063     \\\\\n",
              "\\textbf{94}    &       0.0580  &        0.006     &    10.224  &         0.000        &        0.047    &        0.069     \\\\\n",
              "\\textbf{95}    &       0.0633  &        0.006     &    11.237  &         0.000        &        0.052    &        0.074     \\\\\n",
              "\\textbf{96}    &       0.0142  &        0.006     &     2.525  &         0.012        &        0.003    &        0.025     \\\\\n",
              "\\textbf{97}    &       0.1375  &        0.006     &    24.287  &         0.000        &        0.126    &        0.149     \\\\\n",
              "\\textbf{98}    &       0.1771  &        0.006     &    31.003  &         0.000        &        0.166    &        0.188     \\\\\n",
              "\\textbf{99}    &       0.1051  &        0.006     &    17.929  &         0.000        &        0.094    &        0.117     \\\\\n",
              "\\textbf{100}   &       0.0575  &        0.006     &    10.256  &         0.000        &        0.047    &        0.068     \\\\\n",
              "\\textbf{101}   &       0.0463  &        0.006     &     8.043  &         0.000        &        0.035    &        0.058     \\\\\n",
              "\\textbf{102}   &       0.0423  &        0.006     &     7.506  &         0.000        &        0.031    &        0.053     \\\\\n",
              "\\textbf{103}   &       0.1247  &        0.006     &    21.947  &         0.000        &        0.114    &        0.136     \\\\\n",
              "\\textbf{104}   &       0.1250  &        0.006     &    21.931  &         0.000        &        0.114    &        0.136     \\\\\n",
              "\\textbf{105}   &       0.0945  &        0.006     &    16.624  &         0.000        &        0.083    &        0.106     \\\\\n",
              "\\textbf{106}   &       0.0338  &        0.006     &     6.032  &         0.000        &        0.023    &        0.045     \\\\\n",
              "\\textbf{107}   &       0.0822  &        0.006     &    14.566  &         0.000        &        0.071    &        0.093     \\\\\n",
              "\\textbf{108}   &       0.1127  &        0.006     &    19.790  &         0.000        &        0.102    &        0.124     \\\\\n",
              "\\textbf{109}   &       0.1335  &        0.006     &    23.620  &         0.000        &        0.122    &        0.145     \\\\\n",
              "\\textbf{110}   &       0.0391  &        0.006     &     6.982  &         0.000        &        0.028    &        0.050     \\\\\n",
              "\\textbf{111}   &       0.1133  &        0.006     &    19.976  &         0.000        &        0.102    &        0.124     \\\\\n",
              "\\textbf{112}   &       0.1138  &        0.006     &    20.096  &         0.000        &        0.103    &        0.125     \\\\\n",
              "\\textbf{113}   &       0.0528  &        0.006     &     9.375  &         0.000        &        0.042    &        0.064     \\\\\n",
              "\\textbf{114}   &       0.0487  &        0.006     &     8.581  &         0.000        &        0.038    &        0.060     \\\\\n",
              "\\textbf{115}   &       0.0527  &        0.006     &     9.276  &         0.000        &        0.042    &        0.064     \\\\\n",
              "\\textbf{116}   &       0.0526  &        0.006     &     9.315  &         0.000        &        0.042    &        0.064     \\\\\n",
              "\\textbf{117}   &       0.0390  &        0.006     &     6.903  &         0.000        &        0.028    &        0.050     \\\\\n",
              "\\textbf{118}   &       0.1001  &        0.006     &    17.645  &         0.000        &        0.089    &        0.111     \\\\\n",
              "\\textbf{119}   &       0.0109  &        0.006     &     1.917  &         0.055        &       -0.000    &        0.022     \\\\\n",
              "\\textbf{120}   &       0.0202  &        0.006     &     3.593  &         0.000        &        0.009    &        0.031     \\\\\n",
              "\\textbf{121}   &       0.1393  &        0.006     &    24.717  &         0.000        &        0.128    &        0.150     \\\\\n",
              "\\textbf{122}   &       0.1136  &        0.006     &    20.070  &         0.000        &        0.103    &        0.125     \\\\\n",
              "\\textbf{123}   &       0.0741  &        0.006     &    13.083  &         0.000        &        0.063    &        0.085     \\\\\n",
              "\\textbf{124}   &      -0.0086  &        0.006     &    -1.520  &         0.128        &       -0.020    &        0.002     \\\\\n",
              "\\textbf{125}   &       0.0087  &        0.006     &     1.562  &         0.118        &       -0.002    &        0.020     \\\\\n",
              "\\textbf{126}   &       0.0994  &        0.006     &    17.552  &         0.000        &        0.088    &        0.110     \\\\\n",
              "\\textbf{127}   &       0.3161  &        0.006     &    51.679  &         0.000        &        0.304    &        0.328     \\\\\n",
              "\\textbf{128}   &       0.0097  &        0.006     &     1.742  &         0.082        &       -0.001    &        0.021     \\\\\n",
              "\\textbf{129}   &       0.0097  &        0.006     &     1.729  &         0.084        &       -0.001    &        0.021     \\\\\n",
              "\\textbf{130}   &       0.0872  &        0.006     &    15.460  &         0.000        &        0.076    &        0.098     \\\\\n",
              "\\textbf{131}   &      -0.0530  &        0.006     &    -9.194  &         0.000        &       -0.064    &       -0.042     \\\\\n",
              "\\textbf{132}   &       0.0604  &        0.006     &    10.803  &         0.000        &        0.049    &        0.071     \\\\\n",
              "\\textbf{133}   &       0.1137  &        0.006     &    20.151  &         0.000        &        0.103    &        0.125     \\\\\n",
              "\\textbf{134}   &      -0.0277  &        0.006     &    -4.879  &         0.000        &       -0.039    &       -0.017     \\\\\n",
              "\\textbf{135}   &       0.1354  &        0.006     &    23.456  &         0.000        &        0.124    &        0.147     \\\\\n",
              "\\textbf{136}   &       0.1624  &        0.006     &    28.338  &         0.000        &        0.151    &        0.174     \\\\\n",
              "\\textbf{137}   &       0.0505  &        0.006     &     8.947  &         0.000        &        0.039    &        0.062     \\\\\n",
              "\\textbf{138}   &       0.0633  &        0.006     &    11.286  &         0.000        &        0.052    &        0.074     \\\\\n",
              "\\textbf{139}   &       0.0240  &        0.006     &     4.281  &         0.000        &        0.013    &        0.035     \\\\\n",
              "\\bottomrule\n",
              "\\end{tabular}\n",
              "\\begin{tabular}{lclc}\n",
              "\\textbf{Omnibus:}       & 4428.716 & \\textbf{  Durbin-Watson:     } &     2.002  \\\\\n",
              "\\textbf{Prob(Omnibus):} &   0.000  & \\textbf{  Jarque-Bera (JB):  } & 16391.075  \\\\\n",
              "\\textbf{Skew:}          &  -0.050  & \\textbf{  Prob(JB):          } &      0.00  \\\\\n",
              "\\textbf{Kurtosis:}      &   5.074  & \\textbf{  Cond. No.          } &      181.  \\\\\n",
              "\\bottomrule\n",
              "\\end{tabular}\n",
              "%\\caption{OLS Regression Results}\n",
              "\\end{center}\n",
              "\n",
              "Notes: \\newline\n",
              " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:                 energy   R-squared:                       0.804\n",
              "Model:                            OLS   Adj. R-squared:                  0.803\n",
              "Method:                 Least Squares   F-statistic:                     2664.\n",
              "Date:                Wed, 23 Oct 2024   Prob (F-statistic):               0.00\n",
              "Time:                        20:02:01   Log-Likelihood:                 70658.\n",
              "No. Observations:               91200   AIC:                        -1.410e+05\n",
              "Df Residuals:                   91059   BIC:                        -1.397e+05\n",
              "Df Model:                         140                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "const          0.2416      0.011     21.055      0.000       0.219       0.264\n",
              "0             -0.0033      0.000     -7.714      0.000      -0.004      -0.002\n",
              "1              0.0011      0.000      2.657      0.008       0.000       0.002\n",
              "2             -0.0218      0.001    -39.325      0.000      -0.023      -0.021\n",
              "3              0.1357      0.001    230.204      0.000       0.135       0.137\n",
              "4              0.0230      0.001     44.744      0.000       0.022       0.024\n",
              "5             -0.0822      0.001   -140.880      0.000      -0.083      -0.081\n",
              "6              0.0243      0.001     45.646      0.000       0.023       0.025\n",
              "7              0.0206      0.000     50.378      0.000       0.020       0.021\n",
              "8              0.0404      0.001     80.674      0.000       0.039       0.041\n",
              "9              0.0061      0.000     15.117      0.000       0.005       0.007\n",
              "10            -0.0097      0.001     -6.596      0.000      -0.013      -0.007\n",
              "11             0.0111      0.002      6.740      0.000       0.008       0.014\n",
              "12             0.0028      0.002      1.775      0.076      -0.000       0.006\n",
              "13            -0.0028      0.002     -1.176      0.240      -0.007       0.002\n",
              "14             0.0039      0.002      2.256      0.024       0.001       0.007\n",
              "15            -0.0019      0.002     -1.110      0.267      -0.005       0.001\n",
              "16             0.0049      0.002      2.701      0.007       0.001       0.008\n",
              "17             0.0035      0.002      2.243      0.025       0.000       0.006\n",
              "18             0.0052      0.002      2.815      0.005       0.002       0.009\n",
              "19             0.0044      0.002      2.709      0.007       0.001       0.008\n",
              "20             0.0102      0.002      5.590      0.000       0.007       0.014\n",
              "21             0.0079      0.002      4.563      0.000       0.004       0.011\n",
              "22            -0.0056      0.001     -6.838      0.000      -0.007      -0.004\n",
              "23             0.3046      0.011     26.940      0.000       0.282       0.327\n",
              "24             0.2914      0.011     27.184      0.000       0.270       0.312\n",
              "25             0.3258      0.011     30.493      0.000       0.305       0.347\n",
              "26             0.2972      0.011     27.047      0.000       0.276       0.319\n",
              "27             0.0839      0.006     14.758      0.000       0.073       0.095\n",
              "28             0.0883      0.006     15.554      0.000       0.077       0.099\n",
              "29             0.0616      0.006     10.937      0.000       0.051       0.073\n",
              "30             0.0785      0.006     13.675      0.000       0.067       0.090\n",
              "31             0.0844      0.006     14.971      0.000       0.073       0.095\n",
              "32             0.1709      0.006     29.388      0.000       0.159       0.182\n",
              "33             0.0511      0.006      9.050      0.000       0.040       0.062\n",
              "34             0.0448      0.006      7.926      0.000       0.034       0.056\n",
              "35             0.0433      0.006      7.688      0.000       0.032       0.054\n",
              "36             0.1423      0.006     24.759      0.000       0.131       0.154\n",
              "37             0.0437      0.006      7.730      0.000       0.033       0.055\n",
              "38             0.0235      0.006      4.193      0.000       0.012       0.034\n",
              "39             0.1074      0.006     18.434      0.000       0.096       0.119\n",
              "40             0.0018      0.006      0.321      0.748      -0.009       0.013\n",
              "41            -0.0012      0.006     -0.205      0.837      -0.012       0.010\n",
              "42             0.0694      0.006     11.993      0.000       0.058       0.081\n",
              "43             0.0927      0.006     16.348      0.000       0.082       0.104\n",
              "44             0.1387      0.007     21.025      0.000       0.126       0.152\n",
              "45             0.0221      0.006      3.917      0.000       0.011       0.033\n",
              "46             0.0365      0.006      6.416      0.000       0.025       0.048\n",
              "47             0.0360      0.006      6.305      0.000       0.025       0.047\n",
              "48             0.2038      0.006     35.524      0.000       0.193       0.215\n",
              "49             0.1118      0.006     19.738      0.000       0.101       0.123\n",
              "50             0.1382      0.006     23.595      0.000       0.127       0.150\n",
              "51             0.1050      0.006     18.582      0.000       0.094       0.116\n",
              "52             0.0261      0.006      4.615      0.000       0.015       0.037\n",
              "53             0.1194      0.006     20.785      0.000       0.108       0.131\n",
              "54             0.0593      0.006     10.464      0.000       0.048       0.070\n",
              "55             0.0861      0.006     15.050      0.000       0.075       0.097\n",
              "56             0.0866      0.006     15.285      0.000       0.075       0.098\n",
              "57             0.0459      0.006      8.118      0.000       0.035       0.057\n",
              "58             0.0812      0.006     14.317      0.000       0.070       0.092\n",
              "59             0.0555      0.006      9.813      0.000       0.044       0.067\n",
              "60             0.0600      0.006     10.655      0.000       0.049       0.071\n",
              "61             0.1338      0.006     23.625      0.000       0.123       0.145\n",
              "62             0.0747      0.006     13.252      0.000       0.064       0.086\n",
              "63             0.0147      0.006      2.576      0.010       0.004       0.026\n",
              "64             0.0798      0.006     14.136      0.000       0.069       0.091\n",
              "65             0.0659      0.006     11.668      0.000       0.055       0.077\n",
              "66             0.0203      0.006      3.585      0.000       0.009       0.031\n",
              "67             0.1005      0.006     17.629      0.000       0.089       0.112\n",
              "68             0.1841      0.006     31.079      0.000       0.173       0.196\n",
              "69             0.1170      0.006     20.602      0.000       0.106       0.128\n",
              "70             0.1174      0.006     20.670      0.000       0.106       0.129\n",
              "71             0.0277      0.006      4.893      0.000       0.017       0.039\n",
              "72             0.1912      0.006     33.383      0.000       0.180       0.202\n",
              "73             0.1302      0.006     23.074      0.000       0.119       0.141\n",
              "74             0.1333      0.006     23.375      0.000       0.122       0.144\n",
              "75             0.1661      0.006     28.971      0.000       0.155       0.177\n",
              "76             0.1594      0.006     27.968      0.000       0.148       0.171\n",
              "77             0.0394      0.006      6.942      0.000       0.028       0.051\n",
              "78            -0.0025      0.006     -0.447      0.655      -0.014       0.009\n",
              "79             0.0798      0.006     13.947      0.000       0.069       0.091\n",
              "80             0.1055      0.006     18.264      0.000       0.094       0.117\n",
              "81             0.0853      0.006     15.191      0.000       0.074       0.096\n",
              "82             0.0329      0.006      5.843      0.000       0.022       0.044\n",
              "83             0.0321      0.006      5.706      0.000       0.021       0.043\n",
              "84             0.1629      0.006     28.512      0.000       0.152       0.174\n",
              "85             0.1214      0.006     21.049      0.000       0.110       0.133\n",
              "86             0.0499      0.006      8.658      0.000       0.039       0.061\n",
              "87             0.1258      0.006     22.255      0.000       0.115       0.137\n",
              "88             0.0559      0.006      9.859      0.000       0.045       0.067\n",
              "89             0.0848      0.006     15.020      0.000       0.074       0.096\n",
              "90            -0.0159      0.006     -2.812      0.005      -0.027      -0.005\n",
              "91             0.0734      0.006     12.966      0.000       0.062       0.085\n",
              "92             0.0195      0.006      3.423      0.001       0.008       0.031\n",
              "93             0.0515      0.006      9.028      0.000       0.040       0.063\n",
              "94             0.0580      0.006     10.224      0.000       0.047       0.069\n",
              "95             0.0633      0.006     11.237      0.000       0.052       0.074\n",
              "96             0.0142      0.006      2.525      0.012       0.003       0.025\n",
              "97             0.1375      0.006     24.287      0.000       0.126       0.149\n",
              "98             0.1771      0.006     31.003      0.000       0.166       0.188\n",
              "99             0.1051      0.006     17.929      0.000       0.094       0.117\n",
              "100            0.0575      0.006     10.256      0.000       0.047       0.068\n",
              "101            0.0463      0.006      8.043      0.000       0.035       0.058\n",
              "102            0.0423      0.006      7.506      0.000       0.031       0.053\n",
              "103            0.1247      0.006     21.947      0.000       0.114       0.136\n",
              "104            0.1250      0.006     21.931      0.000       0.114       0.136\n",
              "105            0.0945      0.006     16.624      0.000       0.083       0.106\n",
              "106            0.0338      0.006      6.032      0.000       0.023       0.045\n",
              "107            0.0822      0.006     14.566      0.000       0.071       0.093\n",
              "108            0.1127      0.006     19.790      0.000       0.102       0.124\n",
              "109            0.1335      0.006     23.620      0.000       0.122       0.145\n",
              "110            0.0391      0.006      6.982      0.000       0.028       0.050\n",
              "111            0.1133      0.006     19.976      0.000       0.102       0.124\n",
              "112            0.1138      0.006     20.096      0.000       0.103       0.125\n",
              "113            0.0528      0.006      9.375      0.000       0.042       0.064\n",
              "114            0.0487      0.006      8.581      0.000       0.038       0.060\n",
              "115            0.0527      0.006      9.276      0.000       0.042       0.064\n",
              "116            0.0526      0.006      9.315      0.000       0.042       0.064\n",
              "117            0.0390      0.006      6.903      0.000       0.028       0.050\n",
              "118            0.1001      0.006     17.645      0.000       0.089       0.111\n",
              "119            0.0109      0.006      1.917      0.055      -0.000       0.022\n",
              "120            0.0202      0.006      3.593      0.000       0.009       0.031\n",
              "121            0.1393      0.006     24.717      0.000       0.128       0.150\n",
              "122            0.1136      0.006     20.070      0.000       0.103       0.125\n",
              "123            0.0741      0.006     13.083      0.000       0.063       0.085\n",
              "124           -0.0086      0.006     -1.520      0.128      -0.020       0.002\n",
              "125            0.0087      0.006      1.562      0.118      -0.002       0.020\n",
              "126            0.0994      0.006     17.552      0.000       0.088       0.110\n",
              "127            0.3161      0.006     51.679      0.000       0.304       0.328\n",
              "128            0.0097      0.006      1.742      0.082      -0.001       0.021\n",
              "129            0.0097      0.006      1.729      0.084      -0.001       0.021\n",
              "130            0.0872      0.006     15.460      0.000       0.076       0.098\n",
              "131           -0.0530      0.006     -9.194      0.000      -0.064      -0.042\n",
              "132            0.0604      0.006     10.803      0.000       0.049       0.071\n",
              "133            0.1137      0.006     20.151      0.000       0.103       0.125\n",
              "134           -0.0277      0.006     -4.879      0.000      -0.039      -0.017\n",
              "135            0.1354      0.006     23.456      0.000       0.124       0.147\n",
              "136            0.1624      0.006     28.338      0.000       0.151       0.174\n",
              "137            0.0505      0.006      8.947      0.000       0.039       0.062\n",
              "138            0.0633      0.006     11.286      0.000       0.052       0.074\n",
              "139            0.0240      0.006      4.281      0.000       0.013       0.035\n",
              "==============================================================================\n",
              "Omnibus:                     4428.716   Durbin-Watson:                   2.002\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            16391.075\n",
              "Skew:                          -0.050   Prob(JB):                         0.00\n",
              "Kurtosis:                       5.074   Cond. No.                         181.\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_sm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It doesn't zero out any parameters, however it's not surprising because it isn't using regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.        ,  0.        , -0.        ,  0.09558628,  0.        ,\n",
              "       -0.07828713,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
              "       -0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
              "        0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
              "       -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
              "       -0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
              "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
              "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
              "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
              "       -0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
              "       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
              "        0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
              "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
              "       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
              "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
              "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
              "       -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
              "        0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
              "       -0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
              "       -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
              "        0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
              "        0.        ,  0.        , -0.        , -0.        , -0.        ])"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lasso_model.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, if we now look at lasso model, here it zeroed out every single weight except two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.00331484,  0.00108738, -0.02175282,  0.13565726,  0.02297808,\n",
              "        -0.08215616,  0.02431216,  0.020624  ,  0.0404424 ,  0.00606037,\n",
              "        -0.00973059,  0.01107892,  0.00283289, -0.00277682,  0.00391098,\n",
              "        -0.00189005,  0.00485391,  0.00346283,  0.0051597 ,  0.00437175,\n",
              "         0.0102324 ,  0.0078721 , -0.00563272,  0.30422021,  0.29110713,\n",
              "         0.32549687,  0.29688151,  0.08356111,  0.08799837,  0.06123254,\n",
              "         0.07818433,  0.08406089,  0.17052406,  0.05074075,  0.04443394,\n",
              "         0.04291396,  0.14194216,  0.04330844,  0.02312568,  0.10707253,\n",
              "         0.00147943, -0.0014946 ,  0.0690761 ,  0.09235011,  0.13837182,\n",
              "         0.0218029 ,  0.03612693,  0.03567114,  0.20348552,  0.11148549,\n",
              "         0.1378512 ,  0.1046516 ,  0.02579008,  0.11900132,  0.0589406 ,\n",
              "         0.08573355,  0.08621796,  0.04556559,  0.08080069,  0.05515466,\n",
              "         0.0596831 ,  0.13350283,  0.07435159,  0.01434103,  0.07948159,\n",
              "         0.06552226,  0.01994004,  0.10017219,  0.18375314,  0.1166874 ,\n",
              "         0.11702522,  0.02733091,  0.19086647,  0.1298244 ,  0.13296283,\n",
              "         0.16573031,  0.1590625 ,  0.03909606, -0.0028652 ,  0.07942306,\n",
              "         0.10513833,  0.08495501,  0.0325837 ,  0.03177951,  0.16251871,\n",
              "         0.12103221,  0.04953099,  0.12549302,  0.05554129,  0.08440771,\n",
              "        -0.01627738,  0.07308535,  0.01915623,  0.05117339,  0.05769077,\n",
              "         0.06298855,  0.0138511 ,  0.13716756,  0.17676569,  0.10473262,\n",
              "         0.05715473,  0.04591569,  0.04198886,  0.12432479,  0.12461487,\n",
              "         0.09414949,  0.03345766,  0.08189861,  0.11238199,  0.13312562,\n",
              "         0.03879113,  0.11293981,  0.11349862,  0.05249432,  0.04839181,\n",
              "         0.05238626,  0.0522636 ,  0.03861728,  0.09978879,  0.0105853 ,\n",
              "         0.01985301,  0.13892429,  0.11326778,  0.07378051, -0.00891076,\n",
              "         0.00837671,  0.09901103,  0.31570923,  0.00937523,  0.00938396,\n",
              "         0.08680506, -0.05336078,  0.06001401,  0.11332044, -0.02801939,\n",
              "         0.13502456,  0.16205687,  0.05013279,  0.06298792,  0.02368907]])"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ridge_model.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And ridge doesn't zero out any weights, which is also not surprising because of the way L2 regularization works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLcvGlUZy-Qt"
      },
      "source": [
        "#### 5. [1 point] Implement one of the feature selection algorithms (Elimination by P-value, Forward elimination, Backward elimination), draw conclusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "TnrbRbkwy-Qt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing feature 41 with p-value 0.8374620143895892\n",
            "Removing feature 78 with p-value 0.6910735190457282\n",
            "Removing feature 40 with p-value 0.5075308805016102\n",
            "Removing feature 15 with p-value 0.26425945619253505\n",
            "Removing feature 13 with p-value 0.3809424380380759\n",
            "Removing feature 124 with p-value 0.0698305027257394\n",
            "StatsModels OLS RMSE: 0.11221312709937135\n",
            "StatsModels OLS R²: 0.7999290375531993\n"
          ]
        }
      ],
      "source": [
        "significance_level = 0.05\n",
        "\n",
        "# P-value elimination\n",
        "while True:\n",
        "    model_sm = sm.OLS(y_train, X_train_sm).fit()  \n",
        "    max_p_value = model_sm.pvalues.max()\n",
        "    \n",
        "    if max_p_value > significance_level:\n",
        "        excluded_feature = model_sm.pvalues.idxmax()\n",
        "        print(f\"Removing feature {excluded_feature} with p-value {max_p_value}\")\n",
        "        \n",
        "\n",
        "        X_train_sm = X_train_sm.drop(columns=[excluded_feature]).reset_index(drop=True)\n",
        "        X_test_sm = X_test_sm.drop(columns=[excluded_feature]).reset_index(drop=True)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "preds = model_sm.predict(X_test_sm)\n",
        "\n",
        "\n",
        "rmse_sm = np.sqrt(mean_squared_error(y_test, preds))\n",
        "r2_sm = r2_score(y_test, preds)\n",
        "\n",
        "print(\"StatsModels OLS RMSE:\", rmse_sm)\n",
        "print(\"StatsModels OLS R²:\", r2_sm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see elemination by p-value didn't effect model quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df0eQLdNy-Qt"
      },
      "source": [
        "#### 6. [1 point] Find the best (RMSE) $\\alpha$ for Lasso regression using 4-fold cross-validation. You should choose a value from the logarithmic range $[10^{-4}, 10^{3}]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JPoT3YHqy-Qt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best alpha value: 0.0001\n",
            "Best RMSE: 0.33621615124823123\n",
            "Best R²: 0.796103513769233\n",
            "Test RMSE: 0.12185403186872831\n",
            "Test R²: 0.7640736015658242\n"
          ]
        }
      ],
      "source": [
        "alpha_values = np.logspace(-4, 3, 7)\n",
        "\n",
        "model_lasso = Lasso()\n",
        "\n",
        "param_grid = {\"alpha\": alpha_values}\n",
        "\n",
        "grid_search = GridSearchCV(model_lasso, param_grid, cv=4, scoring=\"neg_root_mean_squared_error\")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "print(f\"Best alpha value: {best_alpha}\")\n",
        "\n",
        "\n",
        "best_rmse = np.sqrt(-grid_search.best_score_)\n",
        "print(f\"Best RMSE: {best_rmse}\")\n",
        "best_r2 = r2_score(y_test, grid_search.predict(X_test))\n",
        "print(f\"Best R²: {best_r2}\")\n",
        "\n",
        "\n",
        "lasso_best = Lasso(alpha=best_alpha)\n",
        "lasso_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "y_pred = lasso_best.predict(X_test_scaled)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"Test RMSE: {test_rmse}\")\n",
        "test_r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Test R²: {test_r2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1PKinJUy-Qt"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "#### 7. [3.5 points] Implement Ridge regression for MSE loss trained using gradient descent.\n",
        "\n",
        "All computations must be vectorized, and Python loops can only be used for gradient descent iterations. The stopping criteria must be (simultaneously):\n",
        "\n",
        "* checking the absolute norm of the difference in weights on two adjacent iterations (e.g., less than some small number of the order of $10^{-6}$, specified by the `tolerance` parameter);\n",
        "\n",
        "* reaching the maximum number of iterations (e.g., 10000, specified by the `max_iter` parameter).\n",
        "\n",
        "You need to do:\n",
        "\n",
        "* Full gradient descent:\n",
        "\n",
        "$$\n",
        "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
        "$$\n",
        "\n",
        "* Stochastic Gradient Descent:\n",
        "\n",
        "$$\n",
        "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
        "$$\n",
        "\n",
        "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is an estimate of the gradient over a set of objects chosen at random.\n",
        "\n",
        "* Moment of method:\n",
        "\n",
        "$$\n",
        "h_0 = 0, \\\\\n",
        "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
        "w_{k + 1} = w_{k} - h_{k + 1}.\n",
        "$$\n",
        "\n",
        "* Adagrad method:\n",
        "\n",
        "$$\n",
        "G_0 = 0, \\\\\n",
        "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k}))^2, \\\\\n",
        "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
        "$$\n",
        "\n",
        "To verify that the optimization process is actually running, we will use the `loss_history` class attribute. After calling the fit method, it should contain the loss function values ​​for all iterations starting from the first (up to the first step along the antigradient).\n",
        "\n",
        "You need to initialize the weights with a random vector from a normal distribution. Below is a template that should contain code implementing all the model variants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oI39UzCLy-Qu"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "\n",
        "class LinReg(BaseEstimator):\n",
        "    def __init__(\n",
        "        self,\n",
        "        delta=0.001,\n",
        "        gd_type=\"Momentum\",\n",
        "        tolerance=1e-4,\n",
        "        max_iter=1000,\n",
        "        w0=None,\n",
        "        eta=1e-2,\n",
        "        alpha=1e-3,\n",
        "        epsilon=1e-8,\n",
        "        reg_cf=1e-4,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        gd_type: str\n",
        "            'GradientDescent', 'StochasticDescent', 'Momentum', 'Adagrad'\n",
        "        delta: float\n",
        "            proportion of object in a batch (for stochastic GD)\n",
        "        tolerance: float\n",
        "            for stopping gradient descent\n",
        "        max_iter: int\n",
        "            maximum number of steps in gradient descent\n",
        "        w0: np.array of shape (d)\n",
        "            init weights\n",
        "        eta: float\n",
        "            learning rate\n",
        "        alpha: float\n",
        "            momentum coefficient\n",
        "        reg_cf: float\n",
        "            regularization coefficient\n",
        "        epsilon: float\n",
        "            numerical stability\n",
        "        \"\"\"\n",
        "\n",
        "        self.delta = delta\n",
        "        self.gd_type = gd_type\n",
        "        self.tolerance = tolerance\n",
        "        self.max_iter = max_iter\n",
        "        self.w0 = w0\n",
        "        self.w = None\n",
        "        self.alpha = alpha\n",
        "        self.eta = eta\n",
        "        self.epsilon = epsilon\n",
        "        self.reg_cf = reg_cf\n",
        "        self.loss_history = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d)\n",
        "        y: np.array of shape (l)\n",
        "        ---\n",
        "        output: self\n",
        "        \"\"\"\n",
        "        self.loss_history = []\n",
        "\n",
        "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "        self.w = self.w0 if self.w0 is not None else np.random.randn(X.shape[1])\n",
        "\n",
        "        self.momentum = np.zeros(X.shape[1])\n",
        "\n",
        "        self.g = np.zeros(X.shape[1])\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.loss_history.append(self.calc_loss(X, y))\n",
        "\n",
        "            if self.gd_type == \"GradientDescent\":\n",
        "                update = self.calc_gradient(X, y)\n",
        "            elif self.gd_type == \"StochasticDescent\":\n",
        "                indices = np.random.choice(X.shape[0], int(X.shape[0] * self.delta), replace=False)\n",
        "                update = self.calc_gradient(X[indices], y[indices])\n",
        "            elif self.gd_type == \"Momentum\":\n",
        "                dw = self.calc_gradient(X, y)\n",
        "                self.momentum = self.alpha * self.momentum + self.eta * dw\n",
        "                update = self.momentum\n",
        "            elif self.gd_type == \"Adagrad\":\n",
        "                dw = self.calc_gradient(X, y)\n",
        "                self.g += dw**2\n",
        "                update = dw / np.sqrt(self.g + self.epsilon)\n",
        "            else:\n",
        "                raise ValueError(\"Invalid gradient descent type: {self.gd_type}\")\n",
        "\n",
        "            # L2 Regularization\n",
        "            update += self.reg_cf * self.w*2\n",
        "            self.w -= self.eta * update\n",
        "            \n",
        "            if np.linalg.norm(self.eta * update) < self.tolerance:\n",
        "                break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.w is None:\n",
        "            raise Exception(\"Not trained yet\")\n",
        "\n",
        "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "        return X @ self.w\n",
        "\n",
        "    def calc_gradient(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
        "        y: np.array of shape (l)\n",
        "        ---\n",
        "        output: np.array of shape (d)\n",
        "        \"\"\"\n",
        "        return 2 * X.T @ (X @ self.w - y) / X.shape[0]\n",
        "\n",
        "    def calc_loss(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d)\n",
        "        y: np.array of shape (l)\n",
        "        ---\n",
        "        output: float\n",
        "        \"\"\"\n",
        "        return np.mean((X @ self.w - y) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QQJEjGVy-Qu"
      },
      "source": [
        "#### 8. [1 point] Train and validate \"manual\" models on the same data, compare the quality with models from Sklearn and StatsModels. Investigate the influence of the `max_iter` and `alpha` parameters on the optimization process. Does it meet your expectations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for GradientDescent model:\n",
            "Test RMSE: 0.12185354700302478\n",
            "Test R²: 0.7640754790973234\n",
            "----------\n",
            "\n",
            "Results for StochasticDescent model:\n",
            "Test RMSE: 0.12205211361254202\n",
            "Test R²: 0.7633059503811204\n",
            "----------\n",
            "\n",
            "Results for Momentum model:\n",
            "Test RMSE: 0.1221762285290963\n",
            "Test R²: 0.762824316786567\n",
            "----------\n",
            "\n",
            "Results for Adagrad model:\n",
            "Test RMSE: 0.12185191595456833\n",
            "Test R²: 0.7640817949044502\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_model(model, X_train, y_train, X_test, y_test):\n",
        "    if not isinstance(y_train, np.ndarray):\n",
        "        y_train = np.array(y_train)\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    print(f\"Results for {model.gd_type} model:\")\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    test_r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"Test R²: {test_r2}\")\n",
        "    print(\"----------\\n\")\n",
        "\n",
        "gd_model = LinReg(gd_type='GradientDescent', eta=1e-1)\n",
        "sgd_model = LinReg(gd_type='StochasticDescent', eta=1e-1)\n",
        "momentum_model = LinReg(gd_type='Momentum', eta=1e-1, alpha=1e-2)\n",
        "adagrad_model = LinReg(gd_type='Adagrad', eta=1)\n",
        "\n",
        "# here I'm using only numerical features because I it's disallowed and trains faster\n",
        "# so I'm going to be comparing the results to the results of models with only numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train[numerical_features])\n",
        "X_test_scaled = scaler.transform(X_test[numerical_features])\n",
        "\n",
        "for model in [gd_model, sgd_model, momentum_model, adagrad_model]:\n",
        "    test_model(model, X_train_scaled, y_train, X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Models achieve same results as models from StatsModels and sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqYtVqv-y-Qu"
      },
      "source": [
        "#### 9. [1 point] Plot graphs of the loss function values ​​as a function of the iteration number for all models (full gradient descent, stochastic gc, Momentum, and Adagrad). Draw conclusions about the convergence rate of various modifications of gradient descent.\n",
        "\n",
        "Don't forget about what a *nice* graph should look like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Xbwhu8BSy-Qu"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1600x500 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "for i, model in enumerate([gd_model, sgd_model, momentum_model, adagrad_model]):\n",
        "    plt.subplot(1, 4, i + 1)\n",
        "    plt.plot(np.arange(0, len(model.loss_history)), model.loss_history)\n",
        "    plt.title(f\"Model {model.gd_type}\")\n",
        "\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adagram converges first, then batch gradient descent, then we have momentum and stochastic doesn't stop early.\n",
        "\n",
        "However, I find this assignment very weird, because each step of SGD is much smaller in terms of FLOPS and samples.\n",
        "Obviously momentum with batch gradient descent achieved worse results then batch gradient descent without momentum, because \n",
        "the point of momentum is to make gradient  descent on mini-batches (SGD, Adam) more stable."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
